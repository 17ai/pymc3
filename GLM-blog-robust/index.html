<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="PyMC devs">
        
        <link rel="shortcut icon" href="../img/favicon.ico">

	<title>Robust Regression - PyMC3</title>

        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href="..">PyMC3</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="..">Overview</a>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Tutorial <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li >
                                <a href="../getting_started/">Getting started</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Examples <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li >
                                <a href="../BEST/">BEST</a>
                            </li>
                        
                            <li >
                                <a href="../stochastic_volatility/">Stochastic Volatility</a>
                            </li>
                        
                            <li >
                                <a href="../hierarchical/">Hierarchical model</a>
                            </li>
                        
                            <li >
                                <a href="../GLM-blog-linear/">Linear Regression</a>
                            </li>
                        
                            <li class="active">
                                <a href="./">Robust Regression</a>
                            </li>
                        
                            <li >
                                <a href="../glm_hierarchical/">Hierarchical linear regression</a>
                            </li>
                        
                            <li >
                                <a href="../pmf-pymc/">Probabilistic Matrix Factorization</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                </ul>
            

            
            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                
                <li >
                    <a rel="next" href="../GLM-blog-linear/">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../glm_hierarchical/">
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                
                <li>
                    <a href="https://github.com/pymc-devs/pymc3">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
            
        </div>
    </div>
</div>

        <div class="container">
            
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#robust-regression">Robust Regression</a></li>
        
            <li><a href="#summary">Summary</a></li>
        
    
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<p>This is the second blog post in a small series on Bayesian GLMs:</p>
<ol>
<li><a href="http://twiecki.github.com/blog/2013/08/12/bayesian-glms-1/">The Inference Button: Bayesian GLMs made easy with PyMC3</a></li>
<li><a href="http://twiecki.github.io/blog/2013/08/27/bayesian-glms-2/">This world is far from Normal(ly distributed): Robust Regression in PyMC3</a></li>
<li><a href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/">The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3</a></li>
</ol>
<p>In this blog post I will write about:</p>
<ul>
<li>How a few outliers can largely affect the fit of linear regression models.</li>
<li>How replacing the normal likelihood with Student T distribution produces robust regression.</li>
<li>How this can easily be done with <code>PyMC3</code> and its new <code>glm</code> module by passing a <code>family</code> object.</li>
</ul>
<p>This is the second part of a series on Bayesian GLMs (click <a href="http://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/">here for part I about linear regression</a>). In this prior post I described how minimizing the squared distance of the regression line is the same as maximizing the likelihood of a Normal distribution with the mean coming from the regression line. This latter probabilistic expression allows us to easily formulate a Bayesian linear regression model.</p>
<p>This worked splendidly on simulated data. The problem with simulated data though is that it's, well, simulated. In the real world things tend to get more messy and assumptions like normality are easily violated by a few outliers. </p>
<p>Lets see what happens if we add some outliers to our simulated data from the last post.</p>
<p>Again, import our modules.</p>
<pre><code>import pymc as pm

import matplotlib.pyplot as plt
import numpy as np

import theano
</code></pre>
<p>Create some toy data but also add some outliers.</p>
<pre><code>size = 100
true_intercept = 1
true_slope = 2

x = np.linspace(0, 1, size)
# y = a + b*x
true_regression_line = true_intercept + true_slope * x
# add noise
y = true_regression_line + np.random.normal(scale=.5, size=size)

# Add outliers
x_out = np.append(x, [.1, .15, .2])
y_out = np.append(y, [8, 6, 9])

data = dict(x=x_out, y=y_out)
</code></pre>
<p>Plot the data together with the true regression line (the three points in the upper left corner are the outliers we added).</p>
<pre><code>fig = plt.figure(figsize=(7, 7))
ax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')
ax.plot(x_out, y_out, 'x', label='sampled data')
ax.plot(x, true_regression_line, label='true regression line', lw=2.)
plt.legend(loc=0);
</code></pre>
<p><img alt="png" src="../GLM-blog-robust_files/GLM-blog-robust_6_0.png" /></p>
<h1 id="robust-regression">Robust Regression</h1>
<p>Lets see what happens if we estimate our Bayesian linear regression model using the <code>glm()</code> function as before. This function takes a <a href="http://patsy.readthedocs.org/en/latest/quickstart.html"><code>Patsy</code></a> string to describe the linear model and adds a Normal likelihood by default. </p>
<pre><code>with pm.Model() as model:
    pm.glm.glm('y ~ x', data)
    trace = pm.sample(2000, pm.NUTS(), progressbar=False)
</code></pre>
<p>To evaluate the fit, I am plotting the posterior predictive regression lines by taking regression parameters from the posterior distribution and plotting a regression line for each (this is all done inside of <code>plot_posterior_predictive()</code>).</p>
<pre><code>plt.subplot(111, xlabel='x', ylabel='y', 
            title='Posterior predictive regression lines')
plt.plot(x_out, y_out, 'x', label='data')
pm.glm.plot_posterior_predictive(trace, samples=100, 
                                 label='posterior predictive regression lines')
plt.plot(x, true_regression_line, 
         label='true regression line', lw=3., c='y')

plt.legend(loc=0);
</code></pre>
<p><img alt="png" src="../GLM-blog-robust_files/GLM-blog-robust_10_0.png" /></p>
<p>As you can see, the fit is quite skewed and we have a fair amount of uncertainty in our estimate as indicated by the wide range of different posterior predictive regression lines. Why is this? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.</p>
<p>A Frequentist would estimate a <a href="http://en.wikipedia.org/wiki/Robust_regression">Robust Regression</a> and use a non-quadratic distance measure to evaluate the fit.</p>
<p>But what's a Bayesian to do? Since the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the <a href="http://en.wikipedia.org/wiki/Student%27s_t-distribution">Student T distribution</a> which has heavier tails as shown next (I read about this trick in <a href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">"The Kruschke"</a>, aka the puppy-book; but I think <a href="http://www.stat.columbia.edu/~gelman/book/">Gelman</a> was the first to formulate this).</p>
<p>Lets look at those two distributions to get a feel for them.</p>
<pre><code>normal_dist = pm.Normal.dist(mu=0, sd=1)
t_dist = pm.T.dist(mu=0, lam=1, nu=1)
x_eval = np.linspace(-8, 8, 300)
plt.plot(x_eval, theano.tensor.exp(normal_dist.logp(x_eval)).eval(), label='Normal', lw=2.)
plt.plot(x_eval, theano.tensor.exp(t_dist.logp(x_eval)).eval(), label='Student T', lw=2.)
plt.xlabel('x')
plt.ylabel('Probability density')
plt.legend();
</code></pre>
<p><img alt="png" src="../GLM-blog-robust_files/GLM-blog-robust_12_0.png" /></p>
<p>As you can see, the probability of values far away from the mean (0 in this case) are much more likely under the <code>T</code> distribution than under the Normal distribution.</p>
<p>To define the usage of a T distribution in <code>PyMC3</code> we can pass a family object -- <code>T</code> -- that specifies that our data is Student T-distributed (see <code>glm.families</code> for more choices). Note that this is the same syntax as <code>R</code> and <code>statsmodels</code> use.</p>
<pre><code>with pm.Model() as model_robust:
    family = pm.glm.families.T()
    pm.glm.glm('y ~ x', data, family=family)

    trace_robust = pm.sample(2000, pm.NUTS(), progressbar=False)

plt.figure(figsize=(5, 5))
plt.plot(x_out, y_out, 'x')
pm.glm.plot_posterior_predictive(trace_robust,
                                 label='posterior predictive regression lines')
plt.plot(x, true_regression_line, 
         label='true regression line', lw=3., c='y')
plt.legend();
</code></pre>
<p><img alt="png" src="../GLM-blog-robust_files/GLM-blog-robust_14_0.png" /></p>
<p>There, much better! The outliers are barely influencing our estimation at all because our likelihood function assumes that outliers are much more probable than under the Normal distribution.</p>
<h2 id="summary">Summary</h2>
<ul>
<li><code>PyMC3</code>'s <code>glm()</code> function allows you to pass in a <code>family</code> object that contains information about the likelihood.</li>
<li>By changing the likelihood from a Normal distribution to a Student T distribution -- which has more mass in the tails -- we can perform <em>Robust Regression</em>.</li>
</ul>
<p>The next post will be about logistic regression in PyMC3 and what the posterior and oatmeal have in common.</p>
<p><em>Extensions</em>: </p>
<ul>
<li>The Student-T distribution has, besides the mean and variance, a third parameter called <em>degrees of freedom</em> that describes how much mass should be put into the tails. Here it is set to 1 which gives maximum mass to the tails (setting this to infinity results in a Normal distribution!). One could easily place a prior on this rather than fixing it which I leave as an exercise for the reader ;).</li>
<li>T distributions can be used as priors as well. I will show this in a future post on hierarchical GLMs.</li>
<li>How do we test if our data is normal or violates that assumption in an important way? Check out this <a href="http://allendowney.blogspot.com/2013/08/are-my-data-normal.html">great blog post</a> by Allen Downey. </li>
</ul></div>
            
        </div>

        <footer class="col-md-12">
            <hr>
            
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>

        

        <script src="../js/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script src="../js/base.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
        <script src="../js/mathjaxhelper.js"></script>

        <!--
        MkDocs version  : 0.12.2
        Docs Build Date : 2015-04-26 18:57:08.568989
        -->
    </body>
</html>