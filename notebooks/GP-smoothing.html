

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Gaussian Process (GP) smoothing &mdash; PyMC3 3.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="PyMC3 3.0 documentation" href="../index.html"/>
        <link rel="up" title="Examples" href="../examples.html"/>
        <link rel="next" title="Dirichlet process mixtures for density estimation" href="dp_mix.html"/>
        <link rel="prev" title="Bayesian Survival Analysis" href="survival_analysis.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyMC3
          

          
          </a>

          
            
            
              <div class="version">
                3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="BEST.html">Bayesian Estimation Supersedes the T-Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="stochastic_volatility.html">Stochastic Volatility model</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-linear.html">The Inference Button: Bayesian GLMs made easy with PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust.html">This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust.html#robust-regression">Robust Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html">PyMC3 Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#glm-robust-regression-with-outlier-detection">GLM Robust Regression with Outlier Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#create-conventional-ols-model">Create Conventional OLS Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#create-robust-model-student-t-method">Create Robust Model: Student-T Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#create-robust-model-with-outliers-hogg-method">Create Robust Model with Outliers: Hogg Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#declare-outliers-and-compare-plots">Declare Outliers and Compare Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html">PyMC3 Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#glm-model-selection">GLM Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#generate-toy-datasets">Generate Toy Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#demonstrate-simple-linear-model">Demonstrate Simple Linear Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#create-higher-order-linear-models">Create Higher-Order Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#compare-deviance-information-criterion-dic">Compare Deviance Information Criterion [DIC]</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#compare-watanabe-akaike-information-criterion-waic">Compare Watanabe - Akaike Information Criterion [WAIC]</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#todo">TODO</a></li>
<li class="toctree-l2"><a class="reference internal" href="rolling_regression.html">Bayesian Rolling Regression in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html">The best of both worlds: Hierarchical Linear Regression in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#the-models">The Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#probabilistic-programming">Probabilistic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#posterior-predictive-check">Posterior Predictive Check</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#shrinkage">Shrinkage</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html">Probabilistic Matrix Factorization for Making Personalized Recommendations</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#methods">Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#evaluation">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#results">Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html">A Hierarchical model for Rugby prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#motivation">Motivation</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#what-do-we-want-to-infer">What do we want to infer?</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#what-do-we-want">What do we want?</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#what-assumptions-do-we-know-for-our-generative-story">What assumptions do we know for our &#8216;generative story&#8217;?</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#the-model">The model.</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#building-of-the-model">Building of the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#results">Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#covariates">Covariates.</a></li>
<li class="toctree-l2"><a class="reference internal" href="posterior_predictive.html">Posterior Predictive Checks in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="posterior_predictive.html#prediction">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="survival_analysis.html">Bayesian Survival Analysis</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Gaussian Process (GP) smoothing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#let-s-try-a-linear-regression-first">Let&#8217;s try a linear regression first</a></li>
<li class="toctree-l3"><a class="reference internal" href="#linear-regression-model-recap">Linear regression model recap</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gaussian-process-smoothing-model">Gaussian Process smoothing model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#let-s-describe-the-above-gp-smoothing-model-in-pymc3">Let&#8217;s describe the above GP-smoothing model in PyMC3</a></li>
<li class="toctree-l3"><a class="reference internal" href="#exploring-different-levels-of-smoothing">Exploring different levels of smoothing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#smoothing-to-the-limits">Smoothing &#8220;to the limits&#8221;</a></li>
<li class="toctree-l3"><a class="reference internal" href="#interactive-smoothing">Interactive smoothing</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="dp_mix.html">Dirichlet process mixtures for density estimation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">PyMC3</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../examples.html">Examples</a> &raquo;</li>
      
    <li>Gaussian Process (GP) smoothing</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/notebooks/GP-smoothing.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="gaussian-process-gp-smoothing">
<span id="gaussian-process-gp-smoothing"></span><h1>Gaussian Process (GP) smoothing<a class="headerlink" href="#gaussian-process-gp-smoothing" title="Permalink to this headline">¶</a></h1>
<p>This example deals with the case when we want to <strong>smooth</strong> the observed data points $(x_i, y_i)$ of some 1-dimensional function $y=f(x)$, by finding the new values $(x_i, y&#8217;_i)$ such that the new data is more &#8220;smooth&#8221; (see more on the definition of smoothness through allocation of variance in the model description below) when moving along the $x$ axis.</p>
<p>It is important to note that we are <strong>not</strong> dealing with the problem of interpolating the function $y=f(x)$ at the unknown values of $x$. Such problem would be called &#8220;regression&#8221; not &#8220;smoothing&#8221;, and will be considered in other examples.</p>
<p>If we assume the functional dependency between $x$ and $y$ is <strong>linear</strong> then, by making the independence and normality assumptions about the noise, we can infer a straight line that approximates the dependency between the variables, i.e. perform a linear regression. We can also fit more complex functional dependencies (like quadratic, cubic, etc), if we know the functional form of the dependency in advance.</p>
<p>However, the <strong>functional form</strong> of $y=f(x)$ is <strong>not always known in advance</strong>, and it might be hard to choose which one to fit, given the data. For example, you wouldn&#8217;t necessarily know which function to use, given the following observed data. Assume you haven&#8217;t seen the formula that generated it:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">pylab</span> <span class="n">inline</span>
<span class="n">figsize</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">);</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="n">Populating</span> <span class="n">the</span> <span class="n">interactive</span> <span class="n">namespace</span> <span class="kn">from</span> <span class="nn">numpy</span> <span class="ow">and</span> <span class="n">matplotlib</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="kn">as</span> <span class="nn">stats</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="o">/</span><span class="mf">15.0</span><span class="p">))</span> <span class="o">+</span> 
     <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>

<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>
<span class="n">title</span><span class="p">(</span><span class="s2">&quot;Observed Data&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GP-smoothing_2_0.png" /></p>
<div class="section" id="let-s-try-a-linear-regression-first">
<span id="let-s-try-a-linear-regression-first"></span><h2>Let&#8217;s try a linear regression first<a class="headerlink" href="#let-s-try-a-linear-regression-first" title="Permalink to this headline">¶</a></h2>
<p>As humans, we see that there is a non-linear dependency with some noise, and we would like to capture that dependency. If we perform a linear regression, we see that the &#8220;smoothed&#8221; data is less than satisfactory:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
<span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">);</span>
<span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">);</span>

<span class="n">lin</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lin</span><span class="o">.</span><span class="n">intercept</span> <span class="o">+</span> <span class="n">lin</span><span class="o">.</span><span class="n">slope</span> <span class="o">*</span> <span class="n">x</span><span class="p">);</span>
<span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear Smoothing&quot;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GP-smoothing_4_0.png" /></p>
</div>
<div class="section" id="linear-regression-model-recap">
<span id="linear-regression-model-recap"></span><h2>Linear regression model recap<a class="headerlink" href="#linear-regression-model-recap" title="Permalink to this headline">¶</a></h2>
<p>The linear regression assumes there is a linear dependency between the input $x$ and output $y$, sprinkled with some noise around it so that for each observed data point we have:</p>
<p>$$ y_i = a + b, x_i + \epsilon_i $$</p>
<p>where the observation errors at each data point satisfy:</p>
<p>$$ \epsilon_i \sim N(0, \sigma^2) $$</p>
<p>with the same $\sigma$, and the errors are independent:</p>
<p>$$ cov(\epsilon_i, \epsilon_j) = 0 : \text{ for } i \neq j $$</p>
<p>The parameters of this model are $a$, $b$, and $\sigma$. It turns out that, under these assumptions, the maximum likelihood estimates of $a$ and $b$ don&#8217;t depend on $\sigma$. Then $\sigma$ can be estimated separately, after finding the most likely values for $a$ and $b$.</p>
</div>
<div class="section" id="gaussian-process-smoothing-model">
<span id="gaussian-process-smoothing-model"></span><h2>Gaussian Process smoothing model<a class="headerlink" href="#gaussian-process-smoothing-model" title="Permalink to this headline">¶</a></h2>
<p>This model allows departure from the linear dependency by assuming that the dependency between $x$ and $y$ is a Brownian motion over the domain of $x$. This doesn&#8217;t go as far as assuming a particular functional dependency between the variables. Instead, by <strong>controlling the standard deviation of the unobserved Brownian motion</strong> we can achieve different levels of smoothness of the recovered functional dependency at the original data points.</p>
<p>The particular model we are going to discuss assumes that the observed data points are <strong>evenly spaced</strong> across the domain of $x$, and therefore can be indexed by $i=1,\dots,N$ without the loss of generality. The model is described as follows:</p>
<p>$$ z_i \sim N(z_{i-1} + \mu, (1 - \alpha)\cdot\sigma^2) : \text{ for } i=2,\dots,N $$</p>
<p>$$ z_1 \sim ImproperFlat(-\infty,\infty) $$</p>
<p>$$ y_i \sim N(z_i, \alpha\cdot\sigma^2) $$</p>
<p>where $z$ is the hidden Brownian motion, $y$ is the observed data, and the total variance $\sigma^2$ of each ovservation is split between the hidden Brownian motion and the noise in proportions of $1 - \alpha$ and $\alpha$ respectively, with parameter $0 &lt; \alpha &lt; 1$ specifying the degree of smoothing.</p>
<p>When we estimate the maximum likelihood values of the hidden process $z_i$ at each of the data points, $i=1,\dots,N$, these values provide an approximation of the functional dependency $y=f(x)$ as $\mathrm{E},[f(x_i)] = z_i$ at the original data points $x_i$ only. Therefore, again, the method is called smoothing and not regression.</p>
</div>
<div class="section" id="let-s-describe-the-above-gp-smoothing-model-in-pymc3">
<span id="let-s-describe-the-above-gp-smoothing-model-in-pymc3"></span><h2>Let&#8217;s describe the above GP-smoothing model in PyMC3<a class="headerlink" href="#let-s-describe-the-above-gp-smoothing-model-in-pymc3" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">shared</span>
<span class="kn">from</span> <span class="nn">pymc3.distributions.timeseries</span> <span class="kn">import</span> <span class="n">GaussianRandomWalk</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">optimize</span>
</pre></div>
</div>
<p>Let&#8217;s create a model with a shared parameter for specifying different levels of smoothing. We use very wide priors for the &#8220;mu&#8221; and &#8220;tau&#8221; parameters of the hidden Brownian motion, which you can adjust according to your application.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">LARGE_NUMBER</span> <span class="o">=</span> <span class="mf">1e5</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span>
<span class="k">with</span> <span class="n">model</span><span class="p">:</span>
    <span class="n">smoothing_param</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="mf">0.9</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;mu&quot;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">LARGE_NUMBER</span><span class="p">)</span>
    <span class="n">tau</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s2">&quot;tau&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">LARGE_NUMBER</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">GaussianRandomWalk</span><span class="p">(</span><span class="s2">&quot;z&quot;</span><span class="p">,</span>
                           <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span>
                           <span class="n">tau</span><span class="o">=</span><span class="n">tau</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">smoothing_param</span><span class="p">),</span> 
                           <span class="n">shape</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s2">&quot;obs&quot;</span><span class="p">,</span> 
                    <span class="n">mu</span><span class="o">=</span><span class="n">z</span><span class="p">,</span> 
                    <span class="n">tau</span><span class="o">=</span><span class="n">tau</span> <span class="o">/</span> <span class="n">smoothing_param</span><span class="p">,</span> 
                    <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Let&#8217;s also make a helper function for inferring the most likely values of $z$:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">infer_z</span><span class="p">(</span><span class="n">smoothing</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">model</span><span class="p">:</span>
        <span class="n">smoothing_param</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">smoothing</span><span class="p">)</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">find_MAP</span><span class="p">(</span><span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="n">z</span><span class="p">],</span> <span class="n">fmin</span><span class="o">=</span><span class="n">optimize</span><span class="o">.</span><span class="n">fmin_l_bfgs_b</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">res</span><span class="p">[</span><span class="s1">&#39;z&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>Please note that in this example, we are only looking at the MAP estimate of the unobserved variables. We are not really interested in inferring the posterior distributions. Instead, we have a control parameter $\alpha$ which lets us allocate the variance between the hidden Brownian motion and the noise. Other goals and/or different models may require sampling to obtain the posterior distributions, but for our goal a MAP estimate will suffice.</p>
</div>
<div class="section" id="exploring-different-levels-of-smoothing">
<span id="exploring-different-levels-of-smoothing"></span><h2>Exploring different levels of smoothing<a class="headerlink" href="#exploring-different-levels-of-smoothing" title="Permalink to this headline">¶</a></h2>
<p>Let&#8217;s try to allocate 50% variance to the noise, and see if the result matches our expectations.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">smoothing</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">z_val</span> <span class="o">=</span> <span class="n">infer_z</span><span class="p">(</span><span class="n">smoothing</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_val</span><span class="p">);</span>
<span class="n">title</span><span class="p">(</span><span class="s2">&quot;Smoothing={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">smoothing</span><span class="p">));</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GP-smoothing_14_0.png" /></p>
<p>It appears that the variance is split evenly between the noise and the hidden process, as expected.</p>
<p>Let&#8217;s try gradually increasing the smoothness parameter to see if we can obtain smoother data:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">smoothing</span> <span class="o">=</span> <span class="mf">0.9</span>
<span class="n">z_val</span> <span class="o">=</span> <span class="n">infer_z</span><span class="p">(</span><span class="n">smoothing</span><span class="p">)</span>

<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
<span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_val</span><span class="p">);</span>
<span class="n">title</span><span class="p">(</span><span class="s2">&quot;Smoothing={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">smoothing</span><span class="p">));</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GP-smoothing_16_0.png" /></p>
</div>
<div class="section" id="smoothing-to-the-limits">
<span id="smoothing-to-the-limits"></span><h2>Smoothing &#8220;to the limits&#8221;<a class="headerlink" href="#smoothing-to-the-limits" title="Permalink to this headline">¶</a></h2>
<p>By increading the smoothing parameter, we can gradually make the inferred values of the hidden Brownian motion approach the average value of the data. This is because as we increase the smoothing parameter, we allow less and less of the variance to be allocated to the Brownian motion, so eventually it aproaches the process which almost doesn&#8217;t change over the domain of $x$:</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axes</span> <span class="o">=</span> <span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">ax</span><span class="p">,</span> <span class="n">smoothing</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">axes</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="p">[</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">,</span> <span class="mf">0.9999</span><span class="p">]):</span>

    <span class="n">z_val</span> <span class="o">=</span> <span class="n">infer_z</span><span class="p">(</span><span class="n">smoothing</span><span class="p">)</span>

    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_val</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Smoothing={:05.4f}&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">smoothing</span><span class="p">))</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GP-smoothing_18_0.png" /></p>
</div>
<div class="section" id="interactive-smoothing">
<span id="interactive-smoothing"></span><h2>Interactive smoothing<a class="headerlink" href="#interactive-smoothing" title="Permalink to this headline">¶</a></h2>
<p>Below you can interactively test different levels of smoothing. Notice, because we use a <strong>shared Theano variable</strong> to specify the smoothing above, the model doesn&#8217;t need to be recompiled every time you move the slider, and so the <strong>inference is fast</strong>!</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">IPython.html.widgets</span> <span class="kn">import</span> <span class="n">interact</span>
<span class="nd">@interact</span><span class="p">(</span><span class="n">smoothing</span><span class="o">=</span><span class="p">[</span><span class="mf">0.01</span><span class="p">,</span><span class="mf">0.99</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">plot_smoothed</span><span class="p">(</span><span class="n">smoothing</span><span class="o">=</span><span class="mf">0.9</span><span class="p">):</span>
    <span class="n">z_val</span> <span class="o">=</span> <span class="n">infer_z</span><span class="p">(</span><span class="n">smoothing</span><span class="p">)</span>

    <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">);</span>
    <span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">z_val</span><span class="p">);</span>
    <span class="n">title</span><span class="p">(</span><span class="s2">&quot;Smoothing={}&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">smoothing</span><span class="p">));</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GP-smoothing_20_0.png" /></p>
<p>This example originally contributed by: Andrey Kuzmenko, http://github.com/akuz</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dp_mix.html" class="btn btn-neutral float-right" title="Dirichlet process mixtures for density estimation" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="survival_analysis.html" class="btn btn-neutral" title="Bayesian Survival Analysis" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, John Salvatier, Christopher Fonnesbeck, Thomas Wiecki.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'3.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>