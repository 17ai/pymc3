

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>GLM Robust Regression with Outlier Detection &mdash; pymc3 3.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="pymc3 3.0 documentation" href="../../index.html"/> 

  
  <script src="../../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../../index.html" class="icon icon-home"> pymc3
          

          
          </a>

          
            
            
              <div class="version">
                3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul>
<li class="toctree-l1"><a class="reference internal" href="../../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../index.html">pymc3</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../index.html">Docs</a> &raquo;</li>
      
    <li>GLM Robust Regression with Outlier Detection</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/notebooks/.ipynb_checkpoints/GLM-robust-with-outlier-detection-checkpoint.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput > div,
div.nbinput div[class^=highlight],
div.nbinput div[class^=highlight] pre,
div.nboutput,
div.nboutput > div,
div.nboutput div[class^=highlight],
div.nboutput div[class^=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class^=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput > :first-child pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput > :first-child pre {
    color: #D84315;
}

/* all prompts */
div.nbinput > :first-child[class^=highlight],
div.nboutput > :first-child[class^=highlight],
div.nboutput > :first-child {
    min-width: 11ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}

/* input/output area */
div.nbinput > :nth-child(2)[class^=highlight],
div.nboutput > :nth-child(2),
div.nboutput > :nth-child(2)[class^=highlight] {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}

/* input area */
div.nbinput > :nth-child(2)[class^=highlight] {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput  > :nth-child(2).stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="GLM-Robust-Regression-with-Outlier-Detection">
<h1>GLM Robust Regression with Outlier Detection<a class="headerlink" href="#GLM-Robust-Regression-with-Outlier-Detection" title="Permalink to this headline">Â¶</a></h1>
<p><strong>A minimal reproducable example of Robust Regression with Outlier
Detection using Hogg 2010 Signal vs Noise method.</strong></p>
<ul class="simple">
<li>This is a complementary approach to the Student-T robust regression
as illustrated in Thomas Wiecki&#8217;s notebook in the <a class="reference external" href="http://pymc-devs.github.io/pymc3/GLM-robust/">PyMC3
documentation</a>, that
approach is also compared here.</li>
<li>This model returns a robust estimate of linear coefficients and an
indication of which datapoints (if any) are outliers.</li>
<li>The likelihood evaluation is essentially a copy of eqn 17 in &#8220;Data
analysis recipes: Fitting a model to data&#8221; - <a class="reference external" href="http://arxiv.org/abs/1008.4686">Hogg
2010</a>.</li>
<li>The model is adapted specifically from Jake Vanderplas&#8217;
<a class="reference external" href="http://www.astroml.org/book_figures/chapter8/fig_outlier_rejection.html">implementation</a>
(3rd model tested).</li>
<li>The dataset is tiny and hardcoded into this Notebook. It contains
errors in both the x and y, but we will deal here with only errors in
y.</li>
</ul>
<p><strong>Note:</strong></p>
<ul class="simple">
<li>Python 3.4 project using latest available
<a class="reference external" href="https://github.com/pymc-devs/pymc3">PyMC3</a></li>
<li>Developed using <a class="reference external" href="https://www.continuum.io/downloads">ContinuumIO
Anaconda</a> distribution on a
Macbook Pro 3GHz i7, 16GB RAM, OSX 10.10.5.</li>
<li>During development I&#8217;ve found that 3 data points are always indicated
as outliers, but the remaining ordering of datapoints by decreasing
outlier-hood is slightly unstable between runs: the posterior surface
appears to have a small number of solutions with similar probability.</li>
<li>Finally, if runs become unstable or Theano throws weird errors, try
clearing the cache <code class="docutils literal"><span class="pre">$&gt;</span> <span class="pre">theano-cache</span> <span class="pre">clear</span></code> and rerunning the
notebook.</li>
</ul>
<p><strong>Package Requirements (shown as a conda-env YAML):</strong></p>
<div class="highlight-default"><div class="highlight"><pre><span></span>$&gt; less conda_env_pymc3_examples.yml

name: pymc3_examples
    channels:
      - defaults
    dependencies:
      - python=3.4
      - ipython
      - ipython-notebook
      - ipython-qtconsole
      - numpy
      - scipy
      - matplotlib
      - pandas
      - seaborn
      - patsy
      - pip

$&gt; conda env create --file conda_env_pymc3_examples.yml

$&gt; source activate pymc3_examples

$&gt; pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3
</pre></div>
</div>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">Â¶</a></h2>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>%matplotlib inline
%qtconsole --colors=linux

import warnings
warnings.filterwarnings(&#39;ignore&#39;)
</pre></div>
</div>
</div>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy import optimize
import pymc3 as pm
import theano as thno
import theano.tensor as T

# configure some basic options
sns.set(style=&quot;darkgrid&quot;, palette=&quot;muted&quot;)
pd.set_option(&#39;display.notebook_repr_html&#39;, True)
plt.rcParams[&#39;figure.figsize&#39;] = 12, 8
np.random.seed(0)
</pre></div>
</div>
</div>
<div class="section" id="Load-and-Prepare-Data">
<h3>Load and Prepare Data<a class="headerlink" href="#Load-and-Prepare-Data" title="Permalink to this headline">Â¶</a></h3>
<p>We&#8217;ll use the Hogg 2010 data available at
<a class="reference external" href="https://github.com/astroML/astroML/blob/master/astroML/datasets/hogg2010test.py">https://github.com/astroML/astroML/blob/master/astroML/datasets/hogg2010test.py</a></p>
<p>It&#8217;s a very small dataset so for convenience, it&#8217;s hardcoded below</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>#### cut &amp; pasted directly from the fetch_hogg2010test() function
## identical to the original dataset as hardcoded in the Hogg 2010 paper

dfhogg = pd.DataFrame(np.array([[1, 201, 592, 61, 9, -0.84],
                                 [2, 244, 401, 25, 4, 0.31],
                                 [3, 47, 583, 38, 11, 0.64],
                                 [4, 287, 402, 15, 7, -0.27],
                                 [5, 203, 495, 21, 5, -0.33],
                                 [6, 58, 173, 15, 9, 0.67],
                                 [7, 210, 479, 27, 4, -0.02],
                                 [8, 202, 504, 14, 4, -0.05],
                                 [9, 198, 510, 30, 11, -0.84],
                                 [10, 158, 416, 16, 7, -0.69],
                                 [11, 165, 393, 14, 5, 0.30],
                                 [12, 201, 442, 25, 5, -0.46],
                                 [13, 157, 317, 52, 5, -0.03],
                                 [14, 131, 311, 16, 6, 0.50],
                                 [15, 166, 400, 34, 6, 0.73],
                                 [16, 160, 337, 31, 5, -0.52],
                                 [17, 186, 423, 42, 9, 0.90],
                                 [18, 125, 334, 26, 8, 0.40],
                                 [19, 218, 533, 16, 6, -0.78],
                                 [20, 146, 344, 22, 5, -0.56]]),
                   columns=[&#39;id&#39;,&#39;x&#39;,&#39;y&#39;,&#39;sigma_y&#39;,&#39;sigma_x&#39;,&#39;rho_xy&#39;])


## for convenience zero-base the &#39;id&#39; and use as index
dfhogg[&#39;id&#39;] = dfhogg[&#39;id&#39;] - 1
dfhogg.set_index(&#39;id&#39;, inplace=True)

## standardize (mean center and divide by 1 sd)
dfhoggs = (dfhogg[[&#39;x&#39;,&#39;y&#39;]] - dfhogg[[&#39;x&#39;,&#39;y&#39;]].mean(0)) / dfhogg[[&#39;x&#39;,&#39;y&#39;]].std(0)
dfhoggs[&#39;sigma_y&#39;] = dfhogg[&#39;sigma_y&#39;] / dfhogg[&#39;y&#39;].std(0)
dfhoggs[&#39;sigma_x&#39;] = dfhogg[&#39;sigma_x&#39;] / dfhogg[&#39;x&#39;].std(0)

## create xlims ylims for plotting
xlims = (dfhoggs[&#39;x&#39;].min() - np.ptp(dfhoggs[&#39;x&#39;])/5
                 ,dfhoggs[&#39;x&#39;].max() + np.ptp(dfhoggs[&#39;x&#39;])/5)
ylims = (dfhoggs[&#39;y&#39;].min() - np.ptp(dfhoggs[&#39;y&#39;])/5
                 ,dfhoggs[&#39;y&#39;].max() + np.ptp(dfhoggs[&#39;y&#39;])/5)

## scatterplot the standardized data
g = sns.FacetGrid(dfhoggs, size=8)
_ = g.map(plt.errorbar, &#39;x&#39;, &#39;y&#39;, &#39;sigma_y&#39;, &#39;sigma_x&#39;, marker=&quot;o&quot;, ls=&#39;&#39;)
_ = g.axes[0][0].set_ylim(ylims)
_ = g.axes[0][0].set_xlim(xlims)

plt.subplots_adjust(top=0.92)
_ = g.fig.suptitle(&#39;Scatterplot of Hogg 2010 dataset after standardization&#39;, fontsize=16)
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_6_0.png" src="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_6_0.png" />
</div>
</div>
<p><strong>Observe</strong>:</p>
<ul class="simple">
<li>Even judging just by eye, you can see these datapoints mostly fall on
/ around a straight line with positive gradient</li>
<li>It looks like a few of the datapoints may be outliers from such a
line</li>
</ul>
</div>
</div>
<div class="section" id="Create-Conventional-OLS-Model">
<h2>Create Conventional OLS Model<a class="headerlink" href="#Create-Conventional-OLS-Model" title="Permalink to this headline">Â¶</a></h2>
<p>The <em>linear model</em> is really simple and conventional:</p>
<div class="math">
\[\bf{y} = \beta^{T} \bf{X} + \bf{\sigma}\]</div>
<p>where:</p>
<div class="line-block">
<div class="line"><span class="math">\(\beta\)</span> = coefs = <span class="math">\(\{1, \beta_{j \in X_{j}}\}\)</span></div>
<div class="line"><span class="math">\(\sigma\)</span> = the measured error in <span class="math">\(y\)</span> in the dataset
<code class="docutils literal"><span class="pre">sigma_y</span></code></div>
</div>
<p><strong>NOTE:</strong> + We&#8217;re using a simple linear OLS model with Normally
distributed priors so that it behaves like a ridge regression</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>with pm.Model() as mdl_ols:

    ## Define weakly informative Normal priors to give Ridge regression
    b0 = pm.Normal(&#39;b0_intercept&#39;, mu=0, sd=100)
    b1 = pm.Normal(&#39;b1_slope&#39;, mu=0, sd=100)

    ## Define linear model
    yest = b0 + b1 * dfhoggs[&#39;x&#39;]

    ## Use y error from dataset, convert into theano variable
    sigma_y = thno.shared(np.asarray(dfhoggs[&#39;sigma_y&#39;],
                            dtype=thno.config.floatX), name=&#39;sigma_y&#39;)

    ## Define Normal likelihood
    likelihood = pm.Normal(&#39;likelihood&#39;, mu=yest, sd=sigma_y, observed=dfhoggs[&#39;y&#39;])

</pre></div>
</div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>with mdl_ols:

    ## find MAP using Powell, seems to be more robust
    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)

    ## take samples
    traces_ols = pm.sample(2000, start=start_MAP, step=pm.NUTS(), progressbar=True)
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Optimization terminated successfully.
         Current function value: 145.777745
         Iterations: 3
         Function evaluations: 118
 [-----------------100%-----------------] 2000 of 2000 complete in 0.9 sec
</pre></div></div>
</div>
<p><strong>NOTE</strong>: I&#8217;ll &#8216;burn&#8217; the traces to only retain the final 1000 samples</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>_ = pm.traceplot(traces_ols[-1000:], figsize=(12,len(traces_ols.varnames)*1.5),
                lines={k: v[&#39;mean&#39;] for k, v in pm.df_summary(traces_ols[-1000:]).iterrows()})
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_15_0.png" src="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_15_0.png" />
</div>
</div>
<p><strong>NOTE:</strong> We&#8217;ll illustrate this OLS fit and compare to the datapoints in
the final plot</p>
<hr class="docutils" />
</div>
<hr class="docutils" />
<div class="section" id="Create-Robust-Model:-Student-T-Method">
<h2>Create Robust Model: Student-T Method<a class="headerlink" href="#Create-Robust-Model:-Student-T-Method" title="Permalink to this headline">Â¶</a></h2>
<p>I&#8217;ve added this brief section in order to directly compare the Student-T
based method exampled in Thomas Wiecki&#8217;s notebook in the <a class="reference external" href="http://pymc-devs.github.io/pymc3/GLM-robust/">PyMC3
documentation</a></p>
<p>Instead of using a Normal distribution for the likelihood, we use a
Student-T, which has fatter tails. In theory this allows outliers to
have a smaller mean square error in the likelihood, and thus have less
influence on the regression estimation. This method does not produce
inlier / outlier flags but is simpler and faster to run than the Signal
Vs Noise model below, so a comparison seems worthwhile.</p>
<p><strong>Note:</strong> we&#8217;ll constrain the Student-T &#8216;degrees of freedom&#8217; parameter
<code class="docutils literal"><span class="pre">nu</span></code> to be an integer, but otherwise leave it as just another
stochastic to be inferred: no need for prior knowledge.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>with pm.Model() as mdl_studentt:

    ## Define weakly informative Normal priors to give Ridge regression
    b0 = pm.Normal(&#39;b0_intercept&#39;, mu=0, sd=100)
    b1 = pm.Normal(&#39;b1_slope&#39;, mu=0, sd=100)

    ## Define linear model
    yest = b0 + b1 * dfhoggs[&#39;x&#39;]

    ## Use y error from dataset, convert into theano variable
    sigma_y = thno.shared(np.asarray(dfhoggs[&#39;sigma_y&#39;],
                            dtype=thno.config.floatX), name=&#39;sigma_y&#39;)

    ## define prior for Student T degrees of freedom
    nu = pm.DiscreteUniform(&#39;nu&#39;, lower=1, upper=100)

    ## Define Student T likelihood
    likelihood = pm.StudentT(&#39;likelihood&#39;, mu=yest, sd=sigma_y, nu=nu
                           ,observed=dfhoggs[&#39;y&#39;])

</pre></div>
</div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>with mdl_studentt:

    ## find MAP using Powell, seems to be more robust
    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)

    ## two-step sampling to allow Metropolis for nu (which is discrete)
    step1 = pm.NUTS([b0, b1])
    step2 = pm.Metropolis([nu])

    ## take samples
    traces_studentt = pm.sample(2000, start=start_MAP, step=[step1, step2], progressbar=True)
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Optimization terminated successfully.
         Current function value: 107.488021
         Iterations: 3
         Function evaluations: 77
 [-----------------100%-----------------] 2000 of 2000 complete in 1.0 sec
</pre></div></div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>_ = pm.traceplot(traces_studentt[-1000:]
            ,figsize=(12,len(traces_studentt.varnames)*1.5)
            ,lines={k: v[&#39;mean&#39;] for k, v in pm.df_summary(traces_studentt[-1000:]).iterrows()})
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_25_0.png" src="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_25_0.png" />
</div>
</div>
<p><strong>Observe:</strong></p>
<ul class="simple">
<li>Both parameters <code class="docutils literal"><span class="pre">b0</span></code> and <code class="docutils literal"><span class="pre">b1</span></code> show quite a skew to the right,
possibly this is the action of a few samples regressing closer to the
OLS estimate which is towards the left</li>
<li>The <code class="docutils literal"><span class="pre">nu</span></code> parameter seems very happy to stick at <code class="docutils literal"><span class="pre">nu</span> <span class="pre">=</span> <span class="pre">1</span></code>,
indicating that a fat-tailed Student-T likelihood has a better fit
than a thin-tailed (Normal-like) Student-T likelihood.</li>
<li>The inference sampling also ran very quickly, almost as quickly as
the conventional OLS</li>
</ul>
<p><strong>NOTE:</strong> We&#8217;ll illustrate this Student-T fit and compare to the
datapoints in the final plot</p>
<hr class="docutils" />
</div>
<hr class="docutils" />
<div class="section" id="Create-Robust-Model-with-Outliers:-Hogg-Method">
<h2>Create Robust Model with Outliers: Hogg Method<a class="headerlink" href="#Create-Robust-Model-with-Outliers:-Hogg-Method" title="Permalink to this headline">Â¶</a></h2>
<p>Please read the paper (Hogg 2010) and Jake Vanderplas&#8217; code for more
complete information about the modelling technique.</p>
<p>The general idea is to create a &#8216;mixture&#8217; model whereby datapoints can
be described by either the linear model (inliers) or a modified linear
model with different mean and larger variance (outliers).</p>
<p>The likelihood is evaluated over a mixture of two likelihoods, one for
&#8216;inliers&#8217;, one for &#8216;outliers&#8217;. A Bernouilli distribution is used to
randomly assign datapoints in N to either the inlier or outlier groups,
and we sample the model as usual to infer robust model parameters and
inlier / outlier flags:</p>
<div class="math">
\[\mathcal{logL} = \sum_{i}^{i=N} log \left[ \frac{(1 - B_{i})}{\sqrt{2 \pi \sigma_{in}^{2}}} exp \left( - \frac{(x_{i} - \mu_{in})^{2}}{2\sigma_{in}^{2}} \right) \right] + \sum_{i}^{i=N} log \left[ \frac{B_{i}}{\sqrt{2 \pi (\sigma_{in}^{2} + \sigma_{out}^{2})}} exp \left( - \frac{(x_{i}- \mu_{out})^{2}}{2(\sigma_{in}^{2} + \sigma_{out}^{2})} \right) \right]\]</div><div class="line-block">
<div class="line">where:</div>
<div class="line"><span class="math">\(\bf{B}\)</span> is Bernoulli-distibuted
<span class="math">\(B_{i} \in [0_{(inlier)},1_{(outlier)}]\)</span></div>
</div>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>def logp_signoise(yobs, is_outlier, yest_in, sigma_y_in, yest_out, sigma_y_out):
    &#39;&#39;&#39;
    Define custom loglikelihood for inliers vs outliers.
    NOTE: in this particular case we don&#39;t need to use theano&#39;s @as_op
    decorator because (as stated by Twiecki in conversation) that&#39;s only
    required if the likelihood cannot be expressed as a theano expression.
    We also now get the gradient computation for free.
    &#39;&#39;&#39;

    # likelihood for inliers
    pdfs_in = T.exp(-(yobs - yest_in + 1e-4)**2 / (2 * sigma_y_in**2))
    pdfs_in /= T.sqrt(2 * np.pi * sigma_y_in**2)
    logL_in = T.sum(T.log(pdfs_in) * (1 - is_outlier))

    # likelihood for outliers
    pdfs_out = T.exp(-(yobs - yest_out + 1e-4)**2 / (2 * (sigma_y_in**2 + sigma_y_out**2)))
    pdfs_out /= T.sqrt(2 * np.pi * (sigma_y_in**2 + sigma_y_out**2))
    logL_out = T.sum(T.log(pdfs_out) * is_outlier)

    return logL_in + logL_out

</pre></div>
</div>
</div>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>with pm.Model() as mdl_signoise:

    ## Define weakly informative Normal priors to give Ridge regression
    b0 = pm.Normal(&#39;b0_intercept&#39;, mu=0, sd=100)
    b1 = pm.Normal(&#39;b1_slope&#39;, mu=0, sd=100)

    ## Define linear model
    yest_in = b0 + b1 * dfhoggs[&#39;x&#39;]

    ## Define weakly informative priors for the mean and variance of outliers
    yest_out = pm.Normal(&#39;yest_out&#39;, mu=0, sd=100)
    sigma_y_out = pm.HalfNormal(&#39;sigma_y_out&#39;, sd=100)

    ## Define Bernoulli inlier / outlier flags according to a hyperprior
    ## fraction of outliers, itself constrained to [0,.5] for symmetry
    frac_outliers = pm.Uniform(&#39;frac_outliers&#39;, lower=0., upper=.5)
    is_outlier = pm.Bernoulli(&#39;is_outlier&#39;, p=frac_outliers, shape=dfhoggs.shape[0])

    ## Extract observed y and sigma_y from dataset, encode as theano objects
    yobs = thno.shared(np.asarray(dfhoggs[&#39;y&#39;], dtype=thno.config.floatX), name=&#39;yobs&#39;)
    sigma_y_in = thno.shared(np.asarray(dfhoggs[&#39;sigma_y&#39;]
                                , dtype=thno.config.floatX), name=&#39;sigma_y_in&#39;)

    ## Use custom likelihood using DensityDist
    likelihood = pm.DensityDist(&#39;likelihood&#39;, logp_signoise,
                        observed={&#39;yobs&#39;:yobs, &#39;is_outlier&#39;:is_outlier,
                                  &#39;yest_in&#39;:yest_in, &#39;sigma_y_in&#39;:sigma_y_in,
                                  &#39;yest_out&#39;:yest_out, &#39;sigma_y_out&#39;:sigma_y_out})

</pre></div>
</div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>with mdl_signoise:

    ## two-step sampling to create Bernoulli inlier/outlier flags
    step1 = pm.NUTS([frac_outliers, yest_out, sigma_y_out, b0, b1])
    step2 = pm.BinaryMetropolis([is_outlier], tune_interval=100)

    ## find MAP using Powell, seems to be more robust
    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)

    ## take samples
    traces_signoise = pm.sample(2000, start=start_MAP, step=[step1,step2], progressbar=True)
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Optimization terminated successfully.
         Current function value: 155.449990
         Iterations: 3
         Function evaluations: 213
 [-----------------100%-----------------] 2000 of 2000 complete in 169.1 sec
</pre></div></div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>_ = pm.traceplot(traces_signoise[-1000:], figsize=(12,len(traces_signoise.varnames)*1.5),
            lines={k: v[&#39;mean&#39;] for k, v in pm.df_summary(traces_signoise[-1000:]).iterrows()})
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_36_0.png" src="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_36_0.png" />
</div>
</div>
<p><strong>NOTE:</strong></p>
<ul class="simple">
<li>During development I&#8217;ve found that 3 datapoints id=[1,2,3] are always
indicated as outliers, but the remaining ordering of datapoints by
decreasing outlier-hood is unstable between runs: the posterior
surface appears to have a small number of solutions with very similar
probability.</li>
<li>The NUTS sampler seems to work okay, and indeed it&#8217;s a nice
opportunity to demonstrate a custom likelihood which is possible to
express as a theano function (thus allowing a gradient-based sampler
like NUTS). However, with a more complicated dataset, I would spend
time understanding this instability and potentially prefer using more
samples under Metropolis-Hastings.</li>
</ul>
<hr class="docutils" />
</div>
<hr class="docutils" />
<div class="section" id="Declare-Outliers-and-Compare-Plots">
<h2>Declare Outliers and Compare Plots<a class="headerlink" href="#Declare-Outliers-and-Compare-Plots" title="Permalink to this headline">Â¶</a></h2>
<p>At each step of the traces, each datapoint may be either an inlier or
outlier. We hope that the datapoints spend an unequal time being one
state or the other, so let&#8217;s take a look at the simple count of states
for each of the 20 datapoints.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>outlier_melt = pd.melt(pd.DataFrame(traces_signoise[&#39;is_outlier&#39;, -1000:],
                                   columns=[&#39;[{}]&#39;.format(int(d)) for d in dfhoggs.index]),
                      var_name=&#39;datapoint_id&#39;, value_name=&#39;is_outlier&#39;)
ax0 = sns.pointplot(y=&#39;datapoint_id&#39;, x=&#39;is_outlier&#39;, data=outlier_melt,
                   kind=&#39;point&#39;, join=False, ci=None, size=4, aspect=2)

_ = ax0.vlines([0,1], 0, 19, [&#39;b&#39;,&#39;r&#39;], &#39;--&#39;)

_ = ax0.set_xlim((-0.1,1.1))
_ = ax0.set_xticks(np.arange(0, 1.1, 0.1))
_ = ax0.set_xticklabels([&#39;{:.0%}&#39;.format(t) for t in np.arange(0,1.1,0.1)])

_ = ax0.yaxis.grid(True, linestyle=&#39;-&#39;, which=&#39;major&#39;, color=&#39;w&#39;, alpha=0.4)
_ = ax0.set_title(&#39;Prop. of the trace where datapoint is an outlier&#39;)
_ = ax0.set_xlabel(&#39;Prop. of the trace where is_outlier == 1&#39;)
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_42_0.png" src="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_42_0.png" />
</div>
</div>
<p><strong>Observe</strong>:</p>
<ul class="simple">
<li>The plot above shows the number of samples in the traces in which
each datapoint is marked as an outlier, expressed as a percentage.</li>
<li>In particular, 3 points [1, 2, 3] spend &gt;=95% of their time as
outliers</li>
<li>Contrastingly, points at the other end of the plot close to 0% are
our strongest inliers.</li>
<li>For comparison, the mean posterior value of <code class="docutils literal"><span class="pre">frac_outliers</span></code> is
~0.35, corresponding to roughly 7 of the 20 datapoints. You can see
these 7 datapoints in the plot above, all those with a value &gt;50% or
thereabouts.</li>
<li>However, only 3 of these points are outliers &gt;=95% of the time.</li>
<li>See note above regarding instability between runs.</li>
</ul>
<p>The 95% cutoff we choose is subjective and arbitrary, but I prefer it
for now, so let&#8217;s declare these 3 to be outliers and see how it looks
compared to Jake Vanderplas&#8217; outliers, which were declared in a slightly
different way as points with means above 0.68.</p>
<p><strong>Note:</strong> + I will declare outliers to be datapoints that have value ==
1 at the 5-percentile cutoff, i.e. in the percentiles from 5 up to 100,
their values are 1. + Try for yourself altering cutoff to larger values,
which leads to an objective ranking of outlier-hood.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [20]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>cutoff = 5
dfhoggs[&#39;outlier&#39;] = np.percentile(traces_signoise[-1000:][&#39;is_outlier&#39;],cutoff, axis=0)
dfhoggs[&#39;outlier&#39;].value_counts()
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>Out[20]:
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>
<span></span>0    17
1     3
Name: outlier, dtype: int64
</pre></div>
</div>
</div>
<div class="section" id="Posterior-Prediction-Plots-for-OLS-vs-StudentT-vs-SignalNoise">
<h3>Posterior Prediction Plots for OLS vs StudentT vs SignalNoise<a class="headerlink" href="#Posterior-Prediction-Plots-for-OLS-vs-StudentT-vs-SignalNoise" title="Permalink to this headline">Â¶</a></h3>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [21]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span>g = sns.FacetGrid(dfhoggs, size=8, hue=&#39;outlier&#39;, hue_order=[True,False],
                  palette=&#39;Set1&#39;, legend_out=False)

lm = lambda x, samp: samp[&#39;b0_intercept&#39;] + samp[&#39;b1_slope&#39;] * x

pm.glm.plot_posterior_predictive(traces_ols[-1000:],
        eval=np.linspace(-3, 3, 10), lm=lm, samples=200, color=&#39;#22CC00&#39;, alpha=.2)

pm.glm.plot_posterior_predictive(traces_studentt[-1000:], lm=lm,
        eval=np.linspace(-3, 3, 10), samples=200, color=&#39;#FFA500&#39;, alpha=.5)

pm.glm.plot_posterior_predictive(traces_signoise[-1000:], lm=lm,
        eval=np.linspace(-3, 3, 10), samples=200, color=&#39;#357EC7&#39;, alpha=.3)

_ = g.map(plt.errorbar, &#39;x&#39;, &#39;y&#39;, &#39;sigma_y&#39;, &#39;sigma_x&#39;, marker=&quot;o&quot;, ls=&#39;&#39;).add_legend()

_ = g.axes[0][0].annotate(&#39;OLS Fit: Green\nStudent-T Fit: Orange\nSignal Vs Noise Fit: Blue&#39;,
                          size=&#39;x-large&#39;, xy=(1,0), xycoords=&#39;axes fraction&#39;,
                          xytext=(-160,10), textcoords=&#39;offset points&#39;)
_ = g.axes[0][0].set_ylim(ylims)
_ = g.axes[0][0].set_xlim(xlims)
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_47_0.png" src="notebooks/.ipynb_checkpoints/../../_build/doctrees/nbsphinx/notebooks_.ipynb_checkpoints_GLM-robust-with-outlier-detection-checkpoint_47_0.png" />
</div>
</div>
<p><strong>Observe</strong>:</p>
<ul class="simple">
<li>The posterior preditive fit for:<ul>
<li>the <strong>OLS model</strong> is shown in <strong>Green</strong> and as expected, it
doesn&#8217;t appear to fit the majority of our datapoints very well,
skewed by outliers</li>
<li>the <strong>Robust Student-T model</strong> is shown in <strong>Orange</strong> and does
appear to fit the &#8216;main axis&#8217; of datapoints quite well, ignoring
outliers</li>
<li>the <strong>Robust Signal vs Noise model</strong> is shown in <strong>Blue</strong> and also
appears to fit the &#8216;main axis&#8217; of datapoints rather well, ignoring
outliers.</li>
</ul>
</li>
<li>We see that the <strong>Robust Signal vs Noise model</strong> also yields specific
estimates of <em>which</em> datapoints are outliers:<ul>
<li>17 &#8216;inlier&#8217; datapoints, in <strong>Blue</strong> and</li>
<li>3 &#8216;outlier&#8217; datapoints shown in <strong>Red</strong>.</li>
<li>From a simple visual inspection, the classification seems fair,
and agrees with Jake Vanderplas&#8217; findings.</li>
</ul>
</li>
<li>Overall, it seems that:<ul>
<li>the <strong>Signal vs Noise model</strong> behaves as promised, yielding a
robust regression estimate and explicit labelling of inliers /
outliers, but</li>
<li>the <strong>Signal vs Noise model</strong> is quite complex and whilst the
regression seems robust and stable, the actual inlier / outlier
labelling seems slightly unstable</li>
<li>if you simply want a robust regression without inlier / outlier
labelling, the <strong>Student-T model</strong> may be a good compromise,
offering a simple model, quick sampling, and a very similar
estimate.</li>
</ul>
</li>
</ul>
<hr class="docutils" />
<p>Example originally contributed by Jonathan Sedar 2015-12-21
<a class="reference external" href="https://github.com/jonsedar">github.com/jonsedar</a></p>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, John Salvatier, Christopher Fonnesbeck, Thomas Wiecki.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../',
            VERSION:'3.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>