

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>&lt;no title&gt; &mdash; PyMC3 3.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PyMC3 3.1 documentation" href="../index.html"/>
        <link rel="up" title="Examples" href="../examples.html"/>
        <link rel="next" title="How to debug a model" href="howto_debugging.html"/>
        <link rel="prev" title="Model comparison" href="model_comparison.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyMC3
          

          
          </a>

          
            
            
              <div class="version">
                3.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyMC3</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../examples.html">Examples</a> &raquo;</li>
        
      <li>&lt;no title&gt;</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/model_averaging.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput div.prompt,
div.nbinput div.input_area,
div.nbinput div[class*=highlight],
div.nbinput div[class*=highlight] pre,
div.nboutput,
div.nbinput div.prompt,
div.nbinput div.output_area,
div.nboutput div[class*=highlight],
div.nboutput div[class*=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput,
    div.nboutput {
        flex-direction: column;
    }
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput div.prompt pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput div.prompt pre {
    color: #D84315;
}

/* all prompts */
div.nbinput div.prompt,
div.nboutput div.prompt {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}
@media (max-width: 540px) {
    div.nbinput div.prompt,
    div.nboutput div.prompt {
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput div.prompt.empty {
        padding: 0;
    }
}

/* disable scrollbars on prompts */
div.nbinput div.prompt pre,
div.nboutput div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput div.input_area,
div.nboutput div.output_area {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput div.input_area,
    div.nboutput div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput div.input_area {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">([</span><span class="s1">&#39;seaborn-darkgrid&#39;</span><span class="p">,</span> <span class="s1">&#39;seaborn-colorblind&#39;</span><span class="p">])</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span>  <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">theano</span> <span class="kn">import</span> <span class="n">shared</span>
</pre></div>
</div>
</div>
<p>When confronted with more than one model we have several options. One of
them is to perform model selection, using for example a given
Information Criterion as exemplified <a class="reference external" href="model_comparsion.ipynb">in this
notebook</a> and this other
<a class="reference external" href="model_comparsion.ipynb">example</a>. Model selection is appealing for
its simplicity, but we are discarding information about the uncertainty
in our models. This is somehow similar to computing the full posterior
and then keep just keep a point-estimate like the posterior mean; we may
become overconfident of what we really know.</p>
<p>One alternative is to perform model selection but discuss the all the
different models together with the computed values of a given
Information Criterion. It is important to put all these numbers and
tests in the context of our problem so that we and our audience can have
a better feeling of the possible limitations and shortcomings of our
methods. If you are in the academic world you can use this approach to
add elements to the discussion section of a paper, presentation, thesis,
and so on.</p>
<p>Yet another approach is to perform model averaging. The idea now is to
generate a meta-model (and meta-predictions) using a weighted average of
the models. One way to compute these weights is to apply this formula:</p>
<div class="math">
\[w_i = \frac {e^{ \frac{1}{2} dIC_i }} {\sum_j^M e^{ - \frac{1}{2} dIC_j }}\]</div>
<p>Where <span class="math">\(dIC_i\)</span> is the difference between the i-esim information
criterion value and the lowest one. Remember that the lowest the value
of the IC, the better. We can use any information criterion we want to
compute a set of weights, but, of course, we cannot mix them.</p>
<p>This formula is a heuristic way to compute the relative probability of
each model (given a fixed set of models) from the information criteria
values. Look how the denominator is just a normalization term to ensure
that the weights sum up to one.</p>
<p>Once we have computed the weights we can use them to get a weighted
posterior predictive samples. PyMC3 offers functions to perform these
steps in a simple way, so let see them in action using an example.</p>
<p>The following example is taken from the superb book <a class="reference external" href="http://xcelab.net/rm/statistical-rethinking/">Statistical
Rethinking</a> by Richard
McElreath. You will find more PyMC3 examples from this book in this
<a class="reference external" href="https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3">repository</a>.
We are going to explore a simplified version of it. Check the book for
the whole example and a more thorough discussion of both, the biological
motivation for this problem and a theoretical/practical discussion of
using Information Criteria to compare, select and average models.</p>
<p>Briefly, our problem is as follows: We want to explore the composition
of milk across several primate species, it is hypothesized that females
from species of primates with larger brains produce more <em>nutritious</em>
milk (loosely speaking this is done <em>in order to</em> support the
development of such big brains). This is an important question for
evolutionary biologists and try to give and answer we will use 3
variables, two predictor variables: the proportion of neocortex compare
to the total mass of the brain and the logarithm of the body mass of the
mothers. And for predicted variable, the kilocalories per gram of milk.
With these variables we are going to build 3 different linear models:</p>
<ol class="arabic simple">
<li>A model using only the neocortex variable</li>
<li>A model using only the logarithm of the mass variable</li>
<li>A model using both variables</li>
</ol>
<p>Let start by uploading the data and centering the <code class="docutils literal"><span class="pre">neocortex</span></code> and
<code class="docutils literal"><span class="pre">log</span> <span class="pre">mass</span></code> variables, for better sampling.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">d</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/milk.csv&#39;</span><span class="p">)</span>
<span class="n">d</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">d</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">d</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>kcal.per.g</th>
      <th>neocortex</th>
      <th>log_mass</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.49</td>
      <td>-0.123706</td>
      <td>-0.831353</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.47</td>
      <td>-0.030706</td>
      <td>0.158647</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.56</td>
      <td>-0.030706</td>
      <td>0.181647</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.89</td>
      <td>0.000294</td>
      <td>-0.579353</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.92</td>
      <td>0.012294</td>
      <td>-1.885353</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>Now that we have the data we are going to build our first model using
only the <code class="docutils literal"><span class="pre">neocortex</span></code>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_0</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">epsilon</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;epsilon&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;neocortex&#39;</span><span class="p">]</span>

    <span class="n">kcal</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;kcal&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;kcal.per.g&#39;</span><span class="p">])</span>
    <span class="n">trace_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 12.469:   8%|▊         | 16237/200000 [00:02&lt;00:27, 6644.09it/s]
Convergence archived at 16300
Interrupted at 16,300 [8%]: Average Loss = 28.436
100%|██████████| 2500/2500 [00:03&lt;00:00, 809.12it/s]
</pre></div></div>
</div>
<p>For the first model I am going to check the posterior using the
<code class="docutils literal"><span class="pre">traceplot</span></code> function, you can do the same for the other models.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace_0</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_model_averaging_6_0.png" src="../_images/notebooks_model_averaging_6_0.png" />
</div>
</div>
<p>The second model is exactly the same as the first one, except we now use
the logarithm of the mass</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_1</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">d</span><span class="p">[</span><span class="s1">&#39;log_mass&#39;</span><span class="p">]</span>

    <span class="n">kcal</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;kcal&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;kcal.per.g&#39;</span><span class="p">])</span>

    <span class="n">trace_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 9.5197:  11%|█         | 21758/200000 [00:03&lt;00:26, 6805.33it/s]
Convergence archived at 22200
Interrupted at 22,200 [11%]: Average Loss = 26.56
100%|██████████| 2500/2500 [00:03&lt;00:00, 781.01it/s]
</pre></div></div>
</div>
<p>And finally the third model using the <code class="docutils literal"><span class="pre">neocortex</span></code> and <code class="docutils literal"><span class="pre">log_mass</span></code>
variables</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_2</span><span class="p">:</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;alpha&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;beta&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="n">mu</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">+</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">d</span><span class="p">[[</span><span class="s1">&#39;neocortex&#39;</span><span class="p">,</span><span class="s1">&#39;log_mass&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

    <span class="n">kcal</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;kcal&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">d</span><span class="p">[</span><span class="s1">&#39;kcal.per.g&#39;</span><span class="p">])</span>

    <span class="n">trace_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
Auto-assigning NUTS sampler...
Initializing NUTS using ADVI...
Average Loss = 9.7679:  11%|█         | 21786/200000 [00:03&lt;00:25, 7027.79it/s]
Convergence archived at 21900
Interrupted at 21,900 [10%]: Average Loss = 26.927
100%|██████████| 2500/2500 [00:04&lt;00:00, 574.80it/s]
</pre></div></div>
</div>
<p>Now that we have sampled the posterior for the 3 models, we are going to
use WAIC (Widely applicable information criterion) to compare the 3
models. We can do this using the <code class="docutils literal"><span class="pre">compare</span></code> function included with
PyMC3.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">traces</span> <span class="o">=</span> <span class="p">[</span><span class="n">trace_0</span><span class="p">,</span> <span class="n">trace_1</span><span class="p">,</span> <span class="n">trace_2</span><span class="p">]</span>
<span class="n">models</span> <span class="o">=</span> <span class="p">[</span><span class="n">model_0</span><span class="p">,</span> <span class="n">model_1</span><span class="p">,</span> <span class="n">model_2</span><span class="p">]</span>
<span class="n">comp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">compare</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="n">models</span><span class="p">)</span>
<span class="n">comp</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>Out[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div>
<style>
    .dataframe thead tr:only-child th {
        text-align: right;
    }

    .dataframe thead th {
        text-align: left;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>WAIC</th>
      <th>pWAIC</th>
      <th>dWAIC</th>
      <th>weight</th>
      <th>SE</th>
      <th>dSE</th>
      <th>warning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2</th>
      <td>-15.5565</td>
      <td>2.44088</td>
      <td>0</td>
      <td>0.951596</td>
      <td>4.7749</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-8.99463</td>
      <td>2.01189</td>
      <td>6.56192</td>
      <td>0.0357726</td>
      <td>4.1411</td>
      <td>0.889976</td>
      <td>1</td>
    </tr>
    <tr>
      <th>0</th>
      <td>-6.91266</td>
      <td>1.9867</td>
      <td>8.64388</td>
      <td>0.0126316</td>
      <td>3.1031</td>
      <td>3.81202</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>We can see that the best model is <code class="docutils literal"><span class="pre">model_2</span></code>, the one with both
predictor variables. Notice the DataFrame is ordered from lowest to
highest WAIC (<em>i.e</em> from <em>better</em> to <em>worst</em> model). Check <a class="reference internal" href="model_comparison.html"><span class="doc">this
notebook</span></a> for a more detailed discussing on
model comparison.</p>
<p>We can also see that we get a column with the relative <code class="docutils literal"><span class="pre">weight</span></code> for
each model (according to the first equation at the beginning of this
notebook). This weights can be <em>vaguely</em> interpreted as the probability
that each model will make the correct predictions on future data. Of
course this interpretation is conditional on the models used to compute
the weights, if we add or remove models the weights will change. And
also is dependent on the assumptions behind WAIC (or any other
Information Criterion used). So try to do not overinterpret these
<code class="docutils literal"><span class="pre">weights</span></code>.</p>
<p>We are going to use these weights to generate predictions based not on a
single model but on the weighted set of models. This is one way to
perform model averaging. Using PyMC3 we can call the <code class="docutils literal"><span class="pre">sample_ppc_w</span></code>
function as follows:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">ppc_w</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc_w</span><span class="p">(</span><span class="n">traces</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">models</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="n">comp</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
100%|██████████| 1000/1000 [00:30&lt;00:00, 32.31it/s]
</pre></div></div>
</div>
<p>We are also going to compute PPCs for the lowest-WAIC model</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">ppc_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc</span><span class="p">(</span><span class="n">trace_2</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">model_2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="stderr output_area docutils container">
<div class="highlight"><pre>
100%|██████████| 1000/1000 [00:36&lt;00:00, 28.42it/s]
</pre></div></div>
</div>
<p>A simple way to compare both kind of predictions is to plot their mean
and hpd interval</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">mean_w</span> <span class="o">=</span> <span class="n">ppc_w</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">hpd_w</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">hpd</span><span class="p">(</span><span class="n">ppc_w</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">mean</span> <span class="o">=</span> <span class="n">ppc_2</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">hpd</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">hpd</span><span class="p">(</span><span class="n">ppc_2</span><span class="p">[</span><span class="s1">&#39;kcal&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="p">[[</span><span class="n">mean</span> <span class="o">-</span> <span class="n">hpd</span><span class="p">]],</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;model 2&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">mean_w</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="p">[[</span><span class="n">mean_w</span> <span class="o">-</span> <span class="n">hpd_w</span><span class="p">]],</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;weighted models&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;kcal per g&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_model_averaging_18_0.png" src="../_images/notebooks_model_averaging_18_0.png" />
</div>
</div>
<p>As we can see the mean value is almost the same for both predictions but
the uncertainty in the weighted model is larger. We have effectively
propagated the uncertainty about which model we should select to the
posterior predictive samples.</p>
<p><strong>Final notes:</strong></p>
<p>There are other ways to average models such as, for example, explicitly
building a meta-model that includes all the models we have. We then
perform parameter inference while jumping between the models. One
problem with this approach is that jumping between models could hamper
the proper sampling of the posterior.</p>
<p>Besides averaging discrete models we can sometimes think of continuous
versions of them. A toy example is to imagine that we have a coin and we
want to estimated it’s degree of bias, a number between 0 and 1 being
0.5 equal chance of head and tails. We could think of two separated
models one with a prior biased towards heads and one towards tails. We
could fit both separate models and then average them using, for example,
IC-derived weights. As an alternative, is to build a hierarchical model
to estimate the prior distribution, instead of contemplating two
discrete models we will be computing a continuous model that includes
these two both discrete models as particular cases. Which approach is
better? That depends on our concrete problem. Do we have good reasons to
think about two discrete models, or is our problem better represented
with a continuous bigger model?</p>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="howto_debugging.html" class="btn btn-neutral float-right" title="How to debug a model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="model_comparison.html" class="btn btn-neutral" title="Model comparison" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, John Salvatier, Christopher Fonnesbeck, Thomas Wiecki.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'3.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>