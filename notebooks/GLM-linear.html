

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>The Inference Button: Bayesian GLMs made easy with PyMC3 &mdash; PyMC3 3.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="PyMC3 3.0 documentation" href="../index.html"/>
        <link rel="up" title="Examples" href="../examples.html"/>
        <link rel="next" title="This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3" href="GLM-robust.html"/>
        <link rel="prev" title="Stochastic Volatility model" href="stochastic_volatility.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyMC3
          

          
          </a>

          
            
            
              <div class="version">
                3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="BEST.html">Bayesian Estimation Supersedes the T-Test</a></li>
<li class="toctree-l2"><a class="reference internal" href="stochastic_volatility.html">Stochastic Volatility model</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">The Inference Button: Bayesian GLMs made easy with PyMC3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear-regression">Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="#probabilistic-reformulation">Probabilistic Reformulation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#bayesian-glms-in-pymc3">Bayesian GLMs in PyMC3</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#generating-data">Generating data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#estimating-the-model">Estimating the model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#analyzing-the-model">Analyzing the model</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#summary">Summary</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#further-reading">Further reading</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust.html">This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust.html#robust-regression">Robust Regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html">PyMC3 Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#glm-robust-regression-with-outlier-detection">GLM Robust Regression with Outlier Detection</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#create-conventional-ols-model">Create Conventional OLS Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#create-robust-model-student-t-method">Create Robust Model: Student-T Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#create-robust-model-with-outliers-hogg-method">Create Robust Model with Outliers: Hogg Method</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-robust-with-outlier-detection.html#declare-outliers-and-compare-plots">Declare Outliers and Compare Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html">PyMC3 Examples</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#glm-model-selection">GLM Model Selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#setup">Setup</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#generate-toy-datasets">Generate Toy Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#demonstrate-simple-linear-model">Demonstrate Simple Linear Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#create-higher-order-linear-models">Create Higher-Order Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#compare-deviance-information-criterion-dic">Compare Deviance Information Criterion [DIC]</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#compare-watanabe-akaike-information-criterion-waic">Compare Watanabe - Akaike Information Criterion [WAIC]</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-model-selection.html#todo">TODO</a></li>
<li class="toctree-l2"><a class="reference internal" href="rolling_regression.html">Bayesian Rolling Regression in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html">The best of both worlds: Hierarchical Linear Regression in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#the-models">The Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#probabilistic-programming">Probabilistic Programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#posterior-predictive-check">Posterior Predictive Check</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#shrinkage">Shrinkage</a></li>
<li class="toctree-l2"><a class="reference internal" href="GLM-hierarchical.html#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html">Probabilistic Matrix Factorization for Making Personalized Recommendations</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#methods">Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#evaluation">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#results">Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="pmf-pymc.html#summary">Summary</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html">A Hierarchical model for Rugby prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#motivation">Motivation</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#what-do-we-want-to-infer">What do we want to infer?</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#what-do-we-want">What do we want?</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#what-assumptions-do-we-know-for-our-generative-story">What assumptions do we know for our &#8216;generative story&#8217;?</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#the-model">The model.</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#building-of-the-model">Building of the model</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#results">Results</a></li>
<li class="toctree-l2"><a class="reference internal" href="rugby_analytics.html#covariates">Covariates.</a></li>
<li class="toctree-l2"><a class="reference internal" href="posterior_predictive.html">Posterior Predictive Checks in PyMC3</a></li>
<li class="toctree-l2"><a class="reference internal" href="posterior_predictive.html#prediction">Prediction</a></li>
<li class="toctree-l2"><a class="reference internal" href="survival_analysis.html">Bayesian Survival Analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="GP-smoothing.html">Gaussian Process (GP) smoothing</a></li>
<li class="toctree-l2"><a class="reference internal" href="dp_mix.html">Dirichlet process mixtures for density estimation</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">PyMC3</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../examples.html">Examples</a> &raquo;</li>
      
    <li>The Inference Button: Bayesian GLMs made easy with PyMC3</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/notebooks/GLM-linear.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="the-inference-button-bayesian-glms-made-easy-with-pymc3">
<span id="the-inference-button-bayesian-glms-made-easy-with-pymc3"></span><h1>The Inference Button: Bayesian GLMs made easy with PyMC3<a class="headerlink" href="#the-inference-button-bayesian-glms-made-easy-with-pymc3" title="Permalink to this headline">¶</a></h1>
<p>Author: Thomas Wiecki</p>
<p>This tutorial appeared as a post in a small series on Bayesian GLMs on my blog:</p>
<ol class="simple">
<li><a class="reference external" href="http://twiecki.github.com/blog/2013/08/12/bayesian-glms-1/">The Inference Button: Bayesian GLMs made easy with PyMC3</a></li>
<li><a class="reference external" href="http://twiecki.github.io/blog/2013/08/27/bayesian-glms-2/">This world is far from Normal(ly distributed): Robust Regression in PyMC3</a></li>
<li><a class="reference external" href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/">The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3</a></li>
</ol>
<p>In this blog post I will talk about:</p>
<ul class="simple">
<li>How the Bayesian Revolution in many scientific disciplines is hindered by poor usability of current Probabilistic Programming languages.</li>
<li>A gentle introduction to Bayesian linear regression and how it differs from the frequentist approach.</li>
<li>A preview of <a class="reference external" href="https://github.com/pymc-devs/pymc/tree/pymc3">PyMC3</a> (currently in alpha) and its new GLM submodule I wrote to allow creation and estimation of Bayesian GLMs as easy as frequentist GLMs in R.</li>
</ul>
<p>Ready? Lets get started!</p>
<p>There is a huge paradigm shift underway in many scientific disciplines: The Bayesian Revolution.</p>
<p>While the theoretical benefits of Bayesian over Frequentist stats have been discussed at length elsewhere (see <em>Further Reading</em> below), there is a major obstacle that hinders wider adoption &#8211; <em>usability</em> (this is one of the reasons DARPA wrote out a huge grant to <a class="reference external" href="http://www.darpa.mil/Our_Work/I2O/Programs/Probabilistic_Programming_for_Advanced_Machine_Learning_%28PPAML%29.aspx">improve Probabilistic Programming</a>).</p>
<p>This is mildly ironic because the beauty of Bayesian statistics is their generality. Frequentist stats have a bazillion different tests for every different scenario. In Bayesian land you define your model exactly as you think is appropriate and hit the <em>Inference Button(TM)</em> (i.e. running the magical MCMC sampling algorithm).</p>
<p>Yet when I ask my colleagues why they use frequentist stats (even though they would like to use Bayesian stats) the answer is that software packages like SPSS or R make it very easy to run all those individuals tests with a single command (and more often then not, they don&#8217;t know the exact model and inference method being used).</p>
<p>While there are great Bayesian software packages like <a class="reference external" href="http://mcmc-jags.sourceforge.net/">JAGS</a>, <a class="reference external" href="http://www.mrc-bsu.cam.ac.uk/bugs/">BUGS</a>, <a class="reference external" href="http://mc-stan.org/">Stan</a> and <a class="reference external" href="http://pymc-devs.github.io/pymc/">PyMC</a>, they are written for Bayesians statisticians who know very well what model they want to build.</p>
<p>Unfortunately, <a class="reference external" href="http://simplystatistics.org/2013/06/14/the-vast-majority-of-statistical-analysis-is-not-performed-by-statisticians/">&#8220;the vast majority of statistical analysis is not performed by statisticians&#8221;</a> &#8211; so what we really need are tools for <em>scientists</em> and not for statisticians.</p>
<p>In the interest of putting my code where my mouth is I wrote a submodule for the upcoming <a class="reference external" href="https://github.com/pymc-devs/pymc/tree/pymc3">PyMC3</a> that makes construction of Bayesian Generalized Linear Models (GLMs) as easy as Frequentist ones in R.</p>
<div class="section" id="linear-regression">
<span id="linear-regression"></span><h2>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this headline">¶</a></h2>
<p>While future blog posts will explore more complex models, I will start here with the simplest GLM &#8211; linear regression.
In general, frequentists think about Linear Regression as follows:</p>
<p>$$ Y = X\beta + \epsilon $$</p>
<p>where $Y$ is the output we want to predict (or <em>dependent</em> variable), $X$ is our predictor (or <em>independent</em> variable), and $\beta$ are the coefficients (or parameters) of the model we want to estimate. $\epsilon$ is an error term which is assumed to be normally distributed.</p>
<p>We can then use Ordinary Least Squares or Maximum Likelihood to find the best fitting $\beta$.</p>
</div>
<div class="section" id="probabilistic-reformulation">
<span id="probabilistic-reformulation"></span><h2>Probabilistic Reformulation<a class="headerlink" href="#probabilistic-reformulation" title="Permalink to this headline">¶</a></h2>
<p>Bayesians take a probabilistic view of the world and express this model in terms of probability distributions. Our above linear regression can be rewritten to yield:</p>
<p>$$ Y \sim \mathcal{N}(X \beta, \sigma^2) $$</p>
<p>In words, we view $Y$ as a random variable (or random vector) of which each element (data point) is distributed according to a Normal distribution. The mean of this normal distribution is provided by our linear predictor with variance $\sigma^2$.</p>
<p>While this is essentially the same model, there are two critical advantages of Bayesian estimation:</p>
<ul class="simple">
<li>Priors: We can quantify any prior knowledge we might have by placing priors on the paramters. For example, if we think that $\sigma$ is likely to be small we would choose a prior with more probability mass on low values.</li>
<li>Quantifying uncertainty: We do not get a single estimate of $\beta$ as above but instead a complete posterior distribution about how likely different values of $\beta$ are. For example, with few data points our uncertainty in $\beta$ will be very high and we&#8217;d be getting very wide posteriors.</li>
</ul>
</div>
<div class="section" id="bayesian-glms-in-pymc3">
<span id="bayesian-glms-in-pymc3"></span><h2>Bayesian GLMs in PyMC3<a class="headerlink" href="#bayesian-glms-in-pymc3" title="Permalink to this headline">¶</a></h2>
<p>With the new GLM module in PyMC3 it is very easy to build this and much more complex models.</p>
<p>First, lets import the required modules.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="kn">from</span> <span class="nn">pymc3</span> <span class="kn">import</span>  <span class="o">*</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
</pre></div>
</div>
<div class="section" id="generating-data">
<span id="generating-data"></span><h3>Generating data<a class="headerlink" href="#generating-data" title="Permalink to this headline">¶</a></h3>
<p>Create some toy data to play around with and scatter-plot it.</p>
<p>Essentially we are creating a regression line defined by intercept and slope and add data points by sampling from a Normal with the mean set to the regression line.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">size</span> <span class="o">=</span> <span class="mi">200</span>
<span class="n">true_intercept</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">true_slope</span> <span class="o">=</span> <span class="mi">2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
<span class="c1"># y = a + b*x</span>
<span class="n">true_regression_line</span> <span class="o">=</span> <span class="n">true_intercept</span> <span class="o">+</span> <span class="n">true_slope</span> <span class="o">*</span> <span class="n">x</span>
<span class="c1"># add noise</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">true_regression_line</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=.</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">size</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Generated data and underlying model&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;sampled data&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_regression_line</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;true regression line&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">2.</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GLM-linear_6_0.png" /></p>
</div>
<div class="section" id="estimating-the-model">
<span id="estimating-the-model"></span><h3>Estimating the model<a class="headerlink" href="#estimating-the-model" title="Permalink to this headline">¶</a></h3>
<p>Lets fit a Bayesian linear regression model to this data. As you can see, model specifications in <code class="docutils literal"><span class="pre">PyMC3</span></code> are wrapped in a <code class="docutils literal"><span class="pre">with</span></code> statement.</p>
<p>Here we use the awesome new <a class="reference external" href="http://arxiv.org/abs/1111.4246">NUTS sampler</a> (our Inference Button) to draw 2000 posterior samples.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span> <span class="c1"># model specifications in PyMC3 are wrapped in a with-statement</span>
    <span class="c1"># Define priors</span>
    <span class="n">sigma</span> <span class="o">=</span> <span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;sigma&#39;</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">testval</span><span class="o">=</span><span class="mf">1.</span><span class="p">)</span>
    <span class="n">intercept</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;Intercept&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">x_coeff</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    
    <span class="c1"># Define likelihood</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">intercept</span> <span class="o">+</span> <span class="n">x_coeff</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> 
                        <span class="n">sd</span><span class="o">=</span><span class="n">sigma</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    
    <span class="c1"># Inference!</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">find_MAP</span><span class="p">()</span> <span class="c1"># Find starting value by optimization</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">scaling</span><span class="o">=</span><span class="n">start</span><span class="p">)</span> <span class="c1"># Instantiate MCMC sampling algorithm</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">start</span><span class="o">=</span><span class="n">start</span><span class="p">,</span> <span class="n">progressbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># draw 2000 posterior samples using NUTS sampling</span>
</pre></div>
</div>
<p>This should be fairly readable for people who know probabilistic programming. However, would my non-statistican friend know what all this does? Moreover, recall that this is an extremely simple model that would be one line in R. Having multiple, potentially transformed regressors, interaction terms or link-functions would also make this much more complex and error prone.</p>
<p>The new <code class="docutils literal"><span class="pre">glm()</span></code> function instead takes a <a class="reference external" href="http://patsy.readthedocs.org/en/latest/quickstart.html">Patsy</a> linear model specifier from which it creates a design matrix. <code class="docutils literal"><span class="pre">glm()</span></code> then adds random variables for each of the coefficients and an appopriate likelihood to the model.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
    <span class="c1"># specify glm and pass in data. The resulting linear model, its likelihood and </span>
    <span class="c1"># and all its parameters are automatically added to our model.</span>
    <span class="n">glm</span><span class="o">.</span><span class="n">glm</span><span class="p">(</span><span class="s1">&#39;y ~ x&#39;</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">find_MAP</span><span class="p">()</span>
    <span class="n">step</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">(</span><span class="n">scaling</span><span class="o">=</span><span class="n">start</span><span class="p">)</span> <span class="c1"># Instantiate MCMC sampling algorithm</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">step</span><span class="p">,</span> <span class="n">progressbar</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1"># draw 2000 posterior samples using NUTS sampling</span>
</pre></div>
</div>
<p>Much shorter, but this code does the exact same thing as the above model specification (you can change priors and everything else too if we wanted). <code class="docutils literal"><span class="pre">glm()</span></code> parses the <code class="docutils literal"><span class="pre">Patsy</span></code> model string, adds random variables for each regressor (<code class="docutils literal"><span class="pre">Intercept</span></code> and slope <code class="docutils literal"><span class="pre">x</span></code> in this case), adds a likelihood (by default, a Normal is chosen), and all other variables (<code class="docutils literal"><span class="pre">sigma</span></code>). Finally, <code class="docutils literal"><span class="pre">glm()</span></code> then initializes the parameters to a good starting point by estimating a frequentist linear model using <a class="reference external" href="http://statsmodels.sourceforge.net/devel/">statsmodels</a>.</p>
<p>If you are not familiar with R&#8217;s syntax, <code class="docutils literal"><span class="pre">'y</span> <span class="pre">~</span> <span class="pre">x'</span></code> specifies that we have an output variable <code class="docutils literal"><span class="pre">y</span></code> that we want to estimate as a linear function of <code class="docutils literal"><span class="pre">x</span></code>.</p>
</div>
<div class="section" id="analyzing-the-model">
<span id="analyzing-the-model"></span><h3>Analyzing the model<a class="headerlink" href="#analyzing-the-model" title="Permalink to this headline">¶</a></h3>
<p>Bayesian inference does not give us only one best fitting line (as maximum likelihood does) but rather a whole posterior distribution of likely parameters. Lets plot the posterior distribution of our parameters and the individual samples we drew.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="mi">100</span><span class="p">:])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">();</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">figure</span><span class="o">.</span><span class="n">Figure</span> <span class="n">at</span> <span class="mh">0x7f574ca72b10</span><span class="o">&gt;</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GLM-linear_15_1.png" /></p>
<p>The left side shows our marginal posterior &#8211; for each parameter value on the x-axis we get a probability on the y-axis that tells us how likely that parameter value is.</p>
<p>There are a couple of things to see here. The first is that our sampling chains for the individual parameters (left side) seem well converged and stationary (there are no large drifts or other odd patterns).</p>
<p>Secondly, the maximum posterior estimate of each variable (the peak in the left side distributions) is very close to the true parameters used to generate the data (<code class="docutils literal"><span class="pre">x</span></code> is the regression coefficient and <code class="docutils literal"><span class="pre">sigma</span></code> is the standard deviation of our normal).</p>
<p>In the GLM we thus do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. Here we are using the <code class="docutils literal"><span class="pre">glm.plot_posterior_predictive()</span></code> convenience function for this.</p>
<div class="highlight-python"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
<span class="n">glm</span><span class="o">.</span><span class="n">plot_posterior_predictive</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> 
                              <span class="n">label</span><span class="o">=</span><span class="s1">&#39;posterior predictive regression lines&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">true_regression_line</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;true regression line&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">3.</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Posterior predictive regression lines&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="png" src="../_images/GLM-linear_18_0.png" /></p>
<p>As you can see, our estimated regression lines are very similar to the true regression line. But since we only have limited data we have <em>uncertainty</em> in our estimates, here expressed by the variability of the lines.</p>
</div>
</div>
<div class="section" id="summary">
<span id="summary"></span><h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Usability is currently a huge hurdle for wider adoption of Bayesian statistics.</li>
<li><code class="docutils literal"><span class="pre">PyMC3</span></code> allows GLM specification with convenient syntax borrowed from R.</li>
<li>Posterior predictive plots allow us to evaluate fit and our uncertainty in it.</li>
</ul>
<div class="section" id="further-reading">
<span id="further-reading"></span><h3>Further reading<a class="headerlink" href="#further-reading" title="Permalink to this headline">¶</a></h3>
<p>This is the first post of a small series on Bayesian GLMs I am preparing. Next week I will describe how the Student T distribution can be used to perform robust linear regression.</p>
<p>Then there are also other good resources on Bayesian statistics:</p>
<ul class="simple">
<li>The excellent book <a class="reference external" href="http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/">Doing Bayesian Data Analysis by John Kruschke</a>.</li>
<li><a class="reference external" href="http://andrewgelman.com/">Andrew Gelman&#8217;s blog</a></li>
<li><a class="reference external" href="https://plus.google.com/u/0/107971134877020469960/posts/KpeRdJKR6Z1">Baeu Cronins blog post on Probabilistic Programming</a></li>
</ul>
</div>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="GLM-robust.html" class="btn btn-neutral float-right" title="This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="stochastic_volatility.html" class="btn btn-neutral" title="Stochastic Volatility model" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, John Salvatier, Christopher Fonnesbeck, Thomas Wiecki.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'3.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>