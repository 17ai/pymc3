

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Variational Inference: Bayesian Neural Networks &mdash; PyMC3 3.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PyMC3 3.0 documentation" href="../index.html"/>
        <link rel="up" title="Examples" href="../examples.html"/>
        <link rel="next" title="Convolutional variational autoencoder with PyMC3 and Keras" href="convolutional_vae_keras_advi.html"/>
        <link rel="prev" title="Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3" href="lda-advi-aevb.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyMC3
          

          
          </a>

          
            
            
              <div class="version">
                3.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../examples.html">Examples</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../examples.html#howto">Howto</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#applied">Applied</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#glm">GLM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#gp">GP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples.html#mixture-models">Mixture Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../examples.html#advi">ADVI</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="GLM-hierarchical-ADVI.html">GLM: Hierarchical Linear Regression with ADVI</a></li>
<li class="toctree-l3"><a class="reference internal" href="GLM-hierarchical-advi-minibatch.html">GLM: Mini-batch ADVI on hierarchical regression model</a></li>
<li class="toctree-l3"><a class="reference internal" href="lda-advi-aevb.html">Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Variational Inference: Bayesian Neural Networks</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Current-trends-in-Machine-Learning">Current trends in Machine Learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Bayesian-Neural-Networks-in-PyMC3">Bayesian Neural Networks in PyMC3</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Lets-look-at-what-the-classifier-has-learned">Lets look at what the classifier has learned</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Mini-batch-ADVI:-Scaling-data-size">Mini-batch ADVI: Scaling data size</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Next-steps">Next steps</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Acknowledgements">Acknowledgements</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="convolutional_vae_keras_advi.html">Convolutional variational autoencoder with PyMC3 and Keras</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../api.html">API Reference</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">PyMC3</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../examples.html">Examples</a> &raquo;</li>
      
    <li>Variational Inference: Bayesian Neural Networks</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/notebooks/bayesian_neural_network_advi.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput,
div.nbinput > div,
div.nbinput div[class^=highlight],
div.nbinput div[class^=highlight] pre,
div.nboutput,
div.nboutput > div,
div.nboutput div[class^=highlight],
div.nboutput div[class^=highlight] pre {
    background: none;
    border: none;
    padding: 0 0;
    margin: 0;
    box-shadow: none;
}

/* avoid gaps between output lines */
div.nboutput div[class^=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput,
div.nboutput {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
}

/* input container */
div.nbinput {
    padding-top: 5px;
}

/* last container */
div.nblast {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput > :first-child pre {
    color: #303F9F;
}

/* output prompt */
div.nboutput > :first-child pre {
    color: #D84315;
}

/* all prompts */
div.nbinput > :first-child[class^=highlight],
div.nboutput > :first-child[class^=highlight],
div.nboutput > :first-child {
    min-width: 8ex;
    padding-top: 0.4em;
    padding-right: 0.4em;
    text-align: right;
    flex: 0;
}

/* input/output area */
div.nbinput > :nth-child(2)[class^=highlight],
div.nboutput > :nth-child(2),
div.nboutput > :nth-child(2)[class^=highlight] {
    padding: 0.4em;
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}

/* input area */
div.nbinput > :nth-child(2)[class^=highlight] {
    border: 1px solid #cfcfcf;
    border-radius: 2px;
    background: #f7f7f7;
}

/* override MathJax center alignment in output cells */
div.nboutput div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.pngmath center alignment in output cells */
div.nboutput div.math p {
    text-align: left;
}

/* standard error */
div.nboutput  > :nth-child(2).stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="Variational-Inference:-Bayesian-Neural-Networks">
<h1>Variational Inference: Bayesian Neural Networks<a class="headerlink" href="#Variational-Inference:-Bayesian-Neural-Networks" title="Permalink to this headline">¶</a></h1>
<ol class="loweralpha simple" start="3">
<li>2016 by Thomas Wiecki</li>
</ol>
<p>Original blog post:
<a class="reference external" href="http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/">http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/</a></p>
<div class="section" id="Current-trends-in-Machine-Learning">
<h2>Current trends in Machine Learning<a class="headerlink" href="#Current-trends-in-Machine-Learning" title="Permalink to this headline">¶</a></h2>
<p>There are currently three big trends in machine learning:
<strong>Probabilistic Programming</strong>, <strong>Deep Learning</strong> and &#8220;<strong>Big Data</strong>&#8221;.
Inside of PP, a lot of innovation is in making things scale using
<strong>Variational Inference</strong>. In this blog post, I will show how to use
<strong>Variational Inference</strong> in
<a class="reference external" href="http://pymc-devs.github.io/pymc3/">PyMC3</a> to fit a simple Bayesian
Neural Network. I will also discuss how bridging Probabilistic
Programming and Deep Learning can open up very interesting avenues to
explore in future research.</p>
<div class="section" id="Probabilistic-Programming-at-scale">
<h3>Probabilistic Programming at scale<a class="headerlink" href="#Probabilistic-Programming-at-scale" title="Permalink to this headline">¶</a></h3>
<p><strong>Probabilistic Programming</strong> allows very flexible creation of custom
probabilistic models and is mainly concerned with <strong>insight</strong> and
learning from your data. The approach is inherently <strong>Bayesian</strong> so we
can specify <strong>priors</strong> to inform and constrain our models and get
uncertainty estimation in form of a <strong>posterior</strong> distribution. Using
<a class="reference external" href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/">MCMC sampling
algorithms</a>
we can draw samples from this posterior to very flexibly estimate these
models. <a class="reference external" href="http://pymc-devs.github.io/pymc3/">PyMC3</a> and
<a class="reference external" href="http://mc-stan.org/">Stan</a> are the current state-of-the-art tools to
consruct and estimate these models. One major drawback of sampling,
however, is that it&#8217;s often very slow, especially for high-dimensional
models. That&#8217;s why more recently, <strong>variational inference</strong> algorithms
have been developed that are almost as flexible as MCMC but much faster.
Instead of drawing samples from the posterior, these algorithms instead
fit a distribution (e.g. normal) to the posterior turning a sampling
problem into and optimization problem.
<a class="reference external" href="http://arxiv.org/abs/1506.03431">ADVI</a> &#8211; Automatic Differentation
Variational Inference &#8211; is implemented in
<a class="reference external" href="http://pymc-devs.github.io/pymc3/">PyMC3</a> and
<a class="reference external" href="http://mc-stan.org/">Stan</a>, as well as a new package called
<a class="reference external" href="https://github.com/blei-lab/edward/">Edward</a> which is mainly
concerned with Variational Inference.</p>
<p>Unfortunately, when it comes to traditional ML problems like
classification or (non-linear) regression, Probabilistic Programming
often plays second fiddle (in terms of accuracy and scalability) to more
algorithmic approaches like <a class="reference external" href="https://en.wikipedia.org/wiki/Ensemble_learning">ensemble
learning</a> (e.g.
<a class="reference external" href="https://en.wikipedia.org/wiki/Random_forest">random forests</a> or
<a class="reference external" href="https://en.wikipedia.org/wiki/Boosting_(machine_learning)">gradient boosted regression
trees</a>.</p>
</div>
<div class="section" id="Deep-Learning">
<h3>Deep Learning<a class="headerlink" href="#Deep-Learning" title="Permalink to this headline">¶</a></h3>
<p>Now in its third renaissance, deep learning has been making headlines
repeatadly by dominating almost any object recognition benchmark,
<a class="reference external" href="https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf">kicking ass at Atari
games</a>, and <a class="reference external" href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">beating
the world-champion Lee Sedol at
Go</a>.
From a statistical point, Neural Networks are extremely good non-linear
function approximators and representation learners. While mostly known
for classification, they have been extended to unsupervised learning
with <a class="reference external" href="https://arxiv.org/abs/1312.6114">AutoEncoders</a> and in all sorts
of other interesting ways (e.g. <a class="reference external" href="https://en.wikipedia.org/wiki/Recurrent_neural_network">Recurrent
Networks</a>, or
<a class="reference external" href="http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html">MDNs</a> to
estimate multimodal distributions). Why do they work so well? No one
really knows as the statistical properties are still not fully
understood.</p>
<p>A large part of the innoviation in deep learning is the ability to train
these extremely complex models. This rests on several pillars: * Speed:
facilitating the GPU allowed for much faster processing. * Software:
frameworks like <a class="reference external" href="http://deeplearning.net/software/theano/">Theano</a>
and <a class="reference external" href="https://www.tensorflow.org/">TensorFlow</a> allow flexible creation
of abstract models that can then be optimized and compiled to CPU or
GPU. * Learning algorithms: training on sub-sets of the data &#8211;
stochastic gradient descent &#8211; allows us to train these models on
massive amounts of data. Techniques like drop-out avoid overfitting. *
Architectural: A lot of innovation comes from changing the input layers,
like for convolutional neural nets, or the output layers, like for
<a class="reference external" href="http://cbonnett.github.io/MDN_EDWARD_KERAS_TF.html">MDNs</a>.</p>
</div>
<div class="section" id="Bridging-Deep-Learning-and-Probabilistic-Programming">
<h3>Bridging Deep Learning and Probabilistic Programming<a class="headerlink" href="#Bridging-Deep-Learning-and-Probabilistic-Programming" title="Permalink to this headline">¶</a></h3>
<p>On one hand we Probabilistic Programming which allows us to build rather
small and focused models in a very principled and well-understood way to
gain insight into our data; on the other hand we have deep learning
which uses many heuristics to train huge and highly complex models that
are amazing at prediction. Recent innovations in variational inference
allow probabilistic programming to scale model complexity as well as
data size. We are thus at the cusp of being able to combine these two
approaches to hopefully unlock new innovations in Machine Learning. For
more motivation, see also <a class="reference external" href="https://twitter.com/dustinvtran">Dustin
Tran&#8217;s</a> recent <a class="reference external" href="http://dustintran.com/blog/a-quick-update-edward-and-some-motivations/">blog
post</a>.</p>
<p>While this would allow Probabilistic Programming to be applied to a much
wider set of interesting problems, I believe this bridging also holds
great promise for innovations in Deep Learning. Some ideas are: *
<strong>Uncertainty in predictions</strong>: As we will see below, the Bayesian
Neural Network informs us about the uncertainty in its predictions. I
think uncertainty is an underappreciated concept in Machine Learning as
it&#8217;s clearly important for real-world applications. But it could also be
useful in training. For example, we could train the model specifically
on samples it is most uncertain about. * <strong>Uncertainty in
representations</strong>: We also get uncertainty estimates of our weights
which could inform us about the stability of the learned representations
of the network. * <strong>Regularization with priors</strong>: Weights are often
L2-regularized to avoid overfitting, this very naturally becomes a
Gaussian prior for the weight coefficients. We could, however, imagine
all kinds of other priors, like spike-and-slab to enforce sparsity (this
would be more like using the L1-norm). * <strong>Transfer learning with
informed priors</strong>: If we wanted to train a network on a new object
recognition data set, we could bootstrap the learning by placing
informed priors centered around weights retrieved from other pre-trained
networks, like <a class="reference external" href="https://arxiv.org/abs/1409.4842">GoogLeNet</a>. *
<strong>Hierarchical Neural Networks</strong>: A very powerful approach in
Probabilistic Programming is hierarchical modeling that allows pooling
of things that were learned on sub-groups to the overall population (see
my tutorial on <a class="reference external" href="http://twiecki.github.io/blog/2014/03/17/bayesian-glms-3/">Hierarchical Linear Regression in
PyMC3</a>).
Applied to Neural Networks, in hierarchical data sets, we could train
individual neural nets to specialize on sub-groups while still being
informed about representations of the overall population. For example,
imagine a network trained to classify car models from pictures of cars.
We could train a hierarchical neural network where a sub-neural network
is trained to tell apart models from only a single manufacturer. The
intuition being that all cars from a certain manufactures share certain
similarities so it would make sense to train individual networks that
specialize on brands. However, due to the individual networks being
connected at a higher layer, they would still share information with the
other specialized sub-networks about features that are useful to all
brands. Interestingly, different layers of the network could be informed
by various levels of the hierarchy &#8211; e.g. early layers that extract
visual lines could be identical in all sub-networks while the
higher-order representations would be different. The hierarchical model
would learn all that from the data. * <strong>Other hybrid architectures</strong>:
We can more freely build all kinds of neural networks. For example,
Bayesian non-parametrics could be used to flexibly adjust the size and
shape of the hidden layers to optimally scale the network architecture
to the problem at hand during training. Currently, this requires costly
hyper-parameter optimization and a lot of tribal knowledge.</p>
</div>
</div>
<div class="section" id="Bayesian-Neural-Networks-in-PyMC3">
<h2>Bayesian Neural Networks in PyMC3<a class="headerlink" href="#Bayesian-Neural-Networks-in-PyMC3" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Generating-data">
<h3>Generating data<a class="headerlink" href="#Generating-data" title="Permalink to this headline">¶</a></h3>
<p>First, lets generate some toy data &#8211; a simple binary classification
problem that&#8217;s not linearly separable.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [1]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">theano</span>
<span class="n">theano</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">floatX</span> <span class="o">=</span> <span class="s1">&#39;float64&#39;</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">theano.tensor</span> <span class="kn">as</span> <span class="nn">T</span>
<span class="kn">import</span> <span class="nn">sklearn</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="kn">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">scale</span>
<span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_moons</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
/Users/fonnescj/anaconda3/envs/dev/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.
  &#34;This module will be removed in 0.20.&#34;, DeprecationWarning)
</pre></div></div>
</div>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [2]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">make_moons</span><span class="p">(</span><span class="n">noise</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">scale</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [3]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">Y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">Y</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 0&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">Y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">Y</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Class 1&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">();</span> <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;Toy binary classification data set&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_bayesian_neural_network_advi_6_0.png" src="../_images/notebooks_bayesian_neural_network_advi_6_0.png" />
</div>
</div>
</div>
<div class="section" id="Model-specification">
<h3>Model specification<a class="headerlink" href="#Model-specification" title="Permalink to this headline">¶</a></h3>
<p>A neural network is quite simple. The basic unit is a
<a class="reference external" href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a> which is
nothing more than <a class="reference external" href="http://pymc-devs.github.io/pymc3/notebooks/posterior_predictive.html#Prediction">logistic
regression</a>.
We use many of these in parallel and then stack them up to get hidden
layers. Here we will use 2 hidden layers with 5 neurons each which is
sufficient for such a simple problem.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [4]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># Trick: Turn inputs and outputs into shared variables.</span>
<span class="c1"># It&#39;s still the same thing, but we can later change the values of the shared variable</span>
<span class="c1"># (to switch in the test-data later) and pymc3 will just use the new data.</span>
<span class="c1"># Kind-of like a pointer we can redirect.</span>
<span class="c1"># For more info, see: http://deeplearning.net/software/theano/library/compile/shared.html</span>
<span class="n">ann_input</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">ann_output</span> <span class="o">=</span> <span class="n">theano</span><span class="o">.</span><span class="n">shared</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>

<span class="n">n_hidden</span> <span class="o">=</span> <span class="mi">5</span>

<span class="c1"># Initialize random weights between each layer</span>
<span class="n">init_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">init_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">)</span>
<span class="n">init_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">neural_network</span><span class="p">:</span>
    <span class="c1"># Weights from input to hidden layer</span>
    <span class="n">weights_in_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;w_in_1&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                             <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">n_hidden</span><span class="p">),</span>
                             <span class="n">testval</span><span class="o">=</span><span class="n">init_1</span><span class="p">)</span>

    <span class="c1"># Weights from 1st to 2nd layer</span>
    <span class="n">weights_1_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;w_1_2&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                            <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,</span> <span class="n">n_hidden</span><span class="p">),</span>
                            <span class="n">testval</span><span class="o">=</span><span class="n">init_2</span><span class="p">)</span>

    <span class="c1"># Weights from hidden layer to output</span>
    <span class="n">weights_2_out</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;w_2_out&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                              <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n_hidden</span><span class="p">,),</span>
                              <span class="n">testval</span><span class="o">=</span><span class="n">init_out</span><span class="p">)</span>

    <span class="c1"># Build neural-network using tanh activation function</span>
    <span class="n">act_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">ann_input</span><span class="p">,</span>
                         <span class="n">weights_in_1</span><span class="p">))</span>
    <span class="n">act_2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">act_1</span><span class="p">,</span>
                         <span class="n">weights_1_2</span><span class="p">))</span>
    <span class="n">act_out</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">nnet</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">act_2</span><span class="p">,</span>
                                   <span class="n">weights_2_out</span><span class="p">))</span>

    <span class="c1"># Binary classification -&gt; Bernoulli likelihood</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s1">&#39;out&#39;</span><span class="p">,</span>
                       <span class="n">act_out</span><span class="p">,</span>
                       <span class="n">observed</span><span class="o">=</span><span class="n">ann_output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>That&#8217;s not so bad. The <code class="docutils literal"><span class="pre">Normal</span></code> priors help regularize the weights.
Usually we would add a constant <code class="docutils literal"><span class="pre">b</span></code> to the inputs but I omitted it
here to keep the code cleaner.</p>
</div>
<div class="section" id="Variational-Inference:-Scaling-model-complexity">
<h3>Variational Inference: Scaling model complexity<a class="headerlink" href="#Variational-Inference:-Scaling-model-complexity" title="Permalink to this headline">¶</a></h3>
<p>We could now just run a MCMC sampler like
<code class="docutils literal"><span class="pre">`NUTS</span></code> &lt;<a class="reference external" href="http://pymc-devs.github.io/pymc3/api.html#nuts">http://pymc-devs.github.io/pymc3/api.html#nuts</a>&gt;`__ which
works pretty well in this case but as I already mentioned, this will
become very slow as we scale our model up to deeper architectures with
more layers.</p>
<p>Instead, we will use the brand-new
<a class="reference external" href="http://pymc-devs.github.io/pymc3/api.html#advi">ADVI</a> variational
inference algorithm which was recently added to <code class="docutils literal"><span class="pre">PyMC3</span></code>. This is much
faster and will scale better. Note, that this is a mean-field
approximation so we ignore correlations in the posterior.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [5]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="k">time</span>

with neural_network:
    # Run ADVI which returns posterior means, standard deviations, and the evidence lower bound (ELBO)
    v_params = pm.variational.advi(n=50000)
</pre></div>
</div>
</div>
<div class="nboutput container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Average ELBO = -131.71: 100%|██████████| 50000/50000 [00:25&lt;00:00, 1965.31it/s]
Finished [100%]: Average ELBO = -129.04
</pre></div></div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
CPU times: user 25.3 s, sys: 685 ms, total: 26 s
Wall time: 29.4 s
</pre></div></div>
</div>
<p>&lt; 40 seconds on my older laptop. That&#8217;s pretty good considering that
NUTS is having a really hard time. Further below we make this even
faster. To make it really fly, we probably want to run the Neural
Network on the GPU.</p>
<p>As samples are more convenient to work with, we can very quickly draw
samples from the variational posterior using <code class="docutils literal"><span class="pre">sample_vp()</span></code> (this is
just sampling from Normal distributions, so not at all the same like
MCMC):</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [6]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">neural_network</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">variational</span><span class="o">.</span><span class="n">sample_vp</span><span class="p">(</span><span class="n">v_params</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
100%|██████████| 5000/5000 [00:00&lt;00:00, 9737.59it/s]
</pre></div></div>
</div>
<p>Plotting the objective function (ELBO) we can see that the optimization
slowly improves the fit over time.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [7]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">v_params</span><span class="o">.</span><span class="n">elbo_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ELBO&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>Out[7]:
</pre></div>
</div>
<div class="highlight-none"><div class="highlight"><pre>
<span></span>&lt;matplotlib.text.Text at 0x116334438&gt;
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_bayesian_neural_network_advi_15_1.png" src="../_images/notebooks_bayesian_neural_network_advi_15_1.png" />
</div>
</div>
<p>Now that we trained our model, lets predict on the hold-out set using a
posterior predictive check (PPC). We use
<code class="docutils literal"><span class="pre">`sample_ppc()</span></code> &lt;<a class="reference external" href="http://pymc-devs.github.io/pymc3/api.html#pymc3.sampling.sample_ppc">http://pymc-devs.github.io/pymc3/api.html#pymc3.sampling.sample_ppc</a>&gt;`__
to generate new data (in this case class predictions) from the posterior
(sampled from the variational estimation).</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [8]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># Replace shared variables with testing set</span>
<span class="n">ann_input</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">ann_output</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">Y_test</span><span class="p">)</span>

<span class="c1"># Creater posterior predictive samples</span>
<span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">neural_network</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="c1"># Use probability of &gt; 0.5 to assume prediction of class 1</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.5</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
100%|██████████| 500/500 [00:05&lt;00:00, 94.45it/s]
</pre></div></div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [9]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s1">&#39;Predicted labels in testing set&#39;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_bayesian_neural_network_advi_18_0.png" src="../_images/notebooks_bayesian_neural_network_advi_18_0.png" />
</div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [10]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">print</span><span class="p">(</span><span class="s1">&#39;Accuracy = {}%&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">((</span><span class="n">Y_test</span> <span class="o">==</span> <span class="n">pred</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="mi">100</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
Accuracy = 96.2%
</pre></div></div>
</div>
<p>Hey, our neural network did all right!</p>
</div>
</div>
<div class="section" id="Lets-look-at-what-the-classifier-has-learned">
<h2>Lets look at what the classifier has learned<a class="headerlink" href="#Lets-look-at-what-the-classifier-has-learned" title="Permalink to this headline">¶</a></h2>
<p>For this, we evaluate the class probability predictions on a grid over
the whole input space.</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [11]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:</span><span class="mi">100j</span><span class="p">,</span><span class="o">-</span><span class="mi">3</span><span class="p">:</span><span class="mi">3</span><span class="p">:</span><span class="mi">100j</span><span class="p">]</span>
<span class="n">grid_2d</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">dummy_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">grid</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [12]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">ann_input</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">grid_2d</span><span class="p">)</span>
<span class="n">ann_output</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">dummy_out</span><span class="p">)</span>

<span class="c1"># Creater posterior predictive samples</span>
<span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_ppc</span><span class="p">(</span><span class="n">trace</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">neural_network</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
100%|██████████| 500/500 [00:08&lt;00:00, 57.93it/s]
</pre></div></div>
</div>
<div class="section" id="Probability-surface">
<h3>Probability surface<a class="headerlink" href="#Probability-surface" title="Permalink to this headline">¶</a></h3>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [13]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">diverging_palette</span><span class="p">(</span><span class="mi">250</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">85</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">contour</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="o">*</span><span class="n">grid</span><span class="p">,</span> <span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Posterior predictive mean probability of class label = 0&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_bayesian_neural_network_advi_25_0.png" src="../_images/notebooks_bayesian_neural_network_advi_25_0.png" />
</div>
</div>
</div>
<div class="section" id="Uncertainty-in-predicted-value">
<h3>Uncertainty in predicted value<a class="headerlink" href="#Uncertainty-in-predicted-value" title="Permalink to this headline">¶</a></h3>
<p>So far, everything I showed we could have done with a non-Bayesian
Neural Network. The mean of the posterior predictive for each
class-label should be identical to maximum likelihood predicted values.
However, we can also look at the standard deviation of the posterior
predictive to get a sense for the uncertainty in our predictions. Here
is what that looks like:</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [14]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">cmap</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">cubehelix_palette</span><span class="p">(</span><span class="n">light</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">as_cmap</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">contour</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="o">*</span><span class="n">grid</span><span class="p">,</span> <span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;out&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_test</span><span class="p">[</span><span class="n">pred</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">cbar</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">(</span><span class="n">contour</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">_</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">xlim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">xlabel</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">);</span>
<span class="n">cbar</span><span class="o">.</span><span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Uncertainty (posterior predictive standard deviation)&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_bayesian_neural_network_advi_27_0.png" src="../_images/notebooks_bayesian_neural_network_advi_27_0.png" />
</div>
</div>
<p>We can see that very close to the decision boundary, our uncertainty as
to which label to predict is highest. You can imagine that associating
predictions with uncertainty is a critical property for many
applications like health care. To further maximize accuracy, we might
want to train the model primarily on samples from that high-uncertainty
region.</p>
</div>
</div>
<div class="section" id="Mini-batch-ADVI:-Scaling-data-size">
<h2>Mini-batch ADVI: Scaling data size<a class="headerlink" href="#Mini-batch-ADVI:-Scaling-data-size" title="Permalink to this headline">¶</a></h2>
<p>So far, we have trained our model on all data at once. Obviously this
won&#8217;t scale to something like ImageNet. Moreover, training on
mini-batches of data (stochastic gradient descent) avoids local minima
and can lead to faster convergence.</p>
<p>Fortunately, ADVI can be run on mini-batches as well. It just requires
some setting up:</p>
<div class="nbinput nblast container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [15]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="c1"># Set back to original data to retrain</span>
<span class="n">ann_input</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">ann_output</span><span class="o">.</span><span class="n">set_value</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>

<span class="c1"># Tensors and RV that will be using mini-batches</span>
<span class="n">minibatch_tensors</span> <span class="o">=</span> <span class="p">[</span><span class="n">ann_input</span><span class="p">,</span> <span class="n">ann_output</span><span class="p">]</span>
<span class="n">minibatch_RVs</span> <span class="o">=</span> <span class="p">[</span><span class="n">out</span><span class="p">]</span>

<span class="c1"># Generator that returns mini-batches in each iteration</span>
<span class="k">def</span> <span class="nf">create_minibatch</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
        <span class="c1"># Return random data samples of set size 100 each iteration</span>
        <span class="n">ixs</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
        <span class="k">yield</span> <span class="n">data</span><span class="p">[</span><span class="n">ixs</span><span class="p">]</span>

<span class="n">minibatches</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span>
    <span class="n">create_minibatch</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span>
    <span class="n">create_minibatch</span><span class="p">(</span><span class="n">Y_train</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">total_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">Y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>While the above might look a bit daunting, I really like the design.
Especially the fact that you define a generator allows for great
flexibility. In principle, we could just pool from a database there and
not have to keep all the data in RAM.</p>
<p>Lets pass those to <code class="docutils literal"><span class="pre">advi_minibatch()</span></code>:</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [16]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="o">%%</span><span class="k">time</span>

with neural_network:
    # Run advi_minibatch
    v_params = pm.variational.advi_minibatch(
        n=50000, minibatch_tensors=minibatch_tensors,
        minibatch_RVs=minibatch_RVs, minibatches=minibatches,
        total_size=total_size, learning_rate=1e-2, epsilon=1.0
    )
</pre></div>
</div>
</div>
<div class="nboutput container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
Average ELBO = -120.61: 100%|██████████| 50000/50000 [00:31&lt;00:00, 1582.90it/s]
Finished minibatch ADVI: ELBO = -120.17
</pre></div></div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<div class="highlight"><pre>
CPU times: user 35.9 s, sys: 676 ms, total: 36.6 s
Wall time: 43.9 s
</pre></div></div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [17]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="k">with</span> <span class="n">neural_network</span><span class="p">:</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">variational</span><span class="o">.</span><span class="n">sample_vp</span><span class="p">(</span><span class="n">v_params</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="stderr container">
<div class="highlight"><pre>
100%|██████████| 5000/5000 [00:00&lt;00:00, 7864.25it/s]
</pre></div></div>
</div>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [18]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">v_params</span><span class="o">.</span><span class="n">elbo_vals</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;ELBO&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;iteration&#39;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_bayesian_neural_network_advi_34_0.png" src="../_images/notebooks_bayesian_neural_network_advi_34_0.png" />
</div>
</div>
<p>As you can see, mini-batch ADVI&#8217;s running time is much lower. It also
seems to converge faster.</p>
<p>For fun, we can also look at the trace. The point is that we also get
uncertainty of our Neural Network weights.</p>
<div class="nbinput container">
<div class="highlight-none"><div class="highlight"><pre>
<span></span>In [19]:
</pre></div>
</div>
<div class="highlight-ipython3"><div class="highlight"><pre>
<span></span><span class="n">pm</span><span class="o">.</span><span class="n">traceplot</span><span class="p">(</span><span class="n">trace</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast container">
<div class="container">
</div>
<div class="container">
<img alt="../_images/notebooks_bayesian_neural_network_advi_36_0.png" src="../_images/notebooks_bayesian_neural_network_advi_36_0.png" />
</div>
</div>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h2>
<p>Hopefully this blog post demonstrated a very powerful new inference
algorithm available in <a class="reference external" href="http://pymc-devs.github.io/pymc3/">PyMC3</a>:
<a class="reference external" href="http://pymc-devs.github.io/pymc3/api.html#advi">ADVI</a>. I also think
bridging the gap between Probabilistic Programming and Deep Learning can
open up many new avenues for innovation in this space, as discussed
above. Specifically, a hierarchical neural network sounds pretty
bad-ass. These are really exciting times.</p>
</div>
<div class="section" id="Next-steps">
<h2>Next steps<a class="headerlink" href="#Next-steps" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal"><span class="pre">`Theano</span></code> &lt;<a class="reference external" href="http://deeplearning.net/software/theano/">http://deeplearning.net/software/theano/</a>&gt;`__, which is used
by <code class="docutils literal"><span class="pre">PyMC3</span></code> as its computational backend, was mainly developed for
estimating neural networks and there are great libraries like
<code class="docutils literal"><span class="pre">`Lasagne</span></code> &lt;<a class="reference external" href="https://github.com/Lasagne/Lasagne">https://github.com/Lasagne/Lasagne</a>&gt;`__ that build on top
of <code class="docutils literal"><span class="pre">Theano</span></code> to make construction of the most common neural network
architectures easy. Ideally, we wouldn&#8217;t have to build the models by
hand as I did above, but use the convenient syntax of <code class="docutils literal"><span class="pre">Lasagne</span></code> to
construct the architecture, define our priors, and run ADVI.</p>
<p>While we haven&#8217;t successfully run <code class="docutils literal"><span class="pre">PyMC3</span></code> on the GPU yet, it should be
fairly straight forward (this is what <code class="docutils literal"><span class="pre">Theano</span></code> does after all) and
further reduce the running time significantly. If you know some
<code class="docutils literal"><span class="pre">Theano</span></code>, this would be a great area for contributions!</p>
<p>You might also argue that the above network isn&#8217;t really deep, but note
that we could easily extend it to have more layers, including
convolutional ones to train on more challenging data sets.</p>
<p>I also presented some of this work at PyData London, view the video
below:</p>
<p>Finally, you can download this NB
<a class="reference external" href="https://github.com/twiecki/WhileMyMCMCGentlySamples/blob/master/content/downloads/notebooks/bayesian_neural_network.ipynb">here</a>.
Leave a comment below, and <a class="reference external" href="https://twitter.com/twiecki">follow me on
twitter</a>.</p>
</div>
<div class="section" id="Acknowledgements">
<h2>Acknowledgements<a class="headerlink" href="#Acknowledgements" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://github.com/taku-y">Taku Yoshioka</a> did a lot of work on ADVI
in PyMC3, including the mini-batch implementation as well as the
sampling from the variational posterior. I&#8217;d also like to the thank the
Stan guys (specifically Alp Kucukelbir and Daniel Lee) for deriving ADVI
and teaching us about it. Thanks also to Chris Fonnesbeck, Andrew
Campbell, Taku Yoshioka, and Peadar Coyle for useful comments on an
earlier draft.</p>
</div>
</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="convolutional_vae_keras_advi.html" class="btn btn-neutral float-right" title="Convolutional variational autoencoder with PyMC3 and Keras" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="lda-advi-aevb.html" class="btn btn-neutral" title="Automatic autoencoding variational Bayes for latent dirichlet allocation with PyMC3" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, John Salvatier, Christopher Fonnesbeck, Thomas Wiecki.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'3.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>