

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Inference &mdash; PyMC3 3.1rc3 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="PyMC3 3.1rc3 documentation" href="../index.html"/>
        <link rel="up" title="API Reference" href="../api.html"/>
        <link rel="next" title="Generalized Linear Models" href="glm.html"/>
        <link rel="prev" title="Mixture" href="distributions/mixture.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> PyMC3
          

          
          </a>

          
            
            
              <div class="version">
                3.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="distributions.html">Distributions</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Inference</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#module-pymc3.sampling">Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-methods">Step-methods</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-pymc3.step_methods.hmc.nuts">NUTS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-pymc3.step_methods.metropolis">Metropolis</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-pymc3.step_methods.slicer">Slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="#hamiltonian-monte-carlo">Hamiltonian Monte Carlo</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#variational">Variational</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#module-pymc3.variational.opvi">OPVI</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-pymc3.variational.approximations">Approximations</a></li>
<li class="toctree-l4"><a class="reference internal" href="#module-pymc3.variational.operators">Operators</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="glm.html">Generalized Linear Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="gp.html">Gaussian Processes</a></li>
<li class="toctree-l2"><a class="reference internal" href="plots.html">Plots</a></li>
<li class="toctree-l2"><a class="reference internal" href="stats.html">Stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="diagnostics.html">Diagnostics</a></li>
<li class="toctree-l2"><a class="reference internal" href="backends.html">Backends</a></li>
<li class="toctree-l2"><a class="reference internal" href="math.html">Math</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">Data</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">PyMC3</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../api.html">API Reference</a> &raquo;</li>
        
      <li>Inference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/api/inference.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast,
.nboutput.nblast {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast + .nbinput {
    margin-top: -19px;
}

/* nice headers on first paragraph of info/warning boxes */
.admonition .first {
    margin: -12px;
    padding: 6px 12px;
    margin-bottom: 12px;
    color: #fff;
    line-height: 1;
    display: block;
}
.admonition.warning .first {
    background: #f0b37e;
}
.admonition.note .first {
    background: #6ab0de;
}
.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}
</style>
<div class="section" id="inference">
<h1>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-pymc3.sampling">
<span id="sampling"></span><h2>Sampling<a class="headerlink" href="#module-pymc3.sampling" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="pymc3.sampling.sample">
<code class="descclassname">pymc3.sampling.</code><code class="descname">sample</code><span class="sig-paren">(</span><em>draws=500</em>, <em>step=None</em>, <em>init='auto'</em>, <em>n_init=200000</em>, <em>start=None</em>, <em>trace=None</em>, <em>chain=0</em>, <em>njobs=1</em>, <em>tune=500</em>, <em>nuts_kwargs=None</em>, <em>step_kwargs=None</em>, <em>progressbar=True</em>, <em>model=None</em>, <em>random_seed=-1</em>, <em>live_plot=False</em>, <em>discard_tuned_samples=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from the posterior using the given step methods.</p>
<p>Multiple step methods are supported via compound step methods.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>draws</strong> (<em>int</em>) &#8211; The number of samples to draw. Defaults to 500. The number of tuned
samples are discarded by default. See discard_tuned_samples.</li>
<li><strong>step</strong> (<em>function</em><em> or </em><em>iterable of functions</em>) &#8211; A step function or collection of functions. If there are variables
without a step methods, step methods for those variables will
be assigned automatically.</li>
<li><strong>init</strong> (<em>str {'ADVI'</em><em>, </em><em>'ADVI_MAP'</em><em>, </em><em>'MAP'</em><em>, </em><em>'NUTS'</em><em>, </em><em>'auto'</em><em>, </em><em>None}</em>) &#8211; <p>Initialization method to use. Only works for auto-assigned step methods.</p>
<ul>
<li>ADVI: Run ADVI to estimate starting points and diagonal covariance
matrix. If njobs &gt; 1 it will sample starting points from the estimated
posterior, otherwise it will use the estimated posterior mean.</li>
<li>ADVI_MAP: Initialize ADVI with MAP and use MAP as starting point.</li>
<li>MAP: Use the MAP as starting point.</li>
<li>NUTS: Run NUTS to estimate starting points and covariance matrix. If
njobs &gt; 1 it will sample starting points from the estimated posterior,
otherwise it will use the estimated posterior mean.</li>
<li>auto : Auto-initialize, if possible. Currently only works when NUTS
is auto-assigned as step method (default).</li>
<li>None: Do not initialize.</li>
</ul>
</li>
<li><strong>n_init</strong> (<em>int</em>) &#8211; Number of iterations of initializer
If &#8216;ADVI&#8217;, number of iterations, if &#8216;nuts&#8217;, number of draws.</li>
<li><strong>start</strong> (<em>dict</em>) &#8211; Starting point in parameter space (or partial point)
Defaults to trace.point(-1)) if there is a trace provided and
model.test_point if not (defaults to empty dict).</li>
<li><strong>trace</strong> (<em>backend</em><em>, </em><em>list</em><em>, or </em><em>MultiTrace</em>) &#8211; This should be a backend instance, a list of variables to track,
or a MultiTrace object with past values. If a MultiTrace object
is given, it must contain samples for the chain number <cite>chain</cite>.
If None or a list of variables, the NDArray backend is used.
Passing either &#8220;text&#8221; or &#8220;sqlite&#8221; is taken as a shortcut to set
up the corresponding backend (with &#8220;mcmc&#8221; used as the base
name).</li>
<li><strong>chain</strong> (<em>int</em>) &#8211; Chain number used to store sample in backend. If <cite>njobs</cite> is
greater than one, chain numbers will start here.</li>
<li><strong>njobs</strong> (<em>int</em>) &#8211; Number of parallel jobs to start. If None, set to number of cpus
in the system - 2.</li>
<li><strong>tune</strong> (<em>int</em>) &#8211; Number of iterations to tune, if applicable (defaults to 500).
These samples will be drawn in addition to samples and discarded
unless discard_tuned_samples is set to True.</li>
<li><strong>nuts_kwargs</strong> (<em>dict</em>) &#8211; <p>Options for the NUTS sampler. See the docstring of NUTS
for a complete list of options. Common options are</p>
<ul>
<li>target_accept: float in [0, 1]. The step size is tuned such
that we approximate this acceptance rate. Higher values like 0.9
or 0.95 often work better for problematic posteriors.</li>
<li>max_treedepth: The maximum depth of the trajectory tree.</li>
<li>step_scale: float, default 0.25
The initial guess for the step size scaled down by <cite>1/n**(1/4)</cite>.</li>
</ul>
<p>If you want to pass options to other step methods, please use
<cite>step_kwargs</cite>.</p>
</li>
<li><strong>step_kwargs</strong> (<em>dict</em>) &#8211; Options for step methods. Keys are the lower case names of
the step method, values are dicts of keyword arguments.
You can find a full list of arguments in the docstring of
the step methods. If you want to pass arguments only to nuts,
you can use <cite>nuts_kwargs</cite>.</li>
<li><strong>progressbar</strong> (<em>bool</em>) &#8211; Whether or not to display a progress bar in the command line. The
bar shows the percentage of completion, the sampling speed in
samples per second (SPS), and the estimated remaining time until
completion (&#8220;expected time of arrival&#8221;; ETA).</li>
<li><strong>model</strong> (Model (optional if in <cite>with</cite> context)) &#8211; </li>
<li><strong>random_seed</strong> (<em>int</em><em> or </em><em>list of ints</em>) &#8211; A list is accepted if more if <cite>njobs</cite> is greater than one.</li>
<li><strong>live_plot</strong> (<em>bool</em>) &#8211; Flag for live plotting the trace while sampling</li>
<li><strong>discard_tuned_samples</strong> (<em>bool</em>) &#8211; Whether to discard posterior samples of the tune interval.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>trace</strong> (<em>pymc3.backends.base.MultiTrace</em>) &#8211; A <cite>MultiTrace</cite> object that contains the samples.</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">pymc3</span> <span class="kn">as</span> <span class="nn">pm</span>
<span class="o">...</span> <span class="n">n</span> <span class="o">=</span> <span class="mi">100</span>
<span class="o">...</span> <span class="n">h</span> <span class="o">=</span> <span class="mi">61</span>
<span class="o">...</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mi">2</span>
<span class="o">...</span> <span class="n">beta</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<div class="highlight-ipython"><div class="highlight"><pre><span></span><span class="o">&gt;&gt;&gt;</span> <span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span> <span class="c1"># context management</span>
<span class="o">...</span>     <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="n">beta</span><span class="p">)</span>
<span class="o">...</span>     <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">h</span><span class="p">)</span>
<span class="o">...</span>     <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">njobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pm</span><span class="o">.</span><span class="n">df_summary</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
       <span class="n">mean</span>        <span class="n">sd</span>  <span class="n">mc_error</span>   <span class="n">hpd_2</span><span class="o">.</span><span class="mi">5</span>  <span class="n">hpd_97</span><span class="o">.</span><span class="mi">5</span>
<span class="n">p</span>  <span class="mf">0.604625</span>  <span class="mf">0.047086</span>   <span class="mf">0.00078</span>  <span class="mf">0.510498</span>  <span class="mf">0.694774</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.sampling.iter_sample">
<code class="descclassname">pymc3.sampling.</code><code class="descname">iter_sample</code><span class="sig-paren">(</span><em>draws</em>, <em>step</em>, <em>start=None</em>, <em>trace=None</em>, <em>chain=0</em>, <em>tune=None</em>, <em>model=None</em>, <em>random_seed=-1</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.iter_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Generator that returns a trace on each iteration using the given
step method.  Multiple step methods supported via compound step
method returns the amount of time taken.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>draws</strong> (<em>int</em>) &#8211; The number of samples to draw</li>
<li><strong>step</strong> (<em>function</em>) &#8211; Step function</li>
<li><strong>start</strong> (<em>dict</em>) &#8211; Starting point in parameter space (or partial point)
Defaults to trace.point(-1)) if there is a trace provided and
model.test_point if not (defaults to empty dict)</li>
<li><strong>trace</strong> (<em>backend</em><em>, </em><em>list</em><em>, or </em><em>MultiTrace</em>) &#8211; This should be a backend instance, a list of variables to track,
or a MultiTrace object with past values. If a MultiTrace object
is given, it must contain samples for the chain number <cite>chain</cite>.
If None or a list of variables, the NDArray backend is used.</li>
<li><strong>chain</strong> (<em>int</em>) &#8211; Chain number used to store sample in backend. If <cite>njobs</cite> is
greater than one, chain numbers will start here.</li>
<li><strong>tune</strong> (<em>int</em>) &#8211; Number of iterations to tune, if applicable (defaults to None)</li>
<li><strong>model</strong> (Model (optional if in <cite>with</cite> context)) &#8211; </li>
<li><strong>random_seed</strong> (<em>int</em><em> or </em><em>list of ints</em>) &#8211; A list is accepted if more if <cite>njobs</cite> is greater than one.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Example</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">trace</span> <span class="ow">in</span> <span class="n">iter_sample</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
</dd></dl>

<dl class="function">
<dt id="pymc3.sampling.sample_ppc">
<code class="descclassname">pymc3.sampling.</code><code class="descname">sample_ppc</code><span class="sig-paren">(</span><em>trace</em>, <em>samples=None</em>, <em>model=None</em>, <em>vars=None</em>, <em>size=None</em>, <em>random_seed=None</em>, <em>progressbar=True</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.sample_ppc" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate posterior predictive samples from a model given a trace.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>trace</strong> (<em>backend</em><em>, </em><em>list</em><em>, or </em><em>MultiTrace</em>) &#8211; Trace generated from MCMC sampling</li>
<li><strong>samples</strong> (<em>int</em>) &#8211; Number of posterior predictive samples to generate. Defaults to the
length of <cite>trace</cite></li>
<li><strong>model</strong> (Model (optional if in <cite>with</cite> context)) &#8211; Model used to generate <cite>trace</cite></li>
<li><strong>vars</strong> (<em>iterable</em>) &#8211; Variables for which to compute the posterior predictive samples.
Defaults to <cite>model.observed_RVs</cite>.</li>
<li><strong>size</strong> (<em>int</em>) &#8211; The number of random draws from the distribution specified by the
parameters in each sample of the trace.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>samples</strong> (<em>dict</em>) &#8211; Dictionary with the variables as keys. The values corresponding
to the posterior predictive samples.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="pymc3.sampling.init_nuts">
<code class="descclassname">pymc3.sampling.</code><code class="descname">init_nuts</code><span class="sig-paren">(</span><em>init='ADVI'</em>, <em>njobs=1</em>, <em>n_init=500000</em>, <em>model=None</em>, <em>random_seed=-1</em>, <em>progressbar=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.sampling.init_nuts" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize and sample from posterior of a continuous model.</p>
<p>This is a convenience function. NUTS convergence and sampling speed is extremely
dependent on the choice of mass/scaling matrix. In our experience, using ADVI
to estimate a diagonal covariance matrix and using this as the scaling matrix
produces robust results over a wide class of continuous models.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>init</strong> (<em>str {'ADVI'</em><em>, </em><em>'ADVI_MAP'</em><em>, </em><em>'MAP'</em><em>, </em><em>'NUTS'}</em>) &#8211; Initialization method to use.
* ADVI : Run ADVI to estimate posterior mean and diagonal covariance matrix.
* ADVI_MAP: Initialize ADVI with MAP and use MAP as starting point.
* MAP : Use the MAP as starting point.
* NUTS : Run NUTS and estimate posterior mean and covariance matrix.</li>
<li><strong>njobs</strong> (<em>int</em>) &#8211; Number of parallel jobs to start.</li>
<li><strong>n_init</strong> (<em>int</em>) &#8211; Number of iterations of initializer
If &#8216;ADVI&#8217;, number of iterations, if &#8216;metropolis&#8217;, number of draws.</li>
<li><strong>model</strong> (Model (optional if in <cite>with</cite> context)) &#8211; </li>
<li><strong>progressbar</strong> (<em>bool</em>) &#8211; Whether or not to display a progressbar for advi sampling.</li>
<li><strong>**kwargs</strong> (<em>keyword arguments</em>) &#8211; Extra keyword arguments are forwarded to pymc3.NUTS.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><ul class="simple">
<li><strong>start</strong> (<em>pymc3.model.Point</em>) &#8211; Starting point for sampler</li>
<li><strong>nuts_sampler</strong> (<em>pymc3.step_methods.NUTS</em>) &#8211; Instantiated and initialized NUTS sampler object</li>
</ul>
</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="step-methods">
<h2>Step-methods<a class="headerlink" href="#step-methods" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-pymc3.step_methods.hmc.nuts">
<span id="nuts"></span><h3>NUTS<a class="headerlink" href="#module-pymc3.step_methods.hmc.nuts" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pymc3.step_methods.hmc.nuts.NUTS">
<em class="property">class </em><code class="descclassname">pymc3.step_methods.hmc.nuts.</code><code class="descname">NUTS</code><span class="sig-paren">(</span><em>vars=None</em>, <em>Emax=1000</em>, <em>target_accept=0.8</em>, <em>gamma=0.05</em>, <em>k=0.75</em>, <em>t0=10</em>, <em>adapt_step_size=True</em>, <em>max_treedepth=10</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.hmc.nuts.NUTS" title="Permalink to this definition">¶</a></dt>
<dd><p>A sampler for continuous variables based on Hamiltonian mechanics.</p>
<p>NUTS automatically tunes the step size and the number of steps per
sample. A detailed description can be found at [1], &#8220;Algorithm 6:
Efficient No-U-Turn Sampler with Dual Averaging&#8221;.</p>
<p>Nuts provides a number of statistics that can be accessed with
<cite>trace.get_sampler_stats</cite>:</p>
<ul class="simple">
<li><cite>mean_tree_accept</cite>: The mean acceptance probability for the tree
that generated this sample. The mean of these values across all
samples but the burn-in should be approximately <cite>target_accept</cite>
(the default for this is 0.8).</li>
<li><cite>diverging</cite>: Whether the trajectory for this sample diverged. If
there are any divergences after burnin, this indicates that
the results might not be reliable. Reparametrization can
often help, but you can also try to increase <cite>target_accept</cite> to
something like 0.9 or 0.95.</li>
<li><cite>energy</cite>: The energy at the point in phase-space where the sample
was accepted. This can be used to identify posteriors with
problematically long tails. See below for an example.</li>
<li><cite>energy_change</cite>: The difference in energy between the start and
the end of the trajectory. For a perfect integrator this would
always be zero.</li>
<li><cite>max_energy_change</cite>: The maximum difference in energy along the
whole trajectory.</li>
<li><cite>depth</cite>: The depth of the tree that was used to generate this sample</li>
<li><cite>tree_size</cite>: The number of leafs of the sampling tree, when the
sample was accepted. This is usually a bit less than
<cite>2 ** depth</cite>. If the tree size is large, the sampler is
using a lot of leapfrog steps to find the next sample. This can for
example happen if there are strong correlations in the posterior,
if the posterior has long tails, if there are regions of high
curvature (&#8220;funnels&#8221;), or if the variance estimates in the mass
matrix are inaccurate. Reparametrisation of the model or estimating
the posterior variances from past samples might help.</li>
<li><cite>tune</cite>: This is <cite>True</cite>, if step size adaptation was turned on when
this sample was generated.</li>
<li><cite>step_size</cite>: The step size used for this sample.</li>
<li><cite>step_size_bar</cite>: The current best known step-size. After the tuning
samples, the step size is set to this value. This should converge
during tuning.</li>
</ul>
<p class="rubric">References</p>
<table class="docutils footnote" frame="void" id="id1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Hoffman, Matthew D., &amp; Gelman, Andrew. (2011). The No-U-Turn Sampler:
Adaptively Setting Path Lengths in Hamiltonian Monte Carlo.</td></tr>
</tbody>
</table>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vars</strong> (<em>list of Theano variables</em><em>, </em><em>default all continuous vars</em>) &#8211; </li>
<li><strong>Emax</strong> (<em>float</em><em>, </em><em>default 1000</em>) &#8211; Maximum energy change allowed during leapfrog steps. Larger
deviations will abort the integration.</li>
<li><strong>target_accept</strong> (<em>float</em><em> (</em><em>0</em><em>,</em><em>1</em><em>)</em><em></em><em>, </em><em>default .8</em>) &#8211; Try to find a step size such that the average acceptance
probability across the trajectories are close to target_accept.
Higher values for target_accept lead to smaller step sizes.</li>
<li><strong>step_scale</strong> (<em>float</em><em>, </em><em>default 0.25</em>) &#8211; Size of steps to take, automatically scaled down by <cite>1/n**(1/4)</cite>.
If step size adaptation is switched off, the resulting step size
is used. If adaptation is enabled, it is used as initial guess.</li>
<li><strong>gamma</strong> (<em>float</em><em>, </em><em>default .05</em>) &#8211; </li>
<li><strong>k</strong> (<em>float</em><em> (</em><em>5</em><em>,</em><em>1</em><em>) </em><em>default .75</em>) &#8211; scaling of speed of adaptation</li>
<li><strong>t0</strong> (<em>int</em><em>, </em><em>default 10</em>) &#8211; slows initial adaptation</li>
<li><strong>adapt_step_size</strong> (<em>bool</em><em>, </em><em>default=True</em>) &#8211; Whether step size adaptation should be enabled. If this is
disabled, <cite>k</cite>, <cite>t0</cite>, <cite>gamma</cite> and <cite>target_accept</cite> are ignored.</li>
<li><strong>max_treedepth</strong> (<em>int</em><em>, </em><em>default=10</em>) &#8211; The maximum tree depth. Trajectories are stoped when this
depth is reached.</li>
<li><strong>integrator</strong> (<em>str</em><em>, </em><em>default &quot;leapfrog&quot;</em>) &#8211; The integrator to use for the trajectories. One of &#8220;leapfrog&#8221;,
&#8220;two-stage&#8221; or &#8220;three-stage&#8221;. The second two can increase
sampling speed for some high dimensional problems.</li>
<li><strong>scaling</strong> (<em>array_like</em><em>, </em><em>ndim = {1</em><em>,</em><em>2}</em>) &#8211; The inverse mass, or precision matrix. One dimensional arrays are
interpreted as diagonal matrices. If <cite>is_cov</cite> is set to True,
this will be interpreded as the mass or covariance matrix.</li>
<li><strong>is_cov</strong> (<em>bool</em><em>, </em><em>default=False</em>) &#8211; Treat the scaling as mass or covariance matrix.</li>
<li><strong>potential</strong> (<em>Potential</em><em>, </em><em>optional</em>) &#8211; An object that represents the Hamiltonian with methods <cite>velocity</cite>,
<cite>energy</cite>, and <cite>random</cite> methods. It can be specified instead
of the scaling matrix.</li>
<li><strong>model</strong> (<em>pymc3.Model</em>) &#8211; The model</li>
<li><strong>kwargs</strong> (<em>passed to BaseHMC</em>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>The step size adaptation stops when <cite>self.tune</cite> is set to False.
This is usually achieved by setting the <cite>tune</cite> parameter if
<cite>pm.sample</cite> to the desired number of tuning steps.</p>
<dl class="method">
<dt id="pymc3.step_methods.hmc.nuts.NUTS.check_trace">
<code class="descname">check_trace</code><span class="sig-paren">(</span><em>strace</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.hmc.nuts.NUTS.check_trace" title="Permalink to this definition">¶</a></dt>
<dd><p>Print warnings for obviously problematic chains.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pymc3.step_methods.metropolis">
<span id="metropolis"></span><h3>Metropolis<a class="headerlink" href="#module-pymc3.step_methods.metropolis" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pymc3.step_methods.metropolis.Metropolis">
<em class="property">class </em><code class="descclassname">pymc3.step_methods.metropolis.</code><code class="descname">Metropolis</code><span class="sig-paren">(</span><em>vars=None</em>, <em>S=None</em>, <em>proposal_dist=None</em>, <em>scaling=1.0</em>, <em>tune=True</em>, <em>tune_interval=100</em>, <em>model=None</em>, <em>mode=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.Metropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>Metropolis-Hastings sampling step</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vars</strong> (<em>list</em>) &#8211; List of variables for sampler</li>
<li><strong>S</strong> (<em>standard deviation</em><em> or </em><em>covariance matrix</em>) &#8211; Some measure of variance to parameterize proposal distribution</li>
<li><strong>proposal_dist</strong> (<em>function</em>) &#8211; Function that returns zero-mean deviates when parameterized with
S (and n). Defaults to normal.</li>
<li><strong>scaling</strong> (<em>scalar</em><em> or </em><em>array</em>) &#8211; Initial scale factor for proposal. Defaults to 1.</li>
<li><strong>tune</strong> (<em>bool</em>) &#8211; Flag for tuning. Defaults to True.</li>
<li><strong>tune_interval</strong> (<em>int</em>) &#8211; The frequency of tuning. Defaults to 100 iterations.</li>
<li><strong>model</strong> (<em>PyMC Model</em>) &#8211; Optional model for sampling step. Defaults to None (taken from context).</li>
<li><strong>mode</strong> (string or <cite>Mode</cite> instance.) &#8211; compilation mode passed to Theano functions</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="pymc3.step_methods.metropolis.BinaryMetropolis">
<em class="property">class </em><code class="descclassname">pymc3.step_methods.metropolis.</code><code class="descname">BinaryMetropolis</code><span class="sig-paren">(</span><em>vars</em>, <em>scaling=1.0</em>, <em>tune=True</em>, <em>tune_interval=100</em>, <em>model=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryMetropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>Metropolis-Hastings optimized for binary variables</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vars</strong> (<em>list</em>) &#8211; List of variables for sampler</li>
<li><strong>scaling</strong> (<em>scalar</em><em> or </em><em>array</em>) &#8211; Initial scale factor for proposal. Defaults to 1.</li>
<li><strong>tune</strong> (<em>bool</em>) &#8211; Flag for tuning. Defaults to True.</li>
<li><strong>tune_interval</strong> (<em>int</em>) &#8211; The frequency of tuning. Defaults to 100 iterations.</li>
<li><strong>model</strong> (<em>PyMC Model</em>) &#8211; Optional model for sampling step. Defaults to None (taken from context).</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="staticmethod">
<dt id="pymc3.step_methods.metropolis.BinaryMetropolis.competence">
<em class="property">static </em><code class="descname">competence</code><span class="sig-paren">(</span><em>var</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryMetropolis.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>BinaryMetropolis is only suitable for binary (bool)
and Categorical variables with k=1.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.step_methods.metropolis.BinaryGibbsMetropolis">
<em class="property">class </em><code class="descclassname">pymc3.step_methods.metropolis.</code><code class="descname">BinaryGibbsMetropolis</code><span class="sig-paren">(</span><em>vars</em>, <em>order='random'</em>, <em>model=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryGibbsMetropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>A Metropolis-within-Gibbs step method optimized for binary variables</p>
<dl class="staticmethod">
<dt id="pymc3.step_methods.metropolis.BinaryGibbsMetropolis.competence">
<em class="property">static </em><code class="descname">competence</code><span class="sig-paren">(</span><em>var</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.BinaryGibbsMetropolis.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>BinaryMetropolis is only suitable for Bernoulli
and Categorical variables with k=2.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.step_methods.metropolis.CategoricalGibbsMetropolis">
<em class="property">class </em><code class="descclassname">pymc3.step_methods.metropolis.</code><code class="descname">CategoricalGibbsMetropolis</code><span class="sig-paren">(</span><em>vars</em>, <em>proposal='uniform'</em>, <em>order='random'</em>, <em>model=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.CategoricalGibbsMetropolis" title="Permalink to this definition">¶</a></dt>
<dd><p>A Metropolis-within-Gibbs step method optimized for categorical variables.
This step method works for Bernoulli variables as well, but it is not
optimized for them, like BinaryGibbsMetropolis is. Step method supports
two types of proposals: A uniform proposal and a proportional proposal,
which was introduced by Liu in his 1996 technical report
&#8220;Metropolized Gibbs Sampler: An Improvement&#8221;.</p>
<dl class="staticmethod">
<dt id="pymc3.step_methods.metropolis.CategoricalGibbsMetropolis.competence">
<em class="property">static </em><code class="descname">competence</code><span class="sig-paren">(</span><em>var</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.metropolis.CategoricalGibbsMetropolis.competence" title="Permalink to this definition">¶</a></dt>
<dd><p>CategoricalGibbsMetropolis is only suitable for Bernoulli and
Categorical variables.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-pymc3.step_methods.slicer">
<span id="slice"></span><h3>Slice<a class="headerlink" href="#module-pymc3.step_methods.slicer" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pymc3.step_methods.slicer.Slice">
<em class="property">class </em><code class="descclassname">pymc3.step_methods.slicer.</code><code class="descname">Slice</code><span class="sig-paren">(</span><em>vars=None</em>, <em>w=1.0</em>, <em>tune=True</em>, <em>model=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.slicer.Slice" title="Permalink to this definition">¶</a></dt>
<dd><p>Univariate slice sampler step method</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vars</strong> (<em>list</em>) &#8211; List of variables for sampler.</li>
<li><strong>w</strong> (<em>float</em>) &#8211; Initial width of slice (Defaults to 1).</li>
<li><strong>tune</strong> (<em>bool</em>) &#8211; Flag for tuning (Defaults to True).</li>
<li><strong>model</strong> (<em>PyMC Model</em>) &#8211; Optional model for sampling step. Defaults to None (taken from context).</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="hamiltonian-monte-carlo">
<h3>Hamiltonian Monte Carlo<a class="headerlink" href="#hamiltonian-monte-carlo" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pymc3.step_methods.hmc.hmc.HamiltonianMC">
<em class="property">class </em><code class="descclassname">pymc3.step_methods.hmc.hmc.</code><code class="descname">HamiltonianMC</code><span class="sig-paren">(</span><em>vars=None</em>, <em>path_length=2.0</em>, <em>step_rand=&lt;function unif&gt;</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.step_methods.hmc.hmc.HamiltonianMC" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>vars</strong> (<em>list of theano variables</em>) &#8211; </li>
<li><strong>path_length</strong> (<em>float</em><em>, </em><em>default=2</em>) &#8211; total length to travel</li>
<li><strong>step_rand</strong> (<em>function float -&gt; float</em><em>, </em><em>default=unif</em>) &#8211; A function which takes the step size and returns an new one used to
randomize the step size at each iteration.</li>
<li><strong>step_scale</strong> (<em>float</em><em>, </em><em>default=0.25</em>) &#8211; Initial size of steps to take, automatically scaled down
by 1/n**(1/4).</li>
<li><strong>scaling</strong> (<em>array_like</em><em>, </em><em>ndim = {1</em><em>,</em><em>2}</em>) &#8211; The inverse mass, or precision matrix. One dimensional arrays are
interpreted as diagonal matrices. If <cite>is_cov</cite> is set to True,
this will be interpreded as the mass or covariance matrix.</li>
<li><strong>is_cov</strong> (<em>bool</em><em>, </em><em>default=False</em>) &#8211; Treat the scaling as mass or covariance matrix.</li>
<li><strong>potential</strong> (<em>Potential</em><em>, </em><em>optional</em>) &#8211; An object that represents the Hamiltonian with methods <cite>velocity</cite>,
<cite>energy</cite>, and <cite>random</cite> methods. It can be specified instead
of the scaling matrix.</li>
<li><strong>model</strong> (<em>pymc3.Model</em>) &#8211; The model</li>
<li><strong>**kwargs</strong> (<em>passed to BaseHMC</em>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
</div>
<div class="section" id="variational">
<h2>Variational<a class="headerlink" href="#variational" title="Permalink to this headline">¶</a></h2>
<div class="section" id="module-pymc3.variational.opvi">
<span id="opvi"></span><h3>OPVI<a class="headerlink" href="#module-pymc3.variational.opvi" title="Permalink to this headline">¶</a></h3>
<p>Variational inference is a great approach for doing really complex,
often intractable Bayesian inference in approximate form. Common methods
(e.g. ADVI) lack from complexity so that approximate posterior does not
reveal the true nature of underlying problem. In some applications it can
yield unreliable decisions.</p>
<p>Recently on NIPS 2017 <a class="reference external" href="https://arxiv.org/abs/1610.09033/">OPVI</a> framework
was presented. It generalizes variational inverence so that the problem is
build with blocks. The first and essential block is Model itself. Second is
Approximation, in some cases <span class="math">\(log Q(D)\)</span> is not really needed. Necessity
depends on the third and forth part of that black box, Operator and
Test Function respectively.</p>
<p>Operator is like an approach we use, it constructs loss from given Model,
Approximation and Test Function. The last one is not needed if we minimize
KL Divergence from Q to posterior. As a drawback we need to compute <span class="math">\(loq Q(D)\)</span>.
Sometimes approximation family is intractable and <span class="math">\(loq Q(D)\)</span> is not available,
here comes LS(Langevin Stein) Operator with a set of test functions.</p>
<p>Test Function has more unintuitive meaning. It is usually used with LS operator
and represents all we want from our approximate distribution. For any given vector
based function of <span class="math">\(z\)</span> LS operator yields zero mean function under posterior.
<span class="math">\(loq Q(D)\)</span> is no more needed. That opens a door to rich approximation
families as neural networks.</p>
<p class="rubric">References</p>
<ul class="simple">
<li>Rajesh Ranganath, Jaan Altosaar, Dustin Tran, David M. Blei
Operator Variational Inference
<a class="reference external" href="https://arxiv.org/abs/1610.09033">https://arxiv.org/abs/1610.09033</a> (2016)</li>
</ul>
<dl class="class">
<dt id="pymc3.variational.opvi.ObjectiveFunction">
<em class="property">class </em><code class="descclassname">pymc3.variational.opvi.</code><code class="descname">ObjectiveFunction</code><span class="sig-paren">(</span><em>op</em>, <em>tf</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction" title="Permalink to this definition">¶</a></dt>
<dd><p>Helper class for construction loss and updates for variational inference</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>op</strong> (<a class="reference internal" href="#pymc3.variational.opvi.Operator" title="pymc3.variational.opvi.Operator"><code class="xref py py-class docutils literal"><span class="pre">Operator</span></code></a>) &#8211; OPVI Functional operator</li>
<li><strong>tf</strong> (<code class="xref py py-class docutils literal"><span class="pre">TestFunction</span></code>) &#8211; OPVI TestFunction</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pymc3.variational.opvi.ObjectiveFunction.random">
<code class="descname">random</code><span class="sig-paren">(</span><em>size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction.random" title="Permalink to this definition">¶</a></dt>
<dd><p>Posterior distribution from initial latent space</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>size</strong> (<cite>int</cite>) &#8211; number of samples from distribution</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><em>posterior space (theano)</em></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.ObjectiveFunction.score_function">
<code class="descname">score_function</code><span class="sig-paren">(</span><em>sc_n_mc=None</em>, <em>more_replacements=None</em>, <em>fn_kwargs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction.score_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Compiles scoring function that operates which takes no inputs and returns Loss</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>sc_n_mc</strong> (<cite>int</cite>) &#8211; number of scoring MC samples</li>
<li><strong>more_replacements</strong> &#8211; Apply custom replacements before compiling a function</li>
<li><strong>fn_kwargs</strong> (<cite>dict</cite>) &#8211; arbitrary kwargs passed to theano.function</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>theano.function</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.ObjectiveFunction.step_function">
<code class="descname">step_function</code><span class="sig-paren">(</span><em>obj_n_mc=None</em>, <em>tf_n_mc=None</em>, <em>obj_optimizer=&lt;function adam&gt;</em>, <em>test_optimizer=&lt;function adam&gt;</em>, <em>more_obj_params=None</em>, <em>more_tf_params=None</em>, <em>more_updates=None</em>, <em>more_replacements=None</em>, <em>score=False</em>, <em>fn_kwargs=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction.step_function" title="Permalink to this definition">¶</a></dt>
<dd><p>Step function that should be called on each optimization step.</p>
<p>Generally it solves the following problem:</p>
<div class="math">
\[\mathbf{\lambda^{*}} = \inf_{\lambda} \sup_{\theta} t(\mathbb{E}_{\lambda}[(O^{p,q}f_{\theta})(z)])\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>obj_n_mc</strong> (<cite>int</cite>) &#8211; Number of monte carlo samples used for approximation of objective gradients</li>
<li><strong>tf_n_mc</strong> (<cite>int</cite>) &#8211; Number of monte carlo samples used for approximation of test function gradients</li>
<li><strong>obj_optimizer</strong> (<em>function</em><em> (</em><em>loss</em><em>, </em><em>params</em><em>) </em><em>-&gt; updates</em>) &#8211; Optimizer that is used for objective params</li>
<li><strong>test_optimizer</strong> (<em>function</em><em> (</em><em>loss</em><em>, </em><em>params</em><em>) </em><em>-&gt; updates</em>) &#8211; Optimizer that is used for test function params</li>
<li><strong>more_obj_params</strong> (<cite>list</cite>) &#8211; Add custom params for objective optimizer</li>
<li><strong>more_tf_params</strong> (<cite>list</cite>) &#8211; Add custom params for test function optimizer</li>
<li><strong>more_updates</strong> (<cite>dict</cite>) &#8211; Add custom updates to resulting updates</li>
<li><strong>score</strong> (<cite>bool</cite>) &#8211; calculate loss on each step? Defaults to False for speed</li>
<li><strong>fn_kwargs</strong> (<cite>dict</cite>) &#8211; Add kwargs to theano.function (e.g. <cite>{&#8216;profile&#8217;: True}</cite>)</li>
<li><strong>more_replacements</strong> (<cite>dict</cite>) &#8211; Apply custom replacements before calculating gradients</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><cite>theano.function</cite></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.ObjectiveFunction.updates">
<code class="descname">updates</code><span class="sig-paren">(</span><em>obj_n_mc=None</em>, <em>tf_n_mc=None</em>, <em>obj_optimizer=&lt;function adam&gt;</em>, <em>test_optimizer=&lt;function adam&gt;</em>, <em>more_obj_params=None</em>, <em>more_tf_params=None</em>, <em>more_updates=None</em>, <em>more_replacements=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.ObjectiveFunction.updates" title="Permalink to this definition">¶</a></dt>
<dd><p>Calculates gradients for objective function, test function and then
constructs updates for optimization step</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>obj_n_mc</strong> (<cite>int</cite>) &#8211; Number of monte carlo samples used for approximation of objective gradients</li>
<li><strong>tf_n_mc</strong> (<cite>int</cite>) &#8211; Number of monte carlo samples used for approximation of test function gradients</li>
<li><strong>obj_optimizer</strong> (<em>function</em><em> (</em><em>loss</em><em>, </em><em>params</em><em>) </em><em>-&gt; updates</em>) &#8211; Optimizer that is used for objective params</li>
<li><strong>test_optimizer</strong> (<em>function</em><em> (</em><em>loss</em><em>, </em><em>params</em><em>) </em><em>-&gt; updates</em>) &#8211; Optimizer that is used for test function params</li>
<li><strong>more_obj_params</strong> (<cite>list</cite>) &#8211; Add custom params for objective optimizer</li>
<li><strong>more_tf_params</strong> (<cite>list</cite>) &#8211; Add custom params for test function optimizer</li>
<li><strong>more_updates</strong> (<cite>dict</cite>) &#8211; Add custom updates to resulting updates</li>
<li><strong>more_replacements</strong> (<cite>dict</cite>) &#8211; Apply custom replacements before calculating gradients</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><code class="xref py py-class docutils literal"><span class="pre">ObjectiveUpdates</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.variational.opvi.Operator">
<em class="property">class </em><code class="descclassname">pymc3.variational.opvi.</code><code class="descname">Operator</code><span class="sig-paren">(</span><em>approx</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Operator" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for Operator</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>approx</strong> (<a class="reference internal" href="#pymc3.variational.opvi.Approximation" title="pymc3.variational.opvi.Approximation"><code class="xref py py-class docutils literal"><span class="pre">Approximation</span></code></a>) &#8211; an approximation instance</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>For implementing Custom operator it is needed to define <a class="reference internal" href="#pymc3.variational.opvi.Operator.apply" title="pymc3.variational.opvi.Operator.apply"><code class="xref py py-func docutils literal"><span class="pre">Operator.apply()</span></code></a> method</p>
<dl class="attribute">
<dt id="pymc3.variational.opvi.Operator.OBJECTIVE">
<code class="descname">OBJECTIVE</code><a class="headerlink" href="#pymc3.variational.opvi.Operator.OBJECTIVE" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <a class="reference internal" href="#pymc3.variational.opvi.ObjectiveFunction" title="pymc3.variational.opvi.ObjectiveFunction"><code class="xref py py-class docutils literal"><span class="pre">ObjectiveFunction</span></code></a></p>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Operator.apply">
<code class="descname">apply</code><span class="sig-paren">(</span><em>f</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Operator.apply" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator itself</p>
<div class="math">
\[(O^{p,q}f_{\theta})(z)\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>f</strong> (<code class="xref py py-class docutils literal"><span class="pre">TestFunction</span></code> or None) &#8211; function that takes <cite>z = self.input</cite> and returns
same dimensional output</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><cite>TensorVariable</cite> &#8211; symbolically applied operator</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.variational.opvi.Approximation">
<em class="property">class </em><code class="descclassname">pymc3.variational.opvi.</code><code class="descname">Approximation</code><span class="sig-paren">(</span><em>local_rv=None</em>, <em>model=None</em>, <em>cost_part_grad_scale=1</em>, <em>scale_cost_to_minibatch=False</em>, <em>random_seed=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for approximations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>cost_part_grad_scale</strong> (<em>float</em><em> or </em><em>scalar tensor</em>) &#8211; Scaling score part of gradient can be useful near optimum for
archiving better convergence properties. Common schedule is
1 at the start and 0 in the end. So slow decay will be ok.
See (Sticking the Landing; Geoffrey Roeder,
Yuhuai Wu, David Duvenaud, 2016) for details</li>
<li><strong>scale_cost_to_minibatch</strong> (<em>bool</em><em>, </em><em>default False</em>) &#8211; Scale cost to minibatch instead of full dataset</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Notes</p>
<p>Defining an approximation needs
custom implementation of the following methods:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="first docutils">
<dt><code class="code docutils literal"><span class="pre">.create_shared_params(**kwargs)</span></code></dt>
<dd>Returns {dict|list|theano.shared}</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="code docutils literal"><span class="pre">.random_global(size=None,</span> <span class="pre">no_rand=False)</span></code></dt>
<dd>Generate samples from posterior. If <cite>no_rand==False</cite>:
sample from MAP of initial distribution.
Returns TensorVariable</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt><code class="code docutils literal"><span class="pre">.log_q_W_global(z)</span></code></dt>
<dd>It is needed only if used with operator
that requires <span class="math">\(logq\)</span> of an approximation
Returns Scalar</dd>
</dl>
</li>
</ul>
</div></blockquote>
<p>You can also override the following methods:</p>
<blockquote>
<div><ul class="simple">
<li><code class="code docutils literal"><span class="pre">._setup(**kwargs)</span></code>
Do some specific stuff having <cite>kwargs</cite> before calling <a class="reference internal" href="#pymc3.variational.opvi.Approximation.create_shared_params" title="pymc3.variational.opvi.Approximation.create_shared_params"><code class="xref py py-func docutils literal"><span class="pre">Approximation.create_shared_params()</span></code></a></li>
<li><code class="code docutils literal"><span class="pre">.check_model(model,</span> <span class="pre">**kwargs)</span></code>
Do some specific check for model having <cite>kwargs</cite></li>
</ul>
</div></blockquote>
<p><cite>kwargs</cite> mentioned above are supplied as additional arguments
for <a class="reference internal" href="#pymc3.variational.opvi.Approximation" title="pymc3.variational.opvi.Approximation"><code class="xref py py-class docutils literal"><span class="pre">Approximation</span></code></a></p>
<p>There are some defaults class attributes for approximation classes that can be
optionally overridden.</p>
<blockquote>
<div><ul class="simple">
<li><code class="code docutils literal"><span class="pre">initial_dist_name</span></code>
string that represents name of the initial distribution.
In most cases if will be <cite>uniform</cite> or <cite>normal</cite></li>
<li><code class="code docutils literal"><span class="pre">initial_dist_map</span></code>
float where initial distribution has maximum density</li>
</ul>
</div></blockquote>
<p class="rubric">References</p>
<ul class="simple">
<li>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016
Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</li>
<li>Kingma, D. P., &amp; Welling, M. (2014).
Auto-Encoding Variational Bayes. stat, 1050, 1.</li>
</ul>
<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.apply_replacements">
<code class="descname">apply_replacements</code><span class="sig-paren">(</span><em>node</em>, <em>deterministic=False</em>, <em>include=None</em>, <em>exclude=None</em>, <em>more_replacements=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.apply_replacements" title="Permalink to this definition">¶</a></dt>
<dd><p>Replace variables in graph with variational approximation. By default, replaces all variables</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>node</strong> (<em>Theano Variables</em><em> (or </em><em>Theano expressions</em><em>)</em><em></em>) &#8211; node or nodes for replacements</li>
<li><strong>deterministic</strong> (<em>bool</em>) &#8211; whether to use zeros as initial distribution
if True - zero initial point will produce constant latent variables</li>
<li><strong>include</strong> (<cite>list</cite>) &#8211; latent variables to be replaced</li>
<li><strong>exclude</strong> (<cite>list</cite>) &#8211; latent variables to be excluded for replacements</li>
<li><strong>more_replacements</strong> (<cite>dict</cite>) &#8211; add custom replacements to graph, e.g. change input source</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>node(s) with replacements</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.check_model">
<code class="descname">check_model</code><span class="sig-paren">(</span><em>model</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.check_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Checks that model is valid for variational inference</p>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.construct_replacements">
<code class="descname">construct_replacements</code><span class="sig-paren">(</span><em>include=None</em>, <em>exclude=None</em>, <em>more_replacements=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.construct_replacements" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct replacements with given conditions</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>include</strong> (<cite>list</cite>) &#8211; latent variables to be replaced</li>
<li><strong>exclude</strong> (<cite>list</cite>) &#8211; latent variables to be excluded for replacements</li>
<li><strong>more_replacements</strong> (<cite>dict</cite>) &#8211; add custom replacements to graph, e.g. change input source</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><cite>dict</cite> &#8211; Replacements</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.create_shared_params">
<code class="descname">create_shared_params</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.create_shared_params" title="Permalink to this definition">¶</a></dt>
<dd><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><em>{dict|list|theano.shared}</em></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.initial">
<code class="descname">initial</code><span class="sig-paren">(</span><em>size</em>, <em>no_rand=False</em>, <em>l=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.initial" title="Permalink to this definition">¶</a></dt>
<dd><p>Initial distribution for constructing posterior</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<cite>int</cite>) &#8211; number of samples</li>
<li><strong>no_rand</strong> (<cite>bool</cite>) &#8211; return zeros if True</li>
<li><strong>l</strong> (<cite>int</cite>) &#8211; length of sample, defaults to latent space dim</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><cite>tt.TensorVariable</cite> &#8211; sampled latent space</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.log_q_W_global">
<code class="descname">log_q_W_global</code><span class="sig-paren">(</span><em>z</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.log_q_W_global" title="Permalink to this definition">¶</a></dt>
<dd><p>log_q_W samples over q for global vars</p>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.log_q_W_local">
<code class="descname">log_q_W_local</code><span class="sig-paren">(</span><em>z</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.log_q_W_local" title="Permalink to this definition">¶</a></dt>
<dd><p>log_q_W samples over q for local vars
Gradient wrt mu, rho in density parametrization
can be scaled to lower variance of ELBO</p>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.logq">
<code class="descname">logq</code><span class="sig-paren">(</span><em>z</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.logq" title="Permalink to this definition">¶</a></dt>
<dd><p>Total logq for approximation</p>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.random">
<code class="descname">random</code><span class="sig-paren">(</span><em>size=None</em>, <em>no_rand=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.random" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements posterior distribution from initial latent space</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<cite>scalar</cite>) &#8211; number of samples from distribution</li>
<li><strong>no_rand</strong> (<cite>bool</cite>) &#8211; whether use deterministic distribution</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>posterior space (theano)</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pymc3.variational.opvi.Approximation.random_fn">
<code class="descname">random_fn</code><a class="headerlink" href="#pymc3.variational.opvi.Approximation.random_fn" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements posterior distribution from initial latent space</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<cite>int</cite>) &#8211; number of samples from distribution</li>
<li><strong>no_rand</strong> (<cite>bool</cite>) &#8211; whether use deterministic distribution</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>posterior space (numpy)</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.random_global">
<code class="descname">random_global</code><span class="sig-paren">(</span><em>size=None</em>, <em>no_rand=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.random_global" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements posterior distribution from initial latent space</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<cite>scalar</cite>) &#8211; number of samples from distribution</li>
<li><strong>no_rand</strong> (<cite>bool</cite>) &#8211; whether use deterministic distribution</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>global posterior space</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.random_local">
<code class="descname">random_local</code><span class="sig-paren">(</span><em>size=None</em>, <em>no_rand=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.random_local" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements posterior distribution from initial latent space</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<cite>scalar</cite>) &#8211; number of samples from distribution</li>
<li><strong>no_rand</strong> (<cite>bool</cite>) &#8211; whether use deterministic distribution</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>local posterior space</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.sample">
<code class="descname">sample</code><span class="sig-paren">(</span><em>draws=1</em>, <em>include_transformed=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from variational posterior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>draws</strong> (<cite>int</cite>) &#8211; Number of random samples.</li>
<li><strong>include_transformed</strong> (<cite>bool</cite>) &#8211; If True, transformed variables are also sampled. Default is False.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>trace</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.backends.base.MultiTrace</span></code>) &#8211; Samples drawn from variational posterior.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.sample_node">
<code class="descname">sample_node</code><span class="sig-paren">(</span><em>node</em>, <em>size=100</em>, <em>more_replacements=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.sample_node" title="Permalink to this definition">¶</a></dt>
<dd><p>Samples given node or nodes over shared posterior</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>node</strong> (<em>Theano Variables</em><em> (or </em><em>Theano expressions</em><em>)</em><em></em>) &#8211; </li>
<li><strong>size</strong> (<em>scalar</em>) &#8211; number of samples</li>
<li><strong>more_replacements</strong> (<cite>dict</cite>) &#8211; add custom replacements to graph, e.g. change input source</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>sampled node(s) with replacements</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.scale_grad">
<code class="descname">scale_grad</code><span class="sig-paren">(</span><em>inp</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.scale_grad" title="Permalink to this definition">¶</a></dt>
<dd><p>Rescale gradient of input</p>
<p class="rubric">References</p>
<ul class="simple">
<li><dl class="first docutils">
<dt>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016</dt>
<dd>Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</dd>
</dl>
</li>
</ul>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.seed">
<code class="descname">seed</code><span class="sig-paren">(</span><em>random_seed=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.seed" title="Permalink to this definition">¶</a></dt>
<dd><p>Reinitialize RandomStream used by this approximation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>random_seed</strong> (<cite>int</cite>) &#8211; New random seed</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.to_flat_input">
<code class="descname">to_flat_input</code><span class="sig-paren">(</span><em>node</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.to_flat_input" title="Permalink to this definition">¶</a></dt>
<dd><p>Replaces vars with flattened view stored in self.input</p>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.opvi.Approximation.view">
<code class="descname">view</code><span class="sig-paren">(</span><em>space</em>, <em>name</em>, <em>reshape=True</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.opvi.Approximation.view" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct view on a variable from flattened <cite>space</cite></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>space</strong> (<em>matrix</em><em> or </em><em>vector</em>) &#8211; space to take view of variable from</li>
<li><strong>name</strong> (<cite>str</cite>) &#8211; name of variable</li>
<li><strong>reshape</strong> (<cite>bool</cite>) &#8211; whether to reshape variable from vectorized view</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>(reshaped) slice of matrix</em> &#8211; variable view</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="id3">
<h3>Inference<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<span class="target" id="module-pymc3.variational.inference"></span><dl class="class">
<dt id="pymc3.variational.inference.ADVI">
<em class="property">class </em><code class="descclassname">pymc3.variational.inference.</code><code class="descname">ADVI</code><span class="sig-paren">(</span><em>local_rv=None</em>, <em>model=None</em>, <em>cost_part_grad_scale=1</em>, <em>scale_cost_to_minibatch=False</em>, <em>random_seed=None</em>, <em>start=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ADVI" title="Permalink to this definition">¶</a></dt>
<dd><p>Automatic Differentiation Variational Inference (ADVI)</p>
<p>This class implements the meanfield ADVI, where the variational
posterior distribution is assumed to be spherical Gaussian without
correlation of parameters and fit to the true posterior distribution.
The means and standard deviations of the variational posterior are referred
to as variational parameters.</p>
<p>For explanation, we classify random variables in probabilistic models into
three types. Observed random variables
<span class="math">\({\cal Y}=\{\mathbf{y}_{i}\}_{i=1}^{N}\)</span> are <span class="math">\(N\)</span> observations.
Each <span class="math">\(\mathbf{y}_{i}\)</span> can be a set of observed random variables,
i.e., <span class="math">\(\mathbf{y}_{i}=\{\mathbf{y}_{i}^{k}\}_{k=1}^{V_{o}}\)</span>, where
<span class="math">\(V_{k}\)</span> is the number of the types of observed random variables
in the model.</p>
<p>The next ones are global random variables
<span class="math">\(\Theta=\{\theta^{k}\}_{k=1}^{V_{g}}\)</span>, which are used to calculate
the probabilities for all observed samples.</p>
<p>The last ones are local random variables
<span class="math">\({\cal Z}=\{\mathbf{z}_{i}\}_{i=1}^{N}\)</span>, where
<span class="math">\(\mathbf{z}_{i}=\{\mathbf{z}_{i}^{k}\}_{k=1}^{V_{l}}\)</span>.
These RVs are used only in AEVB.</p>
<p>The goal of ADVI is to approximate the posterior distribution
<span class="math">\(p(\Theta,{\cal Z}|{\cal Y})\)</span> by variational posterior
<span class="math">\(q(\Theta)\prod_{i=1}^{N}q(\mathbf{z}_{i})\)</span>. All of these terms
are normal distributions (mean-field approximation).</p>
<p><span class="math">\(q(\Theta)\)</span> is parametrized with its means and standard deviations.
These parameters are denoted as <span class="math">\(\gamma\)</span>. While <span class="math">\(\gamma\)</span> is
a constant, the parameters of <span class="math">\(q(\mathbf{z}_{i})\)</span> are dependent on
each observation. Therefore these parameters are denoted as
<span class="math">\(\xi(\mathbf{y}_{i}; \nu)\)</span>, where <span class="math">\(\nu\)</span> is the parameters
of <span class="math">\(\xi(\cdot)\)</span>. For example, <span class="math">\(\xi(\cdot)\)</span> can be a
multilayer perceptron or convolutional neural network.</p>
<p>In addition to <span class="math">\(\xi(\cdot)\)</span>, we can also include deterministic
mappings for the likelihood of observations. We denote the parameters of
the deterministic mappings as <span class="math">\(\eta\)</span>. An example of such mappings is
the deconvolutional neural network used in the convolutional VAE example
in the PyMC3 notebook directory.</p>
<p>This function maximizes the evidence lower bound (ELBO)
<span class="math">\({\cal L}(\gamma, \nu, \eta)\)</span> defined as follows:</p>
<div class="math">
\[\begin{split}{\cal L}(\gamma,\nu,\eta) &amp; =
\mathbf{c}_{o}\mathbb{E}_{q(\Theta)}\left[
\sum_{i=1}^{N}\mathbb{E}_{q(\mathbf{z}_{i})}\left[
\log p(\mathbf{y}_{i}|\mathbf{z}_{i},\Theta,\eta)
\right]\right] \\ &amp;
- \mathbf{c}_{g}KL\left[q(\Theta)||p(\Theta)\right]
- \mathbf{c}_{l}\sum_{i=1}^{N}
    KL\left[q(\mathbf{z}_{i})||p(\mathbf{z}_{i})\right],\end{split}\]</div>
<p>where <span class="math">\(KL[q(v)||p(v)]\)</span> is the Kullback-Leibler divergence</p>
<div class="math">
\[KL[q(v)||p(v)] = \int q(v)\log\frac{q(v)}{p(v)}dv,\]</div>
<p><span class="math">\(\mathbf{c}_{o/g/l}\)</span> are vectors for weighting each term of ELBO.
More precisely, we can write each of the terms in ELBO as follows:</p>
<div class="math">
\[\begin{split}\mathbf{c}_{o}\log p(\mathbf{y}_{i}|\mathbf{z}_{i},\Theta,\eta) &amp; = &amp;
\sum_{k=1}^{V_{o}}c_{o}^{k}
    \log p(\mathbf{y}_{i}^{k}|
           {\rm pa}(\mathbf{y}_{i}^{k},\Theta,\eta)) \\
\mathbf{c}_{g}KL\left[q(\Theta)||p(\Theta)\right] &amp; = &amp;
\sum_{k=1}^{V_{g}}c_{g}^{k}KL\left[
    q(\theta^{k})||p(\theta^{k}|{\rm pa(\theta^{k})})\right] \\
\mathbf{c}_{l}KL\left[q(\mathbf{z}_{i}||p(\mathbf{z}_{i})\right] &amp; = &amp;
\sum_{k=1}^{V_{l}}c_{l}^{k}KL\left[
    q(\mathbf{z}_{i}^{k})||
    p(\mathbf{z}_{i}^{k}|{\rm pa}(\mathbf{z}_{i}^{k}))\right],\end{split}\]</div>
<p>where <span class="math">\({\rm pa}(v)\)</span> denotes the set of parent variables of <span class="math">\(v\)</span>
in the directed acyclic graph of the model.</p>
<p>When using mini-batches, <span class="math">\(c_{o}^{k}\)</span> and <span class="math">\(c_{l}^{k}\)</span> should be
set to <span class="math">\(N/M\)</span>, where <span class="math">\(M\)</span> is the number of observations in each
mini-batch. This is done with supplying <cite>total_size</cite> parameter to
observed nodes (e.g. <code class="code docutils literal"><span class="pre">Normal('x',</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">observed=data,</span> <span class="pre">total_size=10000)</span></code>).
In this case it is possible to automatically determine appropriate scaling for <span class="math">\(logp\)</span>
of observed nodes. Interesting to note that it is possible to have two independent
observed variables with different <cite>total_size</cite> and iterate them independently
during inference.</p>
<p>For working with ADVI, we need to give</p>
<ul>
<li><p class="first">The probabilistic model</p>
<p><cite>model</cite> with three types of RVs (<cite>observed_RVs</cite>,
<cite>global_RVs</cite> and <cite>local_RVs</cite>).</p>
</li>
<li><p class="first">(optional) Minibatches</p>
<p>The tensors to which mini-bathced samples are supplied are
handled separately by using callbacks in <a class="reference internal" href="#pymc3.variational.inference.Inference.fit" title="pymc3.variational.inference.Inference.fit"><code class="xref py py-func docutils literal"><span class="pre">Inference.fit()</span></code></a> method
that change storage of shared theano variable or by <code class="xref py py-func docutils literal"><span class="pre">pymc3.generator()</span></code>
that automatically iterates over minibatches and defined beforehand.</p>
</li>
<li><p class="first">(optional) Parameters of deterministic mappings</p>
<p>They have to be passed along with other params to <a class="reference internal" href="#pymc3.variational.inference.Inference.fit" title="pymc3.variational.inference.Inference.fit"><code class="xref py py-func docutils literal"><span class="pre">Inference.fit()</span></code></a> method
as <cite>more_obj_params</cite> argument.</p>
</li>
</ul>
<p>For more information concerning training stage please reference
<a class="reference internal" href="#pymc3.variational.opvi.ObjectiveFunction.step_function" title="pymc3.variational.opvi.ObjectiveFunction.step_function"><code class="xref py py-func docutils literal"><span class="pre">pymc3.variational.opvi.ObjectiveFunction.step_function()</span></code></a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>cost_part_grad_scale</strong> (<cite>scalar</cite>) &#8211; Scaling score part of gradient can be useful near optimum for
archiving better convergence properties. Common schedule is
1 at the start and 0 in the end. So slow decay will be ok.
See (Sticking the Landing; Geoffrey Roeder,
Yuhuai Wu, David Duvenaud, 2016) for details</li>
<li><strong>scale_cost_to_minibatch</strong> (<cite>bool</cite>) &#8211; Scale cost to minibatch instead of full dataset, default False</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
<li><strong>start</strong> (<cite>Point</cite>) &#8211; starting point for inference</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A.,
and Blei, D. M. (2016). Automatic Differentiation Variational
Inference. arXiv preprint arXiv:1603.00788.</li>
<li>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016
Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</li>
<li>Kingma, D. P., &amp; Welling, M. (2014).
Auto-Encoding Variational Bayes. stat, 1050, 1.</li>
</ul>
<dl class="classmethod">
<dt id="pymc3.variational.inference.ADVI.from_mean_field">
<em class="property">classmethod </em><code class="descname">from_mean_field</code><span class="sig-paren">(</span><em>mean_field</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ADVI.from_mean_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct ADVI from MeanField approximation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>mean_field</strong> (<code class="xref py py-class docutils literal"><span class="pre">MeanField</span></code>) &#8211; approximation to start with</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pymc3.variational.inference.ADVI" title="pymc3.variational.inference.ADVI"><code class="xref py py-class docutils literal"><span class="pre">ADVI</span></code></a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.variational.inference.FullRankADVI">
<em class="property">class </em><code class="descclassname">pymc3.variational.inference.</code><code class="descname">FullRankADVI</code><span class="sig-paren">(</span><em>local_rv=None</em>, <em>model=None</em>, <em>cost_part_grad_scale=1</em>, <em>scale_cost_to_minibatch=False</em>, <em>gpu_compat=False</em>, <em>random_seed=None</em>, <em>start=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.FullRankADVI" title="Permalink to this definition">¶</a></dt>
<dd><p>Full Rank Automatic Differentiation Variational Inference (ADVI)</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>cost_part_grad_scale</strong> (<cite>scalar</cite>) &#8211; Scaling score part of gradient can be useful near optimum for
archiving better convergence properties. Common schedule is
1 at the start and 0 in the end. So slow decay will be ok.
See (Sticking the Landing; Geoffrey Roeder,
Yuhuai Wu, David Duvenaud, 2016) for details</li>
<li><strong>scale_cost_to_minibatch</strong> (<em>bool</em><em>, </em><em>default False</em>) &#8211; Scale cost to minibatch instead of full dataset</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
<li><strong>start</strong> (<cite>Point</cite>) &#8211; starting point for inference</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A.,
and Blei, D. M. (2016). Automatic Differentiation Variational
Inference. arXiv preprint arXiv:1603.00788.</li>
<li>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016
Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</li>
<li>Kingma, D. P., &amp; Welling, M. (2014).
Auto-Encoding Variational Bayes. stat, 1050, 1.</li>
</ul>
<dl class="classmethod">
<dt id="pymc3.variational.inference.FullRankADVI.from_advi">
<em class="property">classmethod </em><code class="descname">from_advi</code><span class="sig-paren">(</span><em>advi</em>, <em>gpu_compat=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.FullRankADVI.from_advi" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct FullRankADVI from ADVI</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>advi</strong> (<a class="reference internal" href="#pymc3.variational.inference.ADVI" title="pymc3.variational.inference.ADVI"><code class="xref py py-class docutils literal"><span class="pre">ADVI</span></code></a>) &#8211; </td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Other Parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><strong>gpu_compat</strong> (<em>bool</em>) &#8211; use GPU compatible version or not</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pymc3.variational.inference.FullRankADVI" title="pymc3.variational.inference.FullRankADVI"><code class="xref py py-class docutils literal"><span class="pre">FullRankADVI</span></code></a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="pymc3.variational.inference.FullRankADVI.from_full_rank">
<em class="property">classmethod </em><code class="descname">from_full_rank</code><span class="sig-paren">(</span><em>full_rank</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.FullRankADVI.from_full_rank" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct FullRankADVI from FullRank approximation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>full_rank</strong> (<code class="xref py py-class docutils literal"><span class="pre">FullRank</span></code>) &#8211; approximation to start with</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pymc3.variational.inference.FullRankADVI" title="pymc3.variational.inference.FullRankADVI"><code class="xref py py-class docutils literal"><span class="pre">FullRankADVI</span></code></a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="classmethod">
<dt id="pymc3.variational.inference.FullRankADVI.from_mean_field">
<em class="property">classmethod </em><code class="descname">from_mean_field</code><span class="sig-paren">(</span><em>mean_field</em>, <em>gpu_compat=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.FullRankADVI.from_mean_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct FullRankADVI from MeanField approximation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>mean_field</strong> (<code class="xref py py-class docutils literal"><span class="pre">MeanField</span></code>) &#8211; approximation to start with</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Other Parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><strong>gpu_compat</strong> (<cite>bool</cite>) &#8211; use GPU compatible version or not</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pymc3.variational.inference.FullRankADVI" title="pymc3.variational.inference.FullRankADVI"><code class="xref py py-class docutils literal"><span class="pre">FullRankADVI</span></code></a></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.variational.inference.SVGD">
<em class="property">class </em><code class="descclassname">pymc3.variational.inference.</code><code class="descname">SVGD</code><span class="sig-paren">(</span><em>n_particles=100</em>, <em>jitter=0.01</em>, <em>model=None</em>, <em>kernel=&lt;pymc3.variational.test_functions.RBF object&gt;</em>, <em>scale_cost_to_minibatch=False</em>, <em>start=None</em>, <em>histogram=None</em>, <em>random_seed=None</em>, <em>local_rv=None</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.SVGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Stein Variational Gradient Descent</p>
<p>This inference is based on Kernelized Stein Discrepancy
it&#8217;s main idea is to move initial noisy particles so that
they fit target distribution best.</p>
<p>Algorithm is outlined below</p>
<dl class="docutils">
<dt><em>Input:</em> A target distribution with density function <span class="math">\(p(x)\)</span></dt>
<dd>and a set of initial particles <span class="math">\({x^0_i}^n_{i=1}\)</span></dd>
</dl>
<p><em>Output:</em> A set of particles <span class="math">\({x_i}^n_{i=1}\)</span> that approximates the target distribution.</p>
<div class="math">
\[\begin{split}x_i^{l+1} &amp;\leftarrow x_i^{l} + \epsilon_l \hat{\phi}^{*}(x_i^l) \\
\hat{\phi}^{*}(x) &amp;= \frac{1}{n}\sum^{n}_{j=1}[k(x^l_j,x) \nabla_{x^l_j} logp(x^l_j)+ \nabla_{x^l_j} k(x^l_j,x)]\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_particles</strong> (<cite>int</cite>) &#8211; number of particles to use for approximation</li>
<li><strong>jitter</strong> (<cite>float</cite>) &#8211; noise sd for initial point</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>kernel</strong> (<cite>callable</cite>) &#8211; kernel function for KSD <span class="math">\(f(histogram) -&gt; (k(x,.), \nabla_x k(x,.))\)</span></li>
<li><strong>scale_cost_to_minibatch</strong> (<em>bool</em><em>, </em><em>default False</em>) &#8211; Scale cost to minibatch instead of full dataset</li>
<li><strong>start</strong> (<cite>Point</cite>) &#8211; initial point for inference</li>
<li><strong>histogram</strong> (<code class="xref py py-class docutils literal"><span class="pre">Empirical</span></code>) &#8211; initialize SVGD with given Empirical approximation instead of default initial particles</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
<li><strong>start</strong> &#8211; starting point for inference</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>Qiang Liu, Dilin Wang (2016)
Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm
arXiv:1608.04471</li>
</ul>
</dd></dl>

<dl class="class">
<dt id="pymc3.variational.inference.ASVGD">
<em class="property">class </em><code class="descclassname">pymc3.variational.inference.</code><code class="descname">ASVGD</code><span class="sig-paren">(</span><em>approx=&lt;class 'pymc3.variational.approximations.FullRank'&gt;</em>, <em>local_rv=None</em>, <em>kernel=&lt;pymc3.variational.test_functions.RBF object&gt;</em>, <em>model=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ASVGD" title="Permalink to this definition">¶</a></dt>
<dd><p>Amortized Stein Variational Gradient Descent</p>
<p>This inference is based on Kernelized Stein Discrepancy
it&#8217;s main idea is to move initial noisy particles so that
they fit target distribution best.</p>
<p>Algorithm is outlined below</p>
<p><em>Input:</em> Parametrized random generator <span class="math">\(R_{\theta}\)</span></p>
<p><em>Output:</em> <span class="math">\(R_{\theta^{*}}\)</span> that approximates the target distribution.</p>
<div class="math">
\[\begin{split}\Delta x_i &amp;= \hat{\phi}^{*}(x_i) \\
\hat{\phi}^{*}(x) &amp;= \frac{1}{n}\sum^{n}_{j=1}[k(x_j,x) \nabla_{x_j} logp(x_j)+ \nabla_{x_j} k(x_j,x)] \\
\Delta_{\theta} &amp;= \frac{1}{n}\sum^{n}_{i=1}\Delta x_i\frac{\partial x_i}{\partial \theta}\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>approx</strong> (<code class="xref py py-class docutils literal"><span class="pre">Approximation</span></code>) &#8211; </li>
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>kernel</strong> (<cite>callable</cite>) &#8211; kernel function for KSD <span class="math">\(f(histogram) -&gt; (k(x,.), \nabla_x k(x,.))\)</span></li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">Model</span></code>) &#8211; </li>
<li><strong>kwargs</strong> (kwargs for <code class="xref py py-class docutils literal"><span class="pre">Approximation</span></code>) &#8211; </li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>Dilin Wang, Yihao Feng, Qiang Liu (2016)
Learning to Sample Using Stein Discrepancy
<a class="reference external" href="http://bayesiandeeplearning.org/papers/BDL_21.pdf">http://bayesiandeeplearning.org/papers/BDL_21.pdf</a></li>
<li>Dilin Wang, Qiang Liu (2016)
Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning
<a class="reference external" href="https://arxiv.org/abs/1611.01722">https://arxiv.org/abs/1611.01722</a></li>
</ul>
<dl class="method">
<dt id="pymc3.variational.inference.ASVGD.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>n=10000</em>, <em>score=None</em>, <em>callbacks=None</em>, <em>progressbar=True</em>, <em>obj_n_mc=30</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.ASVGD.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Amortized Stein Variational Gradient Descent</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>n</strong> (<em>int</em>) &#8211; number of iterations</li>
<li><strong>score</strong> (<em>bool</em>) &#8211; evaluate loss on each iteration or not</li>
<li><strong>callbacks</strong> (<em>list</em><em>[</em><em>function :</em><em> (</em><a class="reference internal" href="#pymc3.variational.opvi.Approximation" title="pymc3.variational.opvi.Approximation"><em>Approximation</em></a><em>, </em><em>losses</em><em>, </em><em>i</em><em>) </em><em>-&gt; None</em><em>]</em><em></em>) &#8211; calls provided functions after each iteration step</li>
<li><strong>progressbar</strong> (<em>bool</em>) &#8211; whether to show progressbar or not</li>
<li><strong>obj_n_mc</strong> (<em>int</em>) &#8211; sample <cite>n</cite> particles for Stein gradient</li>
<li><strong>kwargs</strong> (<em>kwargs</em>) &#8211; additional kwargs for <code class="xref py py-func docutils literal"><span class="pre">ObjectiveFunction.step_function()</span></code></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>Approximation</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.variational.inference.Inference">
<em class="property">class </em><code class="descclassname">pymc3.variational.inference.</code><code class="descname">Inference</code><span class="sig-paren">(</span><em>op</em>, <em>approx</em>, <em>tf</em>, <em>local_rv=None</em>, <em>model=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.Inference" title="Permalink to this definition">¶</a></dt>
<dd><p>Base class for Variational Inference</p>
<p>Communicates Operator, Approximation and Test Function to build Objective Function</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>op</strong> (<em>Operator class</em>) &#8211; </li>
<li><strong>approx</strong> (<em>Approximation class</em><em> or </em><em>instance</em>) &#8211; </li>
<li><strong>tf</strong> (<em>TestFunction instance</em>) &#8211; </li>
<li><strong>local_rv</strong> (<em>dict</em>) &#8211; mapping {model_variable -&gt; local_variable}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>model</strong> (<em>Model</em>) &#8211; PyMC3 Model</li>
<li><strong>kwargs</strong> (<em>kwargs</em>) &#8211; additional kwargs for <code class="xref py py-class docutils literal"><span class="pre">Approximation</span></code></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="pymc3.variational.inference.Inference.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>n=10000</em>, <em>score=None</em>, <em>callbacks=None</em>, <em>progressbar=True</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.Inference.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Performs Operator Variational Inference</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>n</strong> (<em>int</em>) &#8211; number of iterations</li>
<li><strong>score</strong> (<em>bool</em>) &#8211; evaluate loss on each iteration or not</li>
<li><strong>callbacks</strong> (<em>list</em><em>[</em><em>function :</em><em> (</em><a class="reference internal" href="#pymc3.variational.opvi.Approximation" title="pymc3.variational.opvi.Approximation"><em>Approximation</em></a><em>, </em><em>losses</em><em>, </em><em>i</em><em>) </em><em>-&gt; None</em><em>]</em><em></em>) &#8211; calls provided functions after each iteration step</li>
<li><strong>progressbar</strong> (<em>bool</em>) &#8211; whether to show progressbar or not</li>
<li><strong>kwargs</strong> (<em>kwargs</em>) &#8211; additional kwargs for <code class="xref py py-func docutils literal"><span class="pre">ObjectiveFunction.step_function()</span></code></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><em>Approximation</em></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="pymc3.variational.inference.fit">
<code class="descclassname">pymc3.variational.inference.</code><code class="descname">fit</code><span class="sig-paren">(</span><em>n=10000</em>, <em>local_rv=None</em>, <em>method='advi'</em>, <em>model=None</em>, <em>random_seed=None</em>, <em>start=None</em>, <em>inf_kwargs=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.inference.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Handy shortcut for using inference methods in functional way</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>n</strong> (<cite>int</cite>) &#8211; number of iterations</li>
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>method</strong> (str or <a class="reference internal" href="#pymc3.variational.inference.Inference" title="pymc3.variational.inference.Inference"><code class="xref py py-class docutils literal"><span class="pre">Inference</span></code></a>) &#8211; string name is case insensitive in {&#8216;advi&#8217;, &#8216;fullrank_advi&#8217;, &#8216;advi-&gt;fullrank_advi&#8217;, &#8216;svgd&#8217;, &#8216;asvgd&#8217;}</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
<li><strong>inf_kwargs</strong> (<em>dict</em>) &#8211; additional kwargs passed to <a class="reference internal" href="#pymc3.variational.inference.Inference" title="pymc3.variational.inference.Inference"><code class="xref py py-class docutils literal"><span class="pre">Inference</span></code></a></li>
<li><strong>start</strong> (<cite>Point</cite>) &#8211; starting point for inference</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Other Parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><ul class="first simple">
<li><strong>frac</strong> (<cite>float</cite>) &#8211; if method is &#8216;advi-&gt;fullrank_advi&#8217; represents advi fraction when training</li>
<li><strong>kwargs</strong> (<em>kwargs</em>) &#8211; additional kwargs for <a class="reference internal" href="#pymc3.variational.inference.Inference.fit" title="pymc3.variational.inference.Inference.fit"><code class="xref py py-func docutils literal"><span class="pre">Inference.fit()</span></code></a></li>
</ul>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><code class="xref py py-class docutils literal"><span class="pre">Approximation</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-pymc3.variational.approximations">
<span id="approximations"></span><h3>Approximations<a class="headerlink" href="#module-pymc3.variational.approximations" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pymc3.variational.approximations.MeanField">
<em class="property">class </em><code class="descclassname">pymc3.variational.approximations.</code><code class="descname">MeanField</code><span class="sig-paren">(</span><em>local_rv=None</em>, <em>model=None</em>, <em>cost_part_grad_scale=1</em>, <em>scale_cost_to_minibatch=False</em>, <em>random_seed=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.MeanField" title="Permalink to this definition">¶</a></dt>
<dd><p>Mean Field approximation to the posterior where spherical Gaussian family
is fitted to minimize KL divergence from True posterior. It is assumed
that latent space variables are uncorrelated that is the main drawback
of the method</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>start</strong> (<cite>Point</cite>) &#8211; initial mean</li>
<li><strong>cost_part_grad_scale</strong> (<cite>scalar</cite>) &#8211; Scaling score part of gradient can be useful near optimum for
archiving better convergence properties. Common schedule is
1 at the start and 0 in the end. So slow decay will be ok.
See (Sticking the Landing; Geoffrey Roeder,
Yuhuai Wu, David Duvenaud, 2016) for details</li>
<li><strong>scale_cost_to_minibatch</strong> (<cite>bool</cite>) &#8211; Scale cost to minibatch instead of full dataset, default False</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016
Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</li>
</ul>
<dl class="method">
<dt id="pymc3.variational.approximations.MeanField.log_q_W_global">
<code class="descname">log_q_W_global</code><span class="sig-paren">(</span><em>z</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.MeanField.log_q_W_global" title="Permalink to this definition">¶</a></dt>
<dd><p>log_q_W samples over q for global vars</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.variational.approximations.FullRank">
<em class="property">class </em><code class="descclassname">pymc3.variational.approximations.</code><code class="descname">FullRank</code><span class="sig-paren">(</span><em>local_rv=None</em>, <em>model=None</em>, <em>cost_part_grad_scale=1</em>, <em>scale_cost_to_minibatch=False</em>, <em>gpu_compat=False</em>, <em>random_seed=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.FullRank" title="Permalink to this definition">¶</a></dt>
<dd><p>Full Rank approximation to the posterior where Multivariate Gaussian family
is fitted to minimize KL divergence from True posterior. In contrast to
MeanField approach correlations between variables are taken in account. The
main drawback of the method is computational cost.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>model</strong> (<em>PyMC3 model for inference</em>) &#8211; </li>
<li><strong>start</strong> (<em>Point</em>) &#8211; initial mean</li>
<li><strong>cost_part_grad_scale</strong> (<em>float</em><em> or </em><em>scalar tensor</em>) &#8211; Scaling score part of gradient can be useful near optimum for
archiving better convergence properties. Common schedule is
1 at the start and 0 in the end. So slow decay will be ok.
See (Sticking the Landing; Geoffrey Roeder,
Yuhuai Wu, David Duvenaud, 2016) for details</li>
<li><strong>scale_cost_to_minibatch</strong> (<em>bool</em><em>, </em><em>default False</em>) &#8211; Scale cost to minibatch instead of full dataset</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Other Parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><p class="first last"><strong>gpu_compat</strong> (<em>bool</em>) &#8211; use GPU compatible version or not</p>
</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>Geoffrey Roeder, Yuhuai Wu, David Duvenaud, 2016
Sticking the Landing: A Simple Reduced-Variance Gradient for ADVI
approximateinference.org/accepted/RoederEtAl2016.pdf</li>
</ul>
<dl class="classmethod">
<dt id="pymc3.variational.approximations.FullRank.from_mean_field">
<em class="property">classmethod </em><code class="descname">from_mean_field</code><span class="sig-paren">(</span><em>mean_field</em>, <em>gpu_compat=False</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.FullRank.from_mean_field" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct FullRank from MeanField approximation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>mean_field</strong> (<a class="reference internal" href="#pymc3.variational.approximations.MeanField" title="pymc3.variational.approximations.MeanField"><code class="xref py py-class docutils literal"><span class="pre">MeanField</span></code></a>) &#8211; approximation to start with</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">Other Parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><strong>gpu_compat</strong> (<cite>bool</cite>) &#8211; use GPU compatible version or not</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><a class="reference internal" href="#pymc3.variational.approximations.FullRank" title="pymc3.variational.approximations.FullRank"><code class="xref py py-class docutils literal"><span class="pre">FullRank</span></code></a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="pymc3.variational.approximations.FullRank.log_q_W_global">
<code class="descname">log_q_W_global</code><span class="sig-paren">(</span><em>z</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.FullRank.log_q_W_global" title="Permalink to this definition">¶</a></dt>
<dd><p>log_q_W samples over q for global vars</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="pymc3.variational.approximations.Empirical">
<em class="property">class </em><code class="descclassname">pymc3.variational.approximations.</code><code class="descname">Empirical</code><span class="sig-paren">(</span><em>trace</em>, <em>local_rv=None</em>, <em>scale_cost_to_minibatch=False</em>, <em>model=None</em>, <em>random_seed=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.Empirical" title="Permalink to this definition">¶</a></dt>
<dd><p>Builds Approximation instance from a given trace,
it has the same interface as variational approximation</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>trace</strong> (<code class="xref py py-class docutils literal"><span class="pre">MultiTrace</span></code>) &#8211; Trace storing samples (e.g. from step methods)</li>
<li><strong>local_rv</strong> (<em>dict</em><em>[</em><em>var-&gt;tuple</em><em>]</em><em></em>) &#8211; Experimental for Empirical Approximation
mapping {model_variable -&gt; local_variable (<span class="math">\(\mu\)</span>, <span class="math">\(\rho\)</span>)}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>scale_cost_to_minibatch</strong> (<cite>bool</cite>) &#8211; Scale cost to minibatch instead of full dataset, default False</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>random_seed</strong> (<em>None</em><em> or </em><em>int</em>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">model</span><span class="p">:</span>
<span class="gp">... </span>    <span class="n">step</span> <span class="o">=</span> <span class="n">NUTS</span><span class="p">()</span>
<span class="gp">... </span>    <span class="n">trace</span> <span class="o">=</span> <span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">step</span><span class="p">)</span>
<span class="gp">... </span>    <span class="n">histogram</span> <span class="o">=</span> <span class="n">Empirical</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="mi">100</span><span class="p">:])</span>
</pre></div>
</div>
<dl class="classmethod">
<dt id="pymc3.variational.approximations.Empirical.from_noise">
<em class="property">classmethod </em><code class="descname">from_noise</code><span class="sig-paren">(</span><em>size</em>, <em>jitter=0.01</em>, <em>local_rv=None</em>, <em>start=None</em>, <em>model=None</em>, <em>random_seed=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.Empirical.from_noise" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize Histogram with random noise</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<cite>int</cite>) &#8211; number of initial particles</li>
<li><strong>jitter</strong> (<cite>float</cite>) &#8211; initial sd</li>
<li><strong>local_rv</strong> (<cite>dict</cite>) &#8211; mapping {model_variable -&gt; local_variable}
Local Vars are used for Autoencoding Variational Bayes
See (AEVB; Kingma and Welling, 2014) for details</li>
<li><strong>start</strong> (<cite>Point</cite>) &#8211; initial point</li>
<li><strong>model</strong> (<code class="xref py py-class docutils literal"><span class="pre">pymc3.Model</span></code>) &#8211; PyMC3 model for inference</li>
<li><strong>random_seed</strong> (None or <cite>int</cite>) &#8211; leave None to use package global RandomStream or other
valid value to create instance specific one</li>
<li><strong>kwargs</strong> (<em>other kwargs passed to init</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><a class="reference internal" href="#pymc3.variational.approximations.Empirical" title="pymc3.variational.approximations.Empirical"><code class="xref py py-class docutils literal"><span class="pre">Empirical</span></code></a></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="pymc3.variational.approximations.Empirical.histogram">
<code class="descname">histogram</code><a class="headerlink" href="#pymc3.variational.approximations.Empirical.histogram" title="Permalink to this definition">¶</a></dt>
<dd><p>Shortcut to flattened Trace</p>
</dd></dl>

<dl class="attribute">
<dt id="pymc3.variational.approximations.Empirical.histogram_logp">
<code class="descname">histogram_logp</code><a class="headerlink" href="#pymc3.variational.approximations.Empirical.histogram_logp" title="Permalink to this definition">¶</a></dt>
<dd><p>Symbolic logp for every point in trace</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="pymc3.variational.approximations.sample_approx">
<code class="descclassname">pymc3.variational.approximations.</code><code class="descname">sample_approx</code><span class="sig-paren">(</span><em>approx</em>, <em>draws=100</em>, <em>include_transformed=True</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.approximations.sample_approx" title="Permalink to this definition">¶</a></dt>
<dd><p>Draw samples from variational posterior.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>approx</strong> (<code class="xref py py-class docutils literal"><span class="pre">Approximation</span></code>) &#8211; Approximation to sample from</li>
<li><strong>draws</strong> (<cite>int</cite>) &#8211; Number of random samples.</li>
<li><strong>include_transformed</strong> (<cite>bool</cite>) &#8211; If True, transformed variables are also sampled. Default is True.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last"><strong>trace</strong> (class:<cite>pymc3.backends.base.MultiTrace</cite>) &#8211; Samples drawn from variational posterior.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-pymc3.variational.operators">
<span id="operators"></span><h3>Operators<a class="headerlink" href="#module-pymc3.variational.operators" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="pymc3.variational.operators.KL">
<em class="property">class </em><code class="descclassname">pymc3.variational.operators.</code><code class="descname">KL</code><span class="sig-paren">(</span><em>approx</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.operators.KL" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator based on Kullback Leibler Divergence</p>
<div class="math">
\[KL[q(v)||p(v)] = \int q(v)\log\frac{q(v)}{p(v)}dv\]</div>
</dd></dl>

<dl class="class">
<dt id="pymc3.variational.operators.KSD">
<em class="property">class </em><code class="descclassname">pymc3.variational.operators.</code><code class="descname">KSD</code><span class="sig-paren">(</span><em>approx</em><span class="sig-paren">)</span><a class="headerlink" href="#pymc3.variational.operators.KSD" title="Permalink to this definition">¶</a></dt>
<dd><p>Operator based on Kernelized Stein Discrepancy</p>
<dl class="docutils">
<dt>Input: A target distribution with density function <span class="math">\(p(x)\)</span></dt>
<dd>and a set of initial particles <span class="math">\(\{x^0_i\}^n_{i=1}\)</span></dd>
</dl>
<p>Output: A set of particles <span class="math">\(\{x_i\}^n_{i=1}\)</span> that approximates the target distribution.</p>
<div class="math">
\[\begin{split}x_i^{l+1} \leftarrow \epsilon_l \hat{\phi}^{*}(x_i^l) \\
\hat{\phi}^{*}(x) = \frac{1}{n}\sum^{n}_{j=1}[k(x^l_j,x) \nabla_{x^l_j} logp(x^l_j)+ \nabla_{x^l_j} k(x^l_j,x)]\end{split}\]</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>approx</strong> (<code class="xref py py-class docutils literal"><span class="pre">Empirical</span></code>) &#8211; Empirical Approximation used for inference</td>
</tr>
</tbody>
</table>
<p class="rubric">References</p>
<ul class="simple">
<li>Qiang Liu, Dilin Wang (2016)
Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm
arXiv:1608.04471</li>
</ul>
<dl class="attribute">
<dt id="pymc3.variational.operators.KSD.OBJECTIVE">
<code class="descname">OBJECTIVE</code><a class="headerlink" href="#pymc3.variational.operators.KSD.OBJECTIVE" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal"><span class="pre">KSDObjective</span></code></p>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="glm.html" class="btn btn-neutral float-right" title="Generalized Linear Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="distributions/mixture.html" class="btn btn-neutral" title="Mixture" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, John Salvatier, Christopher Fonnesbeck, Thomas Wiecki.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'3.1rc3',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>