<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="PyMC devs">
        
        <link rel="shortcut icon" href="../img/favicon.ico">

	<title>Getting started - PyMC3</title>

        <link href="../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../css/font-awesome-4.0.3.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="../css/highlight.css">

        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
        <![endif]-->

        
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
    <div class="container">

        <!-- Collapsed navigation -->
        <div class="navbar-header">
            
            <!-- Expander button -->
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            

            <!-- Main title -->
            <a class="navbar-brand" href="..">PyMC3</a>
        </div>

        <!-- Expanded navigation -->
        <div class="navbar-collapse collapse">
            
                <!-- Main navigation -->
                <ul class="nav navbar-nav">
                
                
                    <li >
                        <a href="..">Overview</a>
                    </li>
                
                
                
                    <li class="dropdown active">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Tutorial <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li class="active">
                                <a href=".">Getting started</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                
                    <li class="dropdown">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Examples <b class="caret"></b></a>
                        <ul class="dropdown-menu">
                        
                            <li >
                                <a href="../BEST">BEST</a>
                            </li>
                        
                            <li >
                                <a href="../stochastic_volatility">Stochastic Volatility</a>
                            </li>
                        
                            <li >
                                <a href="../hierarchical">Hierarchical</a>
                            </li>
                        
                            <li >
                                <a href="../glm_hierarchical">Hierarchical linear regression</a>
                            </li>
                        
                        </ul>
                    </li>
                
                
                </ul>
            

            
            <!-- Search, Navigation and Repo links -->
            <ul class="nav navbar-nav navbar-right">
                
                
                <li >
                    <a rel="next" href="..">
                        <i class="fa fa-arrow-left"></i> Previous
                    </a>
                </li>
                <li >
                    <a rel="prev" href="../BEST">
                        Next <i class="fa fa-arrow-right"></i>
                    </a>
                </li>
                
                
                <li>
                    <a href="https://github.com/pymc-devs/pymc">
                        
                            <i class="fa fa-github"></i>
                        
                        GitHub
                    </a>
                </li>
                
            </ul>
            
        </div>
    </div>
</div>

        <div class="container">
            
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
    
        <li class="main active"><a href="#probabilistic-programming-in-python-using-pymc">Probabilistic Programming in Python using PyMC</a></li>
        
            <li><a href="#abstract">Abstract</a></li>
        
            <li><a href="#installation">Installation</a></li>
        
            <li><a href="#model-specification">Model specification</a></li>
        
            <li><a href="#inference">Inference</a></li>
        
            <li><a href="#posterior-analysis">Posterior analysis</a></li>
        
            <li><a href="#backends">Backends</a></li>
        
            <li><a href="#defining-arbitrary-stochastic-distributions">Defining arbitrary stochastic distributions</a></li>
        
            <li><a href="#parameter-transformations">Parameter Transformations</a></li>
        
            <li><a href="#deterministic">Deterministic</a></li>
        
    
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="probabilistic-programming-in-python-using-pymc">Probabilistic Programming in Python using PyMC</h1>
<p>Authors: Thomas V. Wiecki, John Salvatier, Christopher Fonnesbeck</p>
<h2 id="abstract">Abstract</h2>
<p>Probabilistic Programming (PP) allows flexible specification of statistical Bayesian models in code. PyMC3 is a new, open-source PP framework that features next generation Markov chain Monte Carlo (MCMC) sampling algorithms such as Hamiltonian Monte Carlo (HMC). In contrast to most other PP frameworks, PyMC3 allows model specification in Python using intuitive syntax. The software itself is written in pure Python which makes it very extensible. The model specification is transparently transcoded via Theano to C and compiled to machine code for maximum speed. Theano also natively supports automatic differentation for computing the gradient required for HMC. In addition, PyMC3 supports parallel sampling natively and allows saving traces to various backends including sqlite. Special syntax for describing general linear models allow for specification of hierarchical multifactor models into oneliners.</p>
<h2 id="installation">Installation</h2>
<p>As PyMC3 is written in pure Python, installation is thus greatly simplified over PyMC2 which required compilation of Fortran code. PyMC3 has the following dependencies: <code>Theano</code>, <code>NumPy</code>, <code>SciPy</code>, and <code>Matplotlib</code>; as well as the following optional dependencies: <code>Pandas</code>, <code>Patsy</code>, <code>Statsmodels</code>.</p>
<p>The code is hosted on GitHub at https://github.com/pymc-devs/pymc and is distributed under the liberal <a href="https://github.com/pymc-devs/pymc/blob/master/LICENSE">Apache License 2.0</a> and contributions are encouraged.</p>
<p>A complete Python installation can most easily be obtained by installing the free <a href="https://store.continuum.io/cshop/anaconda/"><code>Anaconda Python Distribution</code></a> by ContinuumIO. After successfull installation, <code>PyMC3</code> can be installed using <code>pip</code>:</p>
<pre><code>pip install git+https://github.com/pymc-devs/pymc
</code></pre>

<h2 id="model-specification">Model specification</h2>
<p>Consider the following linear Bayesian model:
<mathjax>$$ \alpha \sim \mathcal{N}(0, 1) $$</mathjax>
<mathjax>$$ \beta \sim \mathcal{N}(0, 1) $$</mathjax>
<mathjax>$$ \sigma \sim \lvert\mathcal{N}(0, 1) {\rvert}$$</mathjax>
<mathjax>$$ Y  \sim \mathcal{N}(\alpha + X \beta, \epsilon) $$</mathjax></p>
<p>where <mathjax>$X$</mathjax> is a vector of predictors, and <mathjax>$Y$</mathjax> is a vector outcome variable; <mathjax>$\alpha$</mathjax> the intercept, and <mathjax>$\beta$</mathjax> the slope regression coefficients which follow a standard normal distribution. <mathjax>$\sigma$</mathjax> represents the error term and is modeled as the absolute of a Normal distribution (so-called <em>HalfNormal</em>).</p>
<p>The data might look as follows:</p>
<pre><code>def print_public(module):
    import pprint
    pprint.pprint([name for name in dir(module) if not name.startswith('_')])


import numpy as np
np.random.seed(123)
size = 100
X = np.linspace(0, 1, size)
alpha = 1
beta = 1
Y = alpha + X*beta + np.random.randn(size)


%matplotlib inline
import matplotlib.pyplot as plt
plt.scatter(X, Y)




&lt;matplotlib.collections.PathCollection at 0x7f21eaff0d10&gt;
</code></pre>
<p><img alt="png" src="../getting_started_files/getting_started_4_1.png" /></p>
<pre><code>import pymc3 as pm

with pm.Model() as model_linear:
    alpha = pm.Normal('alpha', mu=0, sd=1)
    beta = pm.Normal('beta', mu=0, sd=1)
    sigma = pm.HalfNormal('sigma', sd=1)
    Y_obs = pm.Normal('Y_obs', mu=alpha + beta * X, sd=sigma, observed=Y)
</code></pre>
<p>As you can see, with a few minor modifications, specification of this model in Python code using <code>PyMC3</code> closely follows the statistical model printed above. <code>PyMC3</code> comes with most commonly used discrete and continuous probability distributions:</p>
<pre><code>print_public(pm.distributions)

['Bernoulli',
 'Beta',
 'BetaBin',
 'Binomial',
 'Bound',
 'Categorical',
 'Cauchy',
 'ChiSquared',
 'ConstantDist',
 'Continuous',
 'DensityDist',
 'Dirichlet',
 'Discrete',
 'DiscreteUniform',
 'Distribution',
 'Exponential',
 'Flat',
 'Gamma',
 'Geometric',
 'HalfCauchy',
 'HalfNormal',
 'InverseGamma',
 'Laplace',
 'Lognormal',
 'Multinomial',
 'MvNormal',
 'NegativeBinomial',
 'Normal',
 'Pareto',
 'Poisson',
 'T',
 'Tpos',
 'Uniform',
 'Wald',
 'Weibull',
 'Wishart',
 'ZeroInflatedPoisson',
 'continuous',
 'discrete',
 'dist_math',
 'distribution',
 'logtransform',
 'meta',
 'multivariate',
 'simplextransform',
 'special',
 'timeseries',
 'transform',
 'transforms']
</code></pre>
<p>The first argument is the name of the random variable (RV). Following arguments specify the priors of the distribution which are described in the doc string:</p>
<pre><code>help(pm.Normal)

Help on class Normal in module pymc.distributions.continuous:

class Normal(pymc.distributions.distribution.Continuous)
 |  Normal log-likelihood.
 |  
 |  .. math::
ight\}
 |  
 |  Parameters
 |  ----------
 |  mu : float
 |      Mean of the distribution.
 |  tau : float
 |      Precision of the distribution, which corresponds to
 |      :math:`1/\sigma^2` (tau &gt; 0).
 |  sd : float
 |      Standard deviation of the distribution. Alternative parameterization.
 |  
 |  .. note::
 |  - :math:`E(X) = \mu`
 |  - :math:`Var(X) = 1/        au`
 |  
 |  Method resolution order:
 |      Normal
 |      pymc.distributions.distribution.Continuous
 |      pymc.distributions.distribution.Distribution
 |      __builtin__.object
 |  
 |  Methods defined here:
 |  
 |  __init__(self, mu=0.0, tau=None, sd=None, *args, **kwargs)
 |  
 |  logp(self, value)
 |  
 |  ----------------------------------------------------------------------
 |  Methods inherited from pymc.distributions.distribution.Distribution:
 |  
 |  __getnewargs__(self)
 |  
 |  default(self)
 |  
 |  get_test_val(self, val, defaults)
 |  
 |  ----------------------------------------------------------------------
 |  Class methods inherited from pymc.distributions.distribution.Distribution:
 |  
 |  dist(cls, *args, **kwargs) from __builtin__.type
 |  
 |  ----------------------------------------------------------------------
 |  Static methods inherited from pymc.distributions.distribution.Distribution:
 |  
 |  __new__(cls, name, *args, **kwargs)
 |  
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from pymc.distributions.distribution.Distribution:
 |  
 |  __dict__
 |      dictionary for instance variables (if defined)
 |  
 |  __weakref__
 |      list of weak references to the object (if defined)
</code></pre>
<p>Make special note of the <code>observed</code> keyword argument supplied here:</p>
<pre><code class="python">    Y_obs = pm.Normal('Y_obs', mu=alpha + beta * X, sd=eps, observed=Y)
</code></pre>

<p>This argument expects a <code>numpy.ndarray</code> object or a <code>pandas.DataFrame</code> and turns Y_obs into a likelihood function.</p>
<p>Also note that this command executes very fast. That's because no computation has taken place yet (more on this below).</p>
<p>Finally, let's take a look at this line:</p>
<pre><code class="python">with pm.Model() as model:
</code></pre>

<p>First, the <code>with</code> statement instantiates a new <code>pm.Model()</code> object and binds it to <code>model</code>. Moreover, the <code>with</code> statement puts this <code>model</code> object in a context variable where it is access from inside upon instantiation of the RVs which register themselves to <code>model</code>.</p>
<p><code>Theano</code> has advanced matrix notation capabilities that enable <code>PyMC3</code> to handle multivariate statistics, including advanced indexing and slicing. Using matrix algebra, the above model can succinctly be rewritten as:</p>
<pre><code>import theano.tensor as T

# Turn X into a 2-D array with a column of 1s for the intercept
X_1 = np.vstack([np.ones_like(X), X])

with pm.Model() as model_matrix:
    theta = pm.Normal('theta', 0, 1, shape=(2, 1))
    sigma = pm.HalfNormal('sigma', sd=1)
    Y_obs = pm.Normal('Y_obs', T.dot(theta.T, X_1), sd=sigma, observed=Y)
</code></pre>
<p>The <code>shape</code> kwarg turns theta into random vector with 2 elements, both of which are distributed according to the standard normal distribution.</p>
<p>To perform the reduction in the <code>Y_obs</code> instantiaion we use the <code>theano</code> <code>dot()</code> function to compute the dot product. At this point it is worth noting that RVs in <code>PyMC3</code> behave like <code>theano</code> expressions which makes them very versatile. For example, using <code>theano</code>'s sigmoid function from the neural net (<code>nnet</code>) submodule we can easily turn this linear regression into a logistic regression:</p>
<pre><code># Turn X into a 2-D array with a column of 1s for the intercept
Y_logistic = Y &gt; np.median(Y) # Binarize Y by splitting at the 50% percentile

with pm.Model() as model_logistic:
    theta = pm.Normal('theta', 0, 1, shape=(2, 1))
    logistic = T.nnet.sigmoid(T.dot(theta.T, X_1))
    Y_obs = pm.Bernoulli('Y_obs', p=logistic, 
                         observed=Y_logistic)
</code></pre>
<p>Further below we describe how more complex GLMs can easily be created in one line using the <code>glm</code> submodule of <code>PyMC3</code> but let us first turn our attention to inference.</p>
<h3 id="note-on-random-variables-and-probability-distributions">Note on Random Variables and Probability Distributions</h3>
<p>As a final note on model specification, <code>PyMC3</code> differentiates between RVs and probability distributions. A RV is specified by a probability distribution, but its also associated to a model and has a specific name -- those are the ones we created above. Probability distributions, however, can exist outside of a pymc model and don't need names. By default, a call to <code>pm.Normal()</code> as used above creates a RV but we can also create just the probability distribution by using the <code>.dist</code> classmethod:</p>
<pre><code>normal_dist = pm.Normal.dist(mu=0, sd=1)
theano_expr = normal_dist.logp(np.array([0, .1]))
theano_expr




Elemwise{switch,no_inplace}.0
</code></pre>
<p>As you can see, by default the <code>logp</code> method only returns a <code>theano</code> expression graph. We have to call <code>.eval()</code> to actually force <code>theano</code> to evaluate the expression graph and compute a value:</p>
<pre><code>theano_expr.eval()




array([-0.91893858, -0.92393858])
</code></pre>
<h2 id="inference">Inference</h2>
<p>As mentioned above, the code examples above have not actually performed any computation. All what was done was to compute -- behind the scenes -- a <code>theano</code> expression graph that allows computation of the summed log probability of the model. On demand, we can ask <code>theano</code> to perform algebraic simplifications and compile the symbolic computation graph to C-code and then natively using the system's <code>gcc</code> compiler. All of this functionality happens transparently and does not burden the <code>PyMC3</code> code base. </p>
<p>A central issue to achieving good convergence is selection of a good starting point to initialize the samplers. <code>PyMC3</code> offers optimization routines to estimate the maximum aposterior (MAP) which is often a sensible choice as a starting value.</p>
<pre><code>with model_matrix:
    start = pm.find_MAP()
print(start)

{'theta': array([[ 1.0330429 ],
       [ 0.96214962]]), 'eps': array(1.1214199169216592)}
</code></pre>
<p>By default, <code>PyMC3</code> uses BFGS optimization but also allows selection of other optimization algorithms from the <code>scipy</code> module:</p>
<pre><code>from scipy import optimize
with model_matrix:
    start = pm.find_MAP(fmin=optimize.fmin_powell)
print(start)
</code></pre>
<p>To actually run MCMC sampling to generate posterior samples we have instantiate a step object that corresponds to a certain kind of MCMC. The <code>pymc.step_methods</code> submodule contains the following samplers, as well as proposal distributions:</p>
<pre><code>print_public(pm.step_methods)
</code></pre>
<p>As you can see, <code>PyMC3</code> supports all samplers that are commonly included in other propalistic programming frameworks like <code>BUGS</code> or <code>JAGS</code>, including Metropolis Hastings with various proposal distributions and Slice sampling. Of special note are the new class of Hamiltonian Monte Carlo samplers like <code>HamiltonianMC</code> as well as the auto-tuned version of it -- the No-U-Turn Sampler (<code>NUTS</code>). This last sampling algorithm is one of the main selling points of the new <code>stan</code> MCMC package (CITE). These Hamiltonian samplers require evaluation of the gradient, something that requires differentation of the logp of the model. Fortunately, <code>theano</code> already supports gradient computation via automatic differentation on the compute graph that <code>PyMC3</code> creates.</p>
<pre><code>with model_matrix:
    step = pm.NUTS(scaling=start) # instantiate sampler
    trace = pm.sample(500, step, start=start) # draw 500 posterior samplers

 [-----------------100%-----------------] 500 of 500 complete in 0.9 sec

/home/wiecki/envs/pymc3/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:117: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility
  from scan_perform.scan_perform import *
</code></pre>
<p>As <code>NUTS</code> requires a scaling matrix we can either compute the Hessian using <code>pm.find_hessian()</code> or pass in a point from which <code>NUTS</code> will compute a diagonal scaling matrix.</p>
<p>The returned <code>trace</code> object can be queried in a similar way as a <code>dict</code> containing a map from RVs to <code>numpy.array</code>s:</p>
<pre><code>trace['theta'][-3:]




array([[[ 0.8046511 ],
        [ 1.47561769]],

       [[ 0.74341003],
        [ 1.32937156]],

       [[ 0.99479954],
        [ 0.85411475]]])
</code></pre>
<p>As mentioned above, we can also run multiple chains in parallel to speed up sampling:</p>
<pre><code>with model_matrix:
    step = pm.NUTS(scaling=start) # instantiate sampler
    trace = pm.sample(500, step, start=start, njobs=4) # draw 500 posterior samplers

 [-----------------100%-----------------] 500 of 500 complete in 1.6 sec
</code></pre>
<p>Behind the scenes, <code>PyMC3</code> will launch multiple Python processes using the <code>multiprocessing</code> module, pickle the model object, distribute it to the workers, and gather the samples in a <code>MultiTrace</code> backend. The resulting <code>trace</code> object will have four chains associated with it:</p>
<pre><code>trace




&lt;MultiTrace: 4 chains, 500 iterations, 2 variables&gt;
</code></pre>
<p>Which can be accessed as a list of numpy arrays:</p>
<pre><code>trace['theta'][1][-5:]




array([[[ 0.77248584],
        [ 1.67067534]],

       [[ 0.73602855],
        [ 1.51512773]],

       [[ 0.71991322],
        [ 1.57903929]],

       [[ 0.76396846],
        [ 1.22617319]],

       [[ 0.58214555],
        [ 1.3127707 ]]])
</code></pre>
<p>More details on the backend can be found further below.</p>
<h2 id="posterior-analysis">Posterior analysis</h2>
<p><code>PyMC3</code> comes with various statistical and plotting capabilities to examine the output. A simple posterior plot can be created using <code>pm.traceplot</code>:</p>
<pre><code>pm.traceplot(trace);
</code></pre>
<p><img alt="png" src="../getting_started_files/getting_started_41_0.png" /></p>
<p>The left column consists of a kernel density estimation (KDE) of the marginal posterios of each RV while the right column contains the trace. As we ran four chains, each subplot will have 4 differently colored lines, one corresponding to each chain. Because <code>theta</code> is a 2-D RV we see two (times four) posteriors in the top row. This allows easy visual inspection of all chains converged to the same posterior.</p>
<p>A forest plot allows for a more succint overview of the posteriors as well as their Gelman-Rubin <mathjax>$\hat{R}$</mathjax> score (CITE):</p>
<pre><code>pm.forestplot(trace)




&lt;matplotlib.gridspec.GridSpec at 0x7f0a99c12c90&gt;
</code></pre>
<p><img alt="png" src="../getting_started_files/getting_started_44_1.png" /></p>
<p>In addition, <code>pm.stats.summary()</code> provides a text-based output of common posterior statistics:</p>
<pre><code>pm.stats.summary(trace)


theta:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------
  ..............................[0, :]...............................
  0.769            0.177            0.009            [0.394, 1.087]
  ..............................[1, :]...............................
  1.210            0.305            0.014            [0.601, 1.786]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|
  .............................[0, :].............................
  0.423          0.645          0.772          0.889          1.126
  .............................[1, :].............................
  0.618          1.002          1.209          1.427          1.811


eps:

  Mean             SD               MC Error         95% HPD interval
  -------------------------------------------------------------------

  0.993            0.073            0.002            [0.858, 1.144]

  Posterior quantiles:
  2.5            25             50             75             97.5
  |--------------|==============|==============|--------------|

  0.863          0.942          0.988          1.039          1.155
</code></pre>
<p>Here the 2-D RV is split into its individual components.</p>
<p>As you can see, <code>PyMC3</code> handles multiple chains as well as multivariate statistics natively.</p>
<h2 id="backends">Backends</h2>
<p><code>PyMC3</code> has support for a small but growing number of backends to store traces in. These can be found in <code>pymc.backends</code>:</p>
<pre><code>print_public(pm.backends)

['NDArray', 'SQLite', 'Text', 'base', 'ndarray', 'sqlite', 'text']
</code></pre>
<p>By default, an in-memory <code>ndarray</code> is used but if the samples would get too large to be held in memory we could use the <code>sqlite</code> backend:</p>
<pre><code>with model_matrix:
    backend = pm.backends.SQLite('trace.sqlite')
    trace = pm.sample(500, step, start=start, trace=backend)

 [-----------------100%-----------------] 500 of 500 complete in 0.9 sec
</code></pre>
<p>The stored trace can then later be loaded using the <code>load</code> command:</p>
<pre><code>with model_matrix:
    trace_loaded = pm.backends.sqlite.load('trace.sqlite')

trace_loaded




&lt;MultiTrace: 1 chains, 1000 iterations, 2 variables&gt;
</code></pre>
<p>More information about <code>backends</code> can be found in the docstring of <code>pymc.backends</code>.</p>
<h2 id="defining-arbitrary-stochastic-distributions">Defining arbitrary stochastic distributions</h2>
<p><code>PyMC3</code> supports two methods to define new distributions:</p>
<h3 id="1-defining-a-distribution-as-a-theano-expression">1. Defining a distribution as a theano expression</h3>
<p>By using <code>pymc.DensityDist</code> you can define new log probability functions that can be used like any other RV. The following is an example inspired by a blog post by Jake Vanderplas on which priors to use for a linear regression. (https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/). For more detail on the derivation of these priors we refer to the blog post. The code to use these custom priors is as follows:</p>
<pre><code>import theano.tensor as T

with pm.Model() as model:
    alpha = pm.Uniform('intercept', -100, 100)
    # Create custom densities
    beta = pm.DensityDist('slope', lambda value: -1.5 * T.log(1 + value**2), testval=0)
    eps = pm.DensityDist('sigma', lambda value: -T.log(T.abs_(value)), testval=1)
    # Create likelihood
    like = pm.Normal('y_est', mu=alpha + beta * X, sd=eps, observed=Y)
</code></pre>
<p>As you can see, <code>DensityDist</code> takes a function (here we are using an anonymous <code>lambda</code> function) that takes the parameter value as input and returns the log probability for that setting.</p>
<h3 id="2-defining-a-distribution-as-a-blackbox-function">2. Defining a distribution as a blackbox function</h3>
<p>Above we used <code>theano</code> to define the math of the probability function. However, often it might not be possible to write a probability distributions as a <code>theano</code> expression. This is the case in many scientific fields including psychology and astrophysics where the likelihood functions are often a model of the process in question and has a complex form that might require numerical approximation. As the rest of <code>PyMC3</code> still expects a <code>theano</code> expression we have to wrap a function as a blackbox <code>theano</code> expression. This is possible with the <code>as_op</code> decorator. As a simple example we can reformulate the above distribution for <code>beta</code> to only use <code>numpy</code> internally. </p>
<pre><code>import theano

@theano.compile.ops.as_op(itypes=[T.dscalar], otypes=[T.dscalar])
def beta_prior(value):
    return -1.5 * np.log(1 + value**2)
</code></pre>
<p>As can be seen, the decorator requires definition of the input and output types. For an overview of the available types, see http://deeplearning.net/software/theano/library/tensor/basic.html#all-fully-typed-constructors.</p>
<p><strong>Note</strong>: Because this function is now a blackbox function it is not possible for <code>theano</code> to inspect it and compute the gradient required for the Hamiltonian samplers. It is thus not possible to use the <code>HMC</code> or <code>NUTS</code> samplers for a model that uses such a distribution and you have to use e.g. <code>Metropolis</code> or <code>Slice</code> sampling.</p>
<p>For a more elaborate example of the usage of <code>as_op</code> we refer to the coal mining disaster model: https://github.com/pymc-devs/pymc/blob/master/pymc/examples/disaster_model_arbitrary_deterministic.py.</p>
<h2 id="parameter-transformations">Parameter Transformations</h2>
<p>To ease the sampling process it is often advisable to transform certain variables. For example, scale parameters can only be positive and thus have a sharp edge at 0 in their posterior probability. This often creates problems for samples, especially those that require the gradient because of non-differentiable areas in the posterior. Parameter transformation can remedy this problem. For example, log-transforming a scale variable allows us to sample across the whole parameter space. <code>PyMC3</code> supports transformations by calling the <code>model.TransformedVar</code> method:</p>
<pre><code>with pm.Model() as model_linear:
    alpha = pm.Normal('alpha', mu=0, sd=1)
    beta = pm.Normal('beta', mu=0, sd=1)

    sigma, log_sigma = model.TransformedVar('sigma', 
                                            pm.HalfNormal.dist(sd=1),
                                            pm.logtransform)

    Y_obs = pm.Normal('Y_obs', mu=alpha + beta * X, sd=sigma, observed=Y)
</code></pre>
<p>Of special note is that <code>TransformedVar</code> returns two distributions: one variable in the transformed space and one in the normal space. The one in the transformed space (here log(sigma)) is the one over which sampling will occur, and the one in the normal space is the one to use throughout the rest of the model.</p>
<h2 id="deterministic">Deterministic</h2>
<p>Above you have already seen that RVs in <code>PyMC3</code> can be easily combined and transformed (see the logistic regression example above). These transformations are called <em>deterministic</em>. However, note that in the above example we do not retain the samples of our transformed values (i.e. the <code>d</code>).  Using the <code>Deterministic</code> class we can explicitly add these deterministic variables to the trace.</p></div>
            
        </div>

        <footer class="col-md-12">
            <hr>
            
            <p>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>

        

        <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
        <script src="../js/bootstrap-3.0.3.min.js"></script>
        <script src="../js/prettify-1.0.min.js"></script>
        <script src="../js/highlight.pack.js"></script>
        <script src="../js/base.js"></script>
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
        <script src="../js/mathjaxhelper.js"></script>
    </body>
</html>