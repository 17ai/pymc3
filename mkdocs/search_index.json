{
    "docs": [
        {
            "location": "/", 
            "text": "PyMC3\n\n\n\n\n\n\nPyMC3 is a python module for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems.\n\n\nCheck out the \nTutorial\n!\n\n\nPyMC3 is Beta software. Users should consider using \nPyMC 2 repository\n.\n\n\nFeatures\n\n\n\n\nIntuitive model specification syntax, for example, \nx ~ N(0,1)\n translates to \nx = Normal(0,1)\n\n\nPowerful sampling algorithms, such as the \nNo U-Turn Sampler\n, allow complex models with thousands of parameters with little specialized knowledge of fitting algorithms.\n\n\nEasy optimization for finding the \nmaximum a posteriori\n(MAP) point\n\n\nTheano\n features\n\n\nNumpy broadcasting and advanced indexing\n\n\nLinear algebra operators\n\n\nComputation optimization and dynamic C compilation\n\n\nSimple extensibility\n\n\nTransparent support for missing value imputation\n\n\n\n\nGetting started\n\n\n\n\nPyMC3 Tutorial\n\n\nCoal Mining Disasters model in \nPyMC 2\n and \nPyMC 3\n\n\nGlobal Health Metrics \n Evaluation model\n case study for GHME 2013\n\n\nStochastic Volatility model\n\n\nSeveral blog posts on linear regression\n\n\nTalk at PyData NYC 2013 on PyMC3\n\n\nPyMC3 port of the models presented in the book \"Doing Bayesian Data Analysis\" by John Kruschke\n\n\nThe PyMC3 examples folder\n\n\n\n\nInstallation\n\n\nThe latest version of PyMC3 can be installed from the master branch using pip:\n\n\npip install --process-dependency-links git+https://github.com/pymc-devs/pymc3\n\n\n\n\nThe \n--process-dependency-links\n flag ensures that the developmental branch of Theano, which PyMC3 requires, is installed. If a recent developmental version of Theano has been installed with another method, this flag can be dropped.\n\n\nAnother option is to clone the repository and install PyMC3 using \npython setup.py install\n or \npython setup.py develop\n.\n\n\nNote:\n Running \npip install pymc\n will install PyMC 2.3, not PyMC3, from PyPI.\n\n\nDependencies\n\n\nPyMC3 is tested on Python 2.7 and 3.3 and depends on Theano, NumPy,\nSciPy, Pandas, and Matplotlib (see setup.py for version information).\n\n\nOptional\n\n\nIn addtion to the above dependencies, the GLM submodule relies on\nPatsy.\n\n\nscikits.sparse\n enables sparse scaling matrices which are useful for large problems. Installation on Ubuntu is easy:\n\n\nsudo apt-get install libsuitesparse-dev\npip install git+https://github.com/njsmith/scikits-sparse.git\n\n\n\n\nOn Mac OS X you can install libsuitesparse 4.2.1 via homebrew (see http://brew.sh/ to install homebrew), manually add a link so the include files are where scikits-sparse expects them, and then install scikits-sparse:\n\n\nbrew install suite-sparse\nln -s /usr/local/Cellar/suite-sparse/4.2.1/include/ /usr/local/include/suitesparse\npip install git+https://github.com/njsmith/scikits-sparse.git\n\n\n\n\nLicense\n\n\nApache License, Version 2.0\n\n\nContributors\n\n\nSee the \nGitHub contributor page", 
            "title": "Overview"
        }, 
        {
            "location": "/#pymc3", 
            "text": "PyMC3 is a python module for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems.  Check out the  Tutorial !  PyMC3 is Beta software. Users should consider using  PyMC 2 repository .", 
            "title": "PyMC3"
        }, 
        {
            "location": "/#features", 
            "text": "Intuitive model specification syntax, for example,  x ~ N(0,1)  translates to  x = Normal(0,1)  Powerful sampling algorithms, such as the  No U-Turn Sampler , allow complex models with thousands of parameters with little specialized knowledge of fitting algorithms.  Easy optimization for finding the  maximum a posteriori (MAP) point  Theano  features  Numpy broadcasting and advanced indexing  Linear algebra operators  Computation optimization and dynamic C compilation  Simple extensibility  Transparent support for missing value imputation", 
            "title": "Features"
        }, 
        {
            "location": "/#getting-started", 
            "text": "PyMC3 Tutorial  Coal Mining Disasters model in  PyMC 2  and  PyMC 3  Global Health Metrics   Evaluation model  case study for GHME 2013  Stochastic Volatility model  Several blog posts on linear regression  Talk at PyData NYC 2013 on PyMC3  PyMC3 port of the models presented in the book \"Doing Bayesian Data Analysis\" by John Kruschke  The PyMC3 examples folder", 
            "title": "Getting started"
        }, 
        {
            "location": "/#installation", 
            "text": "The latest version of PyMC3 can be installed from the master branch using pip:  pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3  The  --process-dependency-links  flag ensures that the developmental branch of Theano, which PyMC3 requires, is installed. If a recent developmental version of Theano has been installed with another method, this flag can be dropped.  Another option is to clone the repository and install PyMC3 using  python setup.py install  or  python setup.py develop .  Note:  Running  pip install pymc  will install PyMC 2.3, not PyMC3, from PyPI.", 
            "title": "Installation"
        }, 
        {
            "location": "/#dependencies", 
            "text": "PyMC3 is tested on Python 2.7 and 3.3 and depends on Theano, NumPy,\nSciPy, Pandas, and Matplotlib (see setup.py for version information).  Optional  In addtion to the above dependencies, the GLM submodule relies on\nPatsy.  scikits.sparse  enables sparse scaling matrices which are useful for large problems. Installation on Ubuntu is easy:  sudo apt-get install libsuitesparse-dev\npip install git+https://github.com/njsmith/scikits-sparse.git  On Mac OS X you can install libsuitesparse 4.2.1 via homebrew (see http://brew.sh/ to install homebrew), manually add a link so the include files are where scikits-sparse expects them, and then install scikits-sparse:  brew install suite-sparse\nln -s /usr/local/Cellar/suite-sparse/4.2.1/include/ /usr/local/include/suitesparse\npip install git+https://github.com/njsmith/scikits-sparse.git", 
            "title": "Dependencies"
        }, 
        {
            "location": "/#license", 
            "text": "Apache License, Version 2.0", 
            "title": "License"
        }, 
        {
            "location": "/#contributors", 
            "text": "See the  GitHub contributor page", 
            "title": "Contributors"
        }, 
        {
            "location": "/getting_started/", 
            "text": "Probabilistic Programming in Python using PyMC\n\n\nAuthors: John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck\n\n\nIntroduction\n\n\nProbabilistic Programming (PP) allows flexible specification of statistical Bayesian models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). This class of samplers work well on high dimensional and complex posterior distributions and allows many complex models to be fit without specialized knowledge about fitting algorithms. HMC and NUTS take advantage of gradient information from the likelihood to achieve much faster convergence than traditional sampling methods, especially for larger models. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo, which means you usually don't need to have specialized knowledge about how the algorithms work. PyMC3, Stan (Stan Development Team, 2014), and the LaplacesDemon package for R are currently the only PP packages to offer HMC.\n\n\nProbabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.\n\n\nWhile most of PyMC3's user-facing features are written in pure Python, it leverages Theano (Bergstra et al., 2010) to transparently transcode models to C and compile it to machine code, thereby boosting performance. Theano is a library that allows expressions to be defined using generalized vector data structures called \ntensors\n, which are tightly integrated with the popular NumPy \nndarray\n data structure, and similarly allows for broadcasting and advanced indexing, just as NumPy arrays do. Theano also automatically optimizes the likelihood's computational graph for speed and provides simple GPU integration.\n\n\nHere, we present a primer on the use of PyMC3 for solving general Bayesian statistical inference and prediction problems. We will first see the basics of how to use PyMC3, motivated by a simple example: installation, data creation, model definition, model fitting and posterior analysis. Then we will cover two case studies and use them to show how to define and fit more sophisticated models. Finally we will show how to extend PyMC3 and discuss other useful features: the Generalized Linear Models subpackage, custom distributions, custom transformations and alternative storage backends.\n\n\nInstallation\n\n\nRunning PyMC3 requires a working Python interpreter, either version 2.7 (or more recent) or 3.4 (or more recent); we recommend that new users install version 3.4. A complete Python installation for Mac OSX, Linux and Windows can most easily be obtained by downloading and installing the free \nAnaconda Python Distribution\n by ContinuumIO. \n\n\nPyMC3\n can also be installed manually using \npip\n (https://pip.pypa.io/en/latest/installing.html):\n\n\npip install git+https://github.com/pymc-devs/pymc3\n\n\n\n\nPyMC3 depends on several third-party Python packages which will be automatically installed when installing via pip. The four required dependencies are: \nTheano\n, \nNumPy\n, \nSciPy\n, and \nMatplotlib\n. To take full advantage of PyMC3, the optional dependencies \nPandas\n and \nPatsy\n should also be installed. These are \nnot\n automatically installed, but can be installed by:\n\n\npip install patsy pandas\n\n\n\n\nThe source code for PyMC3 is hosted on GitHub at https://github.com/pymc-devs/pymc3 and is distributed under the liberal \nApache License 2.0\n. On the GitHub site, users may also report bugs and other issues, as well as contribute code to the project, which we actively encourage.\n\n\nA Motivating Example: Linear Regression\n\n\nTo introduce model definition, fitting and posterior analysis, we first consider a simple Bayesian linear regression model with normal priors for the parameters. We are interested in predicting outcomes \n$Y$\n as normally-distributed observations with an expected value \n$\\mu$\n that is a linear function of two predictor variables, \n$X_1$\n and \n$X_2$\n.\n\n\n$$\\begin{aligned} \nY  \n\\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n\\mu \n= \\alpha + \\beta_1 X_1 + \\beta_2 X_2\n\\end{aligned}$$\n\n\nwhere \n$\\alpha$\n the intercept, and \n$\\beta_i$\n the coefficient for covariate \n$X_i$\n, while \n$\\sigma$\n represents the observation error. Since we are constructing a Bayesian model, the unknown variables in the model must be assigned a prior distribution. Our choices will be zero-mean normal priors with variance of 100 for both regression coefficients (which corresponds to relatively \ndiffuse\n information regarding the true parameter values), and \n$\\sigma$\n is modeled as the absolute of a Normal distribution (so-called \nHalfNormal\n). \n\n\n$$\\begin{aligned} \n\\alpha \n\\sim \\mathcal{N}(0, 100) \\\\\n\\beta_i \n\\sim \\mathcal{N}(0, 100) \\\\\n\\sigma \n\\sim \\lvert\\mathcal{N}(0, 1){\\rvert}\n\\end{aligned}$$\n\n\nGenerating data\n\n\nWe can simulate some artificial data from this model using only NumPy's \nrandom\n module, and then use PyMC3 to try to recover the corresponding parameters. We are intentionally generating the data to closely correspond the PyMC3 model structure.\n\n\nimport numpy as np\n\n# Intialize random number generator\nnp.random.seed(123)\n\n# True parameter values\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\n\n# Size of dataset\nsize = 100\n\n# Predictor variable\nX1 = np.linspace(0, 1, size)\nX2 = np.linspace(0,.2, size)\n\n# Simulate outcome variable\nY = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma\n\n\n\n\nHere is what the simulated data look like. We use the \npylab\n module from the plotting library matplotlib. \n\n\n%matplotlib inline \nimport pylab as pl\n\nfig, axes = pl.subplots(1, 2, sharex=True, figsize=(10,4))\naxes[0].scatter(X1, Y)\naxes[1].scatter(X2, Y)\naxes[0].set_ylabel('Y'); axes[0].set_xlabel('X1'); axes[1].set_xlabel('X2');\n\n\n\n\n\n\nModel Specification\n\n\nSpecifiying this model in PyMC3 is straightforward because the syntax is as close to the statistical notation. For the most part, each line of Python code corresponds to a line in the model notation above. \n\n\nFirst, we import the components we will need from PyMC.\n\n\nfrom pymc3 import Model, Normal, HalfNormal\n\n\n\n\nNow we build our model, which we will present in full first, then explain each part line-by-line.\n\n\nbasic_model = Model()\n\nwith basic_model:\n\n    # Priors for unknown model parameters\n    alpha = Normal('alpha', mu=0, sd=10)\n    beta = Normal('beta', mu=0, sd=10, shape=2)\n    sigma = HalfNormal('sigma', sd=1)\n\n    # Expected value of outcome\n    mu = alpha + beta[0]*X1 + beta[1]*X2\n\n    # Likelihood (sampling distribution) of observations\n    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)\n\n\n\n\nThe first line,\n\n\nbasic_model = Model()\n\n\n\n\ncreates a new \nModel\n object which is a container for the model random variables.\n\n\nFollowing instantiation of the model, the subsequent specification of the model components is performed inside a  \nwith\n statement:\n\n\nwith basic_model:\n\n\n\n\nThis creates a \ncontext manager\n, with our \nbasic_model\n as the context, that includes all statements until the indented block ends. This means all PyMC3 objects introduced in the indented code block below the \nwith\n statement are added to the model behind the scenes. Absent this context manager idiom, we would be forced to manually associate each of the variables with \nbasic_model\n right after we create them. If you try to create a new random variable without a \nwith model:\n statement, it will raise an error since there is no obvious model for the variable to be added to.\n\n\nThe first three statements in the context manager:\n\n\nalpha = Normal('alpha', mu=0, sd=10)\nbeta = Normal('beta', mu=0, sd=10, shape=2)\nsigma = HalfNormal('sigma', sd=1)\n\n\n\n\ncreate a \nstochastic\n random variables with a Normal prior distributions for the regression coefficients with a mean of 0 and standard deviation of 10 for the regression coefficients, and a half-normal distribution for the standard deviation of the observations, \n$\\sigma$\n. These are stochastic because their values are partly determined by its parents, which for priors are simple constants, and partly random (or stochastic). \n\n\nWe call the \nNormal\n constructor to create a random variable to use as a normal prior. The first argument is always the \nname\n of the random variable, which should almost always match the name of the Python variable being assigned to, since it sometimes used to retrieve the variable from the model for summarizing output. The remaining required arguments for a stochastic object are the parameters, in this case \nmu\n, the mean, and \nsd\n, the standard deviation, which we assign hyperparameter values for the model. In general, a distribution's parameters are values that determine the location, shape or scale of the random variable, depending on the parameterization of the distribution. Most commonly used distributions, such as \nBeta\n, \nExponential\n, \nCategorical\n, \nGamma\n, \nBinomial\n and many others, are available in PyMC3.\n\n\nThe \nbeta\n variable has an additional \nshape\n argument to denote it as a vector-valued parameter of size 2. The \nshape\n argument is available for all distributions and specifies the length or shape of the random variable, but is optional for scalar variables, since it defaults to a value of one. It can be an integer, to specify an array, or a tuple, to specify a multidimensional array (\ne.g.\n \nshape=(5,7)\n makes random variable that takes on 5 by 7 matrix values). \n\n\nDetailed notes about distributions, sampling methods and other PyMC3 functions are available via the \nhelp\n function.\n\n\nhelp(Normal) #try help(Model), help(Uniform) or help(basic_model)\n\n\n\n\nHelp on class Normal in module pymc3.distributions.continuous:\n\nclass Normal(pymc3.distributions.distribution.Continuous)\n |  Normal log-likelihood.\n |  \n |  .. math::\night\\}\n |  \n |  Parameters\n |  ----------\n |  mu : float\n |      Mean of the distribution.\n |  tau : float\n |      Precision of the distribution, which corresponds to\n |      :math:`1/\\sigma^2` (tau \n 0).\n |  sd : float\n |      Standard deviation of the distribution. Alternative parameterization.\n |  \n |  .. note::\n |  - :math:`E(X) = \\mu`\n |  - :math:`Var(X) = 1/        au`\n |  \n |  Method resolution order:\n |      Normal\n |      pymc3.distributions.distribution.Continuous\n |      pymc3.distributions.distribution.Distribution\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, mu=0.0, tau=None, sd=None, *args, **kwargs)\n |  \n |  logp(self, value)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __getnewargs__(self)\n |  \n |  default(self)\n |  \n |  get_test_val(self, val, defaults)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  dist(*args, **kwargs) from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __new__(cls, name, *args, **kwargs)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\nHaving defined the priors, the next statement creates the expected value \nmu\n of the outcomes, specifying the linear relationship:\n\n\nmu = alpha + beta * X\n\n\n\n\nThis creates a \ndeterministic\n random variable, which implies that its value is \ncompletely\n determined by its parents' values. That is, there is no uncertainty beyond that which is inherent in the parents' values. Here, \nmu\n is just the sum of the intercept \nalpha\n and the product of the slope \nbeta\n and the predictor variable, whatever their values may be. PyMC3 random variables and data can be arbitrarily added, subtracted, divided, multiplied together and indexed-into to create new random variables. This allows for great model expressivity. Many common mathematical functions like \nsum\n, \nsin\n, \nexp\n and linear algebra functions like \ndot\n (for inner product) and \ninv\n (for inverse) are also provided. \n\n\nThe final line of the model, defines \nY_obs\n, the sampling distribution of the outcomes in the dataset.\n\n\nY_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)\n\n\n\n\nThis is a special case of a stochastic variable that we call an \nobserved stochastic\n, and represents the data likelihood of the model. It is identical to a standard stochastic, except that its \nobserved\n argument, which passes the data two the variable, indicates that the values for this variable were observed, and should not be changed by any fitting algorithm applied to the model. The data can be passed in the form of either a \nnumpy.ndarray\n or \npandas.DataFrame\n object.\n\n\nNotice that, unlike for the priors of the model, the parameters for the normal distribution of \nY_obs\n are not fixed values, but rather are the deterministic object \nmu\n and the stochastic \nsigma\n. This creates parent-child relationships between the likelihood and these two variables.\n\n\nModel fitting\n\n\nHaving completely specified our model, the next step is to obtain posterior estimates for the unknown variables in the model. Ideally, we could calculate the posterior analytically, but for most non-trivial models, this is not feasible. We will consider two approaches, whose appropriateness depends on the structure of the model and the goals of the analysis: finding the \nmaximum a posteriori\n (MAP) point using optimization methods, and computing summaries based on samples drawn from the posterior distribution using Markov Chain Monte Carlo (MCMC) sampling methods.\n\n\nMaximum a posteriori methods\n\n\nThe \nmaximum a posteriori (MAP)\n estimate for a model, is the mode of the posterior distribution and is generally found using numercal optimization methods. This is often fast and easy to do, but only gives a point estimate for the parameters and can be biased if the mode isn't representative of the distribution. PyMC3 provides this functionality with the \nfind_MAP\n function.\n\n\nBelow we find the MAP for our original model. The MAP is returned as a parameter \npoint\n, which is always represented by a Python dictionary of variable names to NumPy arrays of parameter values. \n\n\nfrom pymc3 import find_MAP\n\nmap_estimate = find_MAP(model=basic_model)\n\nprint(map_estimate)\n\n\n\n\n{'sigma': array(1.1211859035810363), 'beta': array([ 1.4679294 ,  0.29358588]), 'alpha': array(1.0136583610288663)}\n\n\n\nBy default, this uses Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm to find the maximum of the log-posterior but also allows selection of other optimization algorithms from the \nscipy.optimize\n module. For example, below we use Powell's method to find the MAP.\n\n\nfrom scipy import optimize\n\nmap_estimate = find_MAP(model=basic_model, fmin=optimize.fmin_powell)\n\nprint(map_estimate)\n\n\n\n\n{'sigma': array(1.1214023015580523), 'beta': array([ 1.1284382,  1.9605768]), 'alpha': array(1.015718140050394)}\n\n\n\nIt is important to note that the MAP estimate is not always reasonable, especially if the mode is at an extreme. This can be a subtle issue; with high dimensional posteriors, one can have areas of extremely high density but low total probability because the volume is very small. This will often occur in hierarchical models with the variance parameter for the random effect. If the individual group means are all the same, the posterior will have near infinite density if the scale parameter for the group means is almost zero, even though the probability of such a small scale parameter will be small since the group means must be extremely close together. \n\n\nMost techniques for finding the MAP estimate also only find a \nlocal\n optimium (which is often good enough), but can fail badly for multimodal posteriors if the different modes are meaningfully different.\n\n\nSampling methods\n\n\nThough finding the MAP is a fast and easy way of obtaining estimates of the unknown model parameters, it is limited because there is no associated estimate of uncertainty produced with the MAP estimates. Instead, a simulation-based approach such as Markov chain Monte Carlo (MCMC) can be used to obtain a Markov chain of values that, given the satisfaction of certain conditions, are indistinguishable from samples from the posterior distribution. \n\n\nTo conduct MCMC sampling to generate posterior samples in PyMC3, we specify a \nstep method\n object that corresponds to a particular MCMC algorithm, such as Metropolis, Slice sampling, or the No-U-Turn Sampler (NUTS). PyMC3's \nstep_methods\n submodule contains the following samplers: \nNUTS\n, \nMetropolis\n, \nSlice\n, \nHamiltonianMC\n, and \nBinaryMetropolis\n.\n\n\nGradient-based sampling methods\n\n\nPyMC3 has the standard sampling algorithms like adaptive Metropolis-Hastings and adaptive slice sampling, but PyMC3's most capable step method is the No-U-Turn Sampler. NUTS is especially useful on models that have many continuous parameters, a situatiuon where other MCMC algorithms work very slowly. It takes advantage of information about where regions of higher probability are, based on the gradient of the log posterior-density. This helps them achieve dramatically faster convergence on large problems than traditional sampling methods achieve. PyMC3 relies on Theano to analytically compute model gradients via automatic differentation of the computational of the posterior density. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo. For random variables that are undifferentiable (namely, discrete variables) NUTS and HMC cannot be used, but they may still be used on the differentiable variables in a model that contains undifferentiable variables. \n\n\nBoth NUTS and HMC require a scaling matrix parameter, which is analogous to the variance parameter for the jump proposal distribution in Metropolis-Hastings, althrough HMC and NUTS use it somewhat differently. The matrix gives the rough shape of the distribution so that NUTS does not make jumps that are too large in some directions and too small in other directions. It is important to set this scaling parameter to a reasonable value to facilitate efficient sampling. This is especially true for models that have many unobserved stochastic random variables or models with highly non-normal posterior distributions. Poor scaling parameters will slow down NUTS significantly, sometimes almost stopping it completely. A reasonable starting point for sampling can also be important for efficient sampling, but not as often.\n\n\nFortunately NUTS can often make good guesses for the scaling parameters. If you pass a point in parameter space (as a dictionary of variable names to parameter values, the same format as returned by \nfind_MAP\n) to HMC or NUTS, they will look at the local curvature of the log posterior-density (the diagonal of the Hessian matrix) at that point to make a guess for a good scaling vector, which often results in a good value. The MAP estimate is often a good point to use to initiate sampling. It is also possible to supply your own vector or scaling matrix to HMC/NUTS, though this is a more advanced use. If you wish to modify a Hessian at a specific point to use as your scaling matrix or vector, you can use \nfind_hessian\n or \nfind_hessian_diag\n.\n\n\nFor our linear regression example in \nbasic_model\n, we will use NUTS to sample 500 draws from the posterior using the MAP as the starting point and scaling point. This must also be performed inside the context of the model.\n\n\nfrom pymc3 import NUTS, sample\n\nwith basic_model:\n\n    # obtain starting values via MAP\n    start = find_MAP(fmin=optimize.fmin_powell)\n\n    # instantiate sampler\n    step = NUTS(scaling=start) \n\n    # draw 500 posterior samples\n    trace = sample(500, step, start=start) \n\n\n\n\n [-----------------100%-----------------] 500 of 500 complete in 3.0 sec\n\n/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)\n\n\n\nThe \nsample\n function returns a \ntrace\n object that can be queried in a similar way to a \ndict\n containing a map from variable names to \nnumpy.array\ns. The first dimension of the array is the sampling index and the later dimensions match the shape of the variable. We can see the last 5 values for the \nalpha\n variable as follows\n\n\ntrace['alpha'][-5:]\n\n\n\n\narray([ 0.56381045,  0.80554633,  0.96843812,  1.03525559,  1.2003014 ])\n\n\n\nPosterior analysis\n\n\nPyMC3\n provides plotting and summarization functions for inspecting the sampling output. A simple posterior plot can be created using \ntraceplot\n.\n\n\nfrom pymc3 import traceplot\n\ntraceplot(trace);\n\n\n\n\n\n\nThe left column consists of a smoothed histogram (using kernel density estimation) of the marginal posteriors of each stochastic random variable while the right column contains the samples of the Markov chain plotted in sequential order. The \nbeta\n variable, being vector-valued, produces two histograms and two sample traces, corresponding to both predictor coefficients.\n\n\nIn addition, \nsummary\n provides a text-based output of common posterior statistics:\n\n\nfrom pymc3 import summary\n\nsummary(trace)\n\n\n\n\nalpha:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.976            0.237            0.014            [0.516, 1.387]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.537          0.818          0.974          1.142          1.409\n\n\nbeta:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.638            2.089            0.177            [-3.268, 5.015]\n  -0.250           10.300           0.876            [-23.525, 17.747]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -2.876         0.275          1.491          2.951          6.163\n  -22.137        -7.277         0.509          6.698          23.299\n\n\nsigma:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.151            0.094            0.004            [0.958, 1.311]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.987          1.081          1.147          1.217          1.353\n\n\n\nCase study 1: Stochastic volatility\n\n\nWe present a case study of stochastic volatility, time varying stock market volatility, to illustrate PyMC3's use in addressing a more realistic problem. The distribution of market returns is highly non-normal, which makes sampling the volatlities significantly more difficult. This example has 400+ parameters so using common sampling algorithms like Metropolis-Hastings would get bogged down, generating highly autocorrelated samples. Instead, we use NUTS, which is dramatically more efficient.\n\n\nThe Model\n\n\nAsset prices have time-varying volatility (variance of day over day \nreturns\n). In some periods, returns are highly variable, while in others they are very stable. Stochastic volatility models address this with a latent volatility variable, which changes over time. The following model is similar to the one described in the NUTS paper (Hoffman 2014, p. 21).\n\n\n$$\\begin{aligned} \n  \\sigma \n\\sim exp(50) \\\\\n  \\nu \n\\sim exp(.1) \\\\\n  s_i \n\\sim \\mathcal{N}(s_{i-1}, \\sigma^{-2}) \\\\\n  log(y_i) \n\\sim t(\\nu, 0, exp(-2 s_i))\n\\end{aligned}$$\n\n\nHere, \n$y$\n is the daily return series which is modeled with a Student-t distribution with an unknown degrees of freedom parameter, and a scale parameter determined by a latent process \n$s$\n. The individual \n$s_i$\n are the individual daily log volatilities in the latent log volatility process. \n\n\nThe Data\n\n\nOur data consist of the last 400 daily returns of the S\nP 500.\n\n\nn = 400\nreturns = np.genfromtxt(\ndata/SP500.csv\n)[-n:]\npl.plot(returns);\n\n\n\n\n\n\nModel Specification\n\n\nAs with the linear regession example, specifying the model in PyMC3 mirrors its statistical specification. This model employs several new distributions: the \nExponential\n distribution for the \n$ \\nu $\n and \n$\\sigma$\n priors, the student-t (\nT\n) distribution for distribution of returns, and the \nGaussianRandomWalk\n for the prior for the latent volatilities.   \n\n\nIt is easier to sample the scale of the log volatility process innovations, \n$\\sigma$\n, on a log scale, so we create it using the model's \nTransformedVar\n method and use the appropriate transformation, \nlogtransform\n, as an argument. \nTransformedVar\n creates one variable in the transformed space and one in the normal space, whereby the one in the transformed space (here \n$\\text{log}(\\sigma) $\n) is the one over which sampling will occur, and the one in the normal space is used throughout the rest of the model. The required arguments for \nTransformedVar\n are a variable name, a distribution and a transformation to use.\n\n\nAlthough, unlike model specifiation in PyMC2, we do not typically provide starting points for variables at the model specification stage, we can also provide an initial value for any distribution (called a \"test value\") using the \ntestval\n argument. This overrides the default test value for the distribution (usually the mean, median or mode of the distribution), and is most often useful if some values are illegal and we want to ensure we select a legal one. The test values for the distributions are also used as a starting point for sampling and optimization by default, though this is easily overriden. \n\n\nThe vector of latent volatilities \ns\n is given a prior distribution by \nGaussianRandomWalk\n. As its name suggests GaussianRandomWalk is a vector valued distribution where the values of the vector form a random normal walk of length n, as specified by the \nshape\n argument. The scale of the innovations of the random walk, \nsigma\n, is specified in terms of the precision of the normally distributed innovations and can be a scalar or vector. \n\n\nfrom pymc3 import Exponential, T, logtransform, exp, Deterministic\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nwith Model() as sp500_model:\n\n    nu = Exponential('nu', 1./10, testval=.1)\n\n    sigma, log_sigma = sp500_model.TransformedVar('sigma', Exponential.dist(1./.02, testval=.1),\n                                            logtransform)\n\n    s = GaussianRandomWalk('s', sigma**-2, shape=n)\n\n    volatility_process = Deterministic('volatility_process', exp(-2*s))\n\n    r = T('r', nu, lam=volatility_process, observed=returns)\n\n\n\n\nNotice that we transform the log volatility process \ns\n into the volatility process by \nexp(-2*s)\n. Here, \nexp\n is a Theano function, rather than the corresponding function in NumPy; Theano provides a large subset of the mathematical functions that NumPy does.\n\n\nAlso note that we have declared the \nModel\n name \nsp500_model\n in the first occurrence of the context manager, rather than splitting it into two lines, as we did for the first example.\n\n\nFitting\n\n\nBefore we draw samples from the posterior, it is prudent to find a decent starting valuwa by finding a point of relatively high probability. For this model, the full \nmaximum a posteriori\n (MAP) point over all variables is degenerate and has infinite density. But, if we fix \nlog_sigma\n and \nnu\n it is no longer degenerate, so we find the MAP with respect only to the volatility process \ns\n keeping \nlog_sigma\n and \nnu\n constant at their default values (remember that we set \ntestval=.1\n for \nsigma\n). We use the Limited-memory BFGS (L-BFGS) optimizer, which is provided by the \nscipy.optimize\n package, as it is more efficient for high dimensional functions and we have 400 stochastic random variables (mostly from \ns\n).\n\n\nTo do the sampling, we do a short initial run to put us in a volume of high probability, then start again at the new starting point. \ntrace[-1]\n gives us the last point in the sampling trace. NUTS will recalculate the scaling parameters based on the new point, and in this case it leads to faster sampling due to better scaling.\n\n\nimport scipy\nwith sp500_model:\n    start = find_MAP(vars=[s], fmin=scipy.optimize.fmin_l_bfgs_b)\n\n    step = NUTS(scaling=start)\n    trace = sample(50, step, progressbar=False)\n\n    # Start next run at the last sampled position.\n    step = NUTS(scaling=trace[-1], gamma=.25)\n    trace = sample(400, step, start=trace[-1])\n\n\n\n\n [-----------------100%-----------------] 400 of 400 complete in 32.0 sec\n\n\n\nWe can check our samples by looking at the traceplot for \nnu\n and \nlog_sigma\n.\n\n\n#figsize(12,6)\ntraceplot(trace, [nu, log_sigma]);\n\n\n\n\n\n\nFinally we plot the distribution of volatility paths by plotting many of our sampled volatility paths on the same graph. Each is rendered partially transparent (via the \nalpha\n argument in Matplotlib's \nplot\n function) so the regions where many paths overlap are shaded more darkly.\n\n\npl.title(str(volatility_process));\npl.plot(trace[volatility_process][::10].T,'b', alpha=.03);\npl.xlabel('time');\npl.ylabel('log volatility');\n\n\n\n\n\n\nCase study 2: Occupancy estimation\n\n\nEcologists often use survey data to make inferences regarding the abundance and distribution of plants and animals. Such data are often \nzero-inflated\n, whereby there are more zeros observed than you would expect if the data were distributed according to some common distribution. This is sometimes due to habitat heterogeneity, which causes areas of low quality to be unoccupied by a particular species. However, some sites may be unoccupied simply due to chance.\n\n\nHere is an example of such data; each element in the array (n=100) represents a count of a particular species among a set of sites. The data are clearly zero-inflated:\n\n\ny = np.array([0, 2, 1, 0, 4, 2, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 6, 0, 0, 0, 2, 1,\n       2, 0, 0, 0, 1, 0, 0, 0, 4, 2, 0, 0, 0, 1, 0, 2, 4, 0, 0, 1, 0, 0, 0,\n       0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0,\n       0, 0, 3, 0, 2, 0, 1, 2, 2, 2, 2, 3, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 0,\n       0, 0, 2, 0, 0, 1, 0, 0])\n\npl.hist(y, bins=range(7));\n\n\n\n\n\n\nOne approach for dealing with excess zeros is to use a \nmixture model\n. The mixture model contains two components: one which models the count data without inflated zeros (here, an abundance model), and another that accounts for the occurrence of excess zeros (here, a habitat suitability model). In this model the, abundance component is conditional on the habitat being suitable. Suitability is a binary variable, which indicates suitability (\n$z=1$\n) with some probability \n$p$\n and unsuitability (\n$z=0$\n) with probability \n$1-p$\n. If it is a suitable habitat then the abundance is modeled according to a Poisson distributtion with mean and varaince \n$\\theta$\n, whereas unsuitable patches always have zero aundance.\n\n\n$$\\begin{aligned}\n  p \n\\sim Beta(1,1) \\\\\n  \\theta \n\\sim Unif(0,100) \\\\\n  z_i \n\\sim \\text{Bernoulli}(p) \\\\\n  (y_i|z_i=1) \n\\sim \\text{Poisson}(\\theta) \\\\ \n  (y_i|z_i=0) \n= 0\n\\end{aligned}$$\n\n\nPyMC3 includes a \nZeroInflatedPoisson\n distribution class among its standard distributions, which takes a conditional mean parameter as well as an array of indicators for the excess zeros. Since we do not know which zeros are excess \na priori\n, this array is modeled as a latent variable using a Bernoulli distribution, with a hyperparameter representing the occupancy rate.\n\n\nfrom pymc3 import Beta, Bernoulli, ZeroInflatedPoisson, Uniform, Poisson\n\nwith Model() as zip_model:\n\n    # Estimated occupancy\n    p = Beta('p', 1, 1)\n\n    # Latent variable for occupancy\n    z = Bernoulli('z', p, shape=y.shape)\n\n    # Estimated mean count\n    theta = Uniform('theta', 0, 100)\n\n    # Poisson likelihood\n    yd = ZeroInflatedPoisson('y', theta, z, observed=y)\n\n\n\n\nNotice that since the latent occupancy indicators are discrete, we cannot use a gradient-based MCMC step method like HMC or NUTS for this variable. Instead, we will sample using a \nBinaryMetropolis\n sampler that proposes only binary values at each iteration for \nz\n; for the continuous-valued parameters, \ntheta\n and \np\n we will use a standard \nMetropolis\n sampler. \n\n\nWe sample with both samplers at once by passing them to \nsample\n in a list. Each new sample is generated by first applying \nstep1\n then \nstep2\n.\n\n\nfrom pymc3 import Metropolis, BinaryMetropolis, sample \n\nwith zip_model:\n\n    start = {'p': 0.5, 'z': (y \n 0), 'theta': 5, 'yd_missing': np.array([1,1])}\n\n\n    step1 = Metropolis([theta, p])\n\n    step2 = BinaryMetropolis([z])\n\n    trace = sample(10000, [step1, step2], start)\n\n\n\n\n [-----------------100%-----------------] 10000 of 10000 complete in 5.4 sec\n\n\n\nThe resulting posteriors for the unknown parameters suggest an occupancy rate in the neighborhood of 0.3 to 0.4, and an expected count (conditional on occupancy) of just over 2.\n\n\ntraceplot(trace[5000:], vars=['p', 'theta']);\n\n\n\n\n\n\nArbitrary deterministics\n\n\nDue to its reliance on Theano, PyMC3 provides many mathematical functions and operators for transforming random variables into new random variables. However, the library of functions in Theano is not exhaustive, therefore Theano and PyMC3 provide functionality for creating arbitrary Theano functions in pure Python, and including these functions in PyMC models. This is supported with the \nas_op\n function decorator.\n\n\nTheano needs to know the types of the inputs and outputs of a function, which are specified for \nas_op\n by \nitypes\n for inputs and \notypes\n for outputs. The Theano documentation includes \nan overview of the available types\n.\n\n\nimport theano.tensor as T \nfrom theano.compile.ops import as_op\n\n@as_op(itypes=[T.lscalar], otypes=[T.lscalar])\ndef crazy_modulo3(value):\n    if value \n 0: \n        return value % 3\n    else :\n        return (-value + 1) % 3\n\nwith Model() as model_deterministic:\n    a = Poisson('a', 1)\n    b = crazy_modulo3(a)\n\n\n\n\nAn important drawback of this approach is that it is not possible for \ntheano\n to inspect these functions in order to compute the gradient required for the Hamiltonian-based samplers. Therefore, it is not possible to use the HMC or NUTS samplers for a model that uses such an operator. However, it is possible to add a gradient if we inherit from \ntheano.Op\n instead of using \nas_op\n. The PyMC example set includes \na more elaborate example of the usage of \nas_op\n.\n\n\nArbitrary distributions\n\n\nSimilarly, the library of statistical distributions in PyMC3 is not exhaustive, but PyMC allows for the creation of user-defined functions for an arbitrary probability distribution. For simple statistical distributions, the \nDensityDist\n function takes as an argument any function that calculates a log-probability \n$log(p(x))$\n. This function may employ other random variables in its calculation. Here is an example inspired by a blog post by Jake Vanderplas on which priors to use for a linear regression (Vanderplas, 2014). \n\n\nimport theano.tensor as T\nfrom pymc3 import DensityDist\n\nwith Model() as model:\n    alpha = Uniform('intercept', -100, 100)\n\n    # Create custom densities\n    beta = DensityDist('beta', lambda value: -1.5 * T.log(1 + value**2), testval=0)\n    eps = DensityDist('eps', lambda value: -T.log(T.abs_(value)), testval=1)\n\n    # Create likelihood\n    like = Normal('y_est', mu=alpha + beta * X, sd=eps, observed=Y)\n\n\n\n\nFor more complex distributions, one can create a subclass of \nContinuous\n or \nDiscrete\n and provide the custom \nlogp\n function, as required. This is how the built-in distributions in PyMC are specified. As an example, fields like psychology and astrophysics have complex likelihood functions for a particular process that may require numerical approximation. In these cases, it is impossible to write the function in terms of predefined theano operators and we must use a custom theano operator using \nas_op\n or inheriting from \ntheano.Op\n. \n\n\nImplementing the \nbeta\n variable above as a \nContinuous\n subclass is shown below, along with a sub-function using the \nas_op\n decorator, though this is not strictly necessary.\n\n\nfrom pymc3.distributions import Continuous\n\nclass Beta(Continuous):\n    def __init__(self, mu, *args, **kwargs):\n        super(Beta, self).__init__(*args, **kwargs)\n        self.mu = mu\n        self.mode = mu\n\n    def logp(self, value):\n        mu = self.mu\n        return beta_logp(value - mu)\n\n@as_op(itypes=[T.dscalar], otypes=[T.dscalar])\ndef beta_logp(value):\n    return -1.5 * np.log(1 + (value)**2)\n\n\nwith Model() as model:\n    beta = Beta('slope', mu=0, testval=0)\n\n\n\n\nGeneralized Linear Models\n\n\nGeneralized Linear Models (GLMs) are a class of flexible models that are widely used to estimate regression relationships between a single outcome variable and one or multiple predictors. Because these models are so common, \nPyMC3\n offers a \nglm\n submodule that allows flexible creation of various GLMs with an intuitive \nR\n-like syntax that is implemented via the \npatsy\n module.\n\n\nThe \nglm\n submodule requires data to be included as a \npandas\n \nDataFrame\n. Hence, for our linear regression example:\n\n\n# Convert X and Y to a pandas DataFrame\nimport pandas \ndf = pandas.DataFrame({'x1': X1, 'x2': X2, 'y': Y})\n\n\n\n\nThe model can then be very concisely specified in one line of code.\n\n\nfrom pymc3.glm import glm\n\nwith Model() as model_glm:\n    glm('y ~ x1 + x2', df)\n\n\n\n\nThe error distribution, if not specified via the \nfamily\n argument, is assumed to be normal. In the case of logistic regression, this can be modified by passing in a \nBinomial\n family object.\n\n\nfrom pymc3.glm.families import Binomial\n\ndf_logistic = pandas.DataFrame({'x1': X1, 'x2': X2, 'y': Y \n 0})\n\nwith Model() as model_glm_logistic:\n    glm('y ~ x1 + x2', df_logistic, family=Binomial())\n\n\n\n\nBackends\n\n\nPyMC3\n has support for different ways to store samples during and after sampling, called backends, including in-memory (default), text file, and SQLite. These can be found in \npymc.backends\n:\n\n\nBy default, an in-memory \nndarray\n is used but if the samples would get too large to be held in memory we could use the \nsqlite\n backend:\n\n\nfrom pymc3.backends import SQLite\n\nwith model_glm_logistic:\n    backend = SQLite('trace.sqlite')\n    trace = sample(5000, Metropolis(), trace=backend)\n\n\n\n\n [-----------------100%-----------------] 5000 of 5000 complete in 3.8 sec\n\n\n\nsummary(trace, vars=['x1', 'x2'])\n\n\n\n\nx1:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  -11.926          6.125            0.610            [-20.747, -1.430]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -20.409        -16.261        -14.169        -4.829         -1.018\n\n\nx2:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  59.498           30.716           3.059            [5.914, 102.835]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  5.031          22.983         70.320         80.770         102.425\n\n\n\nThe stored trace can then later be loaded using the \nload\n command:\n\n\nfrom pymc3.backends.sqlite import load\n\nwith basic_model:\n    trace_loaded = load('trace.sqlite')\n\ntrace_loaded \n\n\n\n\nMultiTrace: 1 chains, 5000 iterations, 4 variables\n\n\n\n\nMore information about \nbackends\n can be found in the docstring of \npymc.backends\n.\n\n\nReferences\n\n\nPatil, A., D. Huard and C.J. Fonnesbeck. (2010) PyMC: Bayesian Stochastic Modelling in Python. Journal of Statistical Software, 35(4), pp. 1-81\n\n\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. (2012) \u201cTheano: new features and speed improvements\u201d. NIPS 2012 deep learning workshop.\n\n\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010) \u201cTheano: A CPU and GPU Math Expression Compiler\u201d. Proceedings of the Python for Scientific Computing Conference (SciPy) 2010. June 30 - July 3, Austin, TX\n\n\nLunn, D.J., Thomas, A., Best, N., and Spiegelhalter, D. (2000) WinBUGS -- a Bayesian modelling framework: concepts, structure, and extensibility. Statistics and Computing, 10:325--337.\n\n\nNeal, R.M. Slice sampling. Annals of Statistics. (2003). doi:10.2307/3448413.\n\n\nvan Rossum, G. The Python Library Reference Release 2.6.5., (2010). URL http://docs.python.org/library/.\n\n\nDuane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987) \u201cHybrid Monte Carlo\u201d, Physics Letters, vol. 195, pp. 216-222.\n\n\nStan Development Team. (2014). Stan: A C++ Library for Probability and Sampling, Version 2.5.0.   http://mc-stan.org. \n\n\nGamerman, D. Markov Chain Monte Carlo: statistical simulation for Bayesian inference. Chapman and Hall, 1997.\n\n\nHoffman, M. D., \n Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research, 30.\n\n\nVanderplas, Jake. \"Frequentism and Bayesianism IV: How to be a Bayesian in Python.\" Pythonic Perambulations. N.p., 14 Jun 2014. Web. 27 May. 2015. \nhttps://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/\n.", 
            "title": "Getting started"
        }, 
        {
            "location": "/getting_started/#probabilistic-programming-in-python-using-pymc", 
            "text": "Authors: John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck", 
            "title": "Probabilistic Programming in Python using PyMC"
        }, 
        {
            "location": "/getting_started/#introduction", 
            "text": "Probabilistic Programming (PP) allows flexible specification of statistical Bayesian models in code. PyMC3 is a new, open-source PP framework with an intutive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). This class of samplers work well on high dimensional and complex posterior distributions and allows many complex models to be fit without specialized knowledge about fitting algorithms. HMC and NUTS take advantage of gradient information from the likelihood to achieve much faster convergence than traditional sampling methods, especially for larger models. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo, which means you usually don't need to have specialized knowledge about how the algorithms work. PyMC3, Stan (Stan Development Team, 2014), and the LaplacesDemon package for R are currently the only PP packages to offer HMC.  Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.  While most of PyMC3's user-facing features are written in pure Python, it leverages Theano (Bergstra et al., 2010) to transparently transcode models to C and compile it to machine code, thereby boosting performance. Theano is a library that allows expressions to be defined using generalized vector data structures called  tensors , which are tightly integrated with the popular NumPy  ndarray  data structure, and similarly allows for broadcasting and advanced indexing, just as NumPy arrays do. Theano also automatically optimizes the likelihood's computational graph for speed and provides simple GPU integration.  Here, we present a primer on the use of PyMC3 for solving general Bayesian statistical inference and prediction problems. We will first see the basics of how to use PyMC3, motivated by a simple example: installation, data creation, model definition, model fitting and posterior analysis. Then we will cover two case studies and use them to show how to define and fit more sophisticated models. Finally we will show how to extend PyMC3 and discuss other useful features: the Generalized Linear Models subpackage, custom distributions, custom transformations and alternative storage backends.", 
            "title": "Introduction"
        }, 
        {
            "location": "/getting_started/#installation", 
            "text": "Running PyMC3 requires a working Python interpreter, either version 2.7 (or more recent) or 3.4 (or more recent); we recommend that new users install version 3.4. A complete Python installation for Mac OSX, Linux and Windows can most easily be obtained by downloading and installing the free  Anaconda Python Distribution  by ContinuumIO.   PyMC3  can also be installed manually using  pip  (https://pip.pypa.io/en/latest/installing.html):  pip install git+https://github.com/pymc-devs/pymc3  PyMC3 depends on several third-party Python packages which will be automatically installed when installing via pip. The four required dependencies are:  Theano ,  NumPy ,  SciPy , and  Matplotlib . To take full advantage of PyMC3, the optional dependencies  Pandas  and  Patsy  should also be installed. These are  not  automatically installed, but can be installed by:  pip install patsy pandas  The source code for PyMC3 is hosted on GitHub at https://github.com/pymc-devs/pymc3 and is distributed under the liberal  Apache License 2.0 . On the GitHub site, users may also report bugs and other issues, as well as contribute code to the project, which we actively encourage.", 
            "title": "Installation"
        }, 
        {
            "location": "/getting_started/#a-motivating-example-linear-regression", 
            "text": "To introduce model definition, fitting and posterior analysis, we first consider a simple Bayesian linear regression model with normal priors for the parameters. We are interested in predicting outcomes  $Y$  as normally-distributed observations with an expected value  $\\mu$  that is a linear function of two predictor variables,  $X_1$  and  $X_2$ .  $$\\begin{aligned} \nY   \\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n\\mu  = \\alpha + \\beta_1 X_1 + \\beta_2 X_2\n\\end{aligned}$$  where  $\\alpha$  the intercept, and  $\\beta_i$  the coefficient for covariate  $X_i$ , while  $\\sigma$  represents the observation error. Since we are constructing a Bayesian model, the unknown variables in the model must be assigned a prior distribution. Our choices will be zero-mean normal priors with variance of 100 for both regression coefficients (which corresponds to relatively  diffuse  information regarding the true parameter values), and  $\\sigma$  is modeled as the absolute of a Normal distribution (so-called  HalfNormal ).   $$\\begin{aligned} \n\\alpha  \\sim \\mathcal{N}(0, 100) \\\\\n\\beta_i  \\sim \\mathcal{N}(0, 100) \\\\\n\\sigma  \\sim \\lvert\\mathcal{N}(0, 1){\\rvert}\n\\end{aligned}$$  Generating data  We can simulate some artificial data from this model using only NumPy's  random  module, and then use PyMC3 to try to recover the corresponding parameters. We are intentionally generating the data to closely correspond the PyMC3 model structure.  import numpy as np\n\n# Intialize random number generator\nnp.random.seed(123)\n\n# True parameter values\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\n\n# Size of dataset\nsize = 100\n\n# Predictor variable\nX1 = np.linspace(0, 1, size)\nX2 = np.linspace(0,.2, size)\n\n# Simulate outcome variable\nY = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma  Here is what the simulated data look like. We use the  pylab  module from the plotting library matplotlib.   %matplotlib inline \nimport pylab as pl\n\nfig, axes = pl.subplots(1, 2, sharex=True, figsize=(10,4))\naxes[0].scatter(X1, Y)\naxes[1].scatter(X2, Y)\naxes[0].set_ylabel('Y'); axes[0].set_xlabel('X1'); axes[1].set_xlabel('X2');   Model Specification  Specifiying this model in PyMC3 is straightforward because the syntax is as close to the statistical notation. For the most part, each line of Python code corresponds to a line in the model notation above.   First, we import the components we will need from PyMC.  from pymc3 import Model, Normal, HalfNormal  Now we build our model, which we will present in full first, then explain each part line-by-line.  basic_model = Model()\n\nwith basic_model:\n\n    # Priors for unknown model parameters\n    alpha = Normal('alpha', mu=0, sd=10)\n    beta = Normal('beta', mu=0, sd=10, shape=2)\n    sigma = HalfNormal('sigma', sd=1)\n\n    # Expected value of outcome\n    mu = alpha + beta[0]*X1 + beta[1]*X2\n\n    # Likelihood (sampling distribution) of observations\n    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)  The first line,  basic_model = Model()  creates a new  Model  object which is a container for the model random variables.  Following instantiation of the model, the subsequent specification of the model components is performed inside a   with  statement:  with basic_model:  This creates a  context manager , with our  basic_model  as the context, that includes all statements until the indented block ends. This means all PyMC3 objects introduced in the indented code block below the  with  statement are added to the model behind the scenes. Absent this context manager idiom, we would be forced to manually associate each of the variables with  basic_model  right after we create them. If you try to create a new random variable without a  with model:  statement, it will raise an error since there is no obvious model for the variable to be added to.  The first three statements in the context manager:  alpha = Normal('alpha', mu=0, sd=10)\nbeta = Normal('beta', mu=0, sd=10, shape=2)\nsigma = HalfNormal('sigma', sd=1)  create a  stochastic  random variables with a Normal prior distributions for the regression coefficients with a mean of 0 and standard deviation of 10 for the regression coefficients, and a half-normal distribution for the standard deviation of the observations,  $\\sigma$ . These are stochastic because their values are partly determined by its parents, which for priors are simple constants, and partly random (or stochastic).   We call the  Normal  constructor to create a random variable to use as a normal prior. The first argument is always the  name  of the random variable, which should almost always match the name of the Python variable being assigned to, since it sometimes used to retrieve the variable from the model for summarizing output. The remaining required arguments for a stochastic object are the parameters, in this case  mu , the mean, and  sd , the standard deviation, which we assign hyperparameter values for the model. In general, a distribution's parameters are values that determine the location, shape or scale of the random variable, depending on the parameterization of the distribution. Most commonly used distributions, such as  Beta ,  Exponential ,  Categorical ,  Gamma ,  Binomial  and many others, are available in PyMC3.  The  beta  variable has an additional  shape  argument to denote it as a vector-valued parameter of size 2. The  shape  argument is available for all distributions and specifies the length or shape of the random variable, but is optional for scalar variables, since it defaults to a value of one. It can be an integer, to specify an array, or a tuple, to specify a multidimensional array ( e.g.   shape=(5,7)  makes random variable that takes on 5 by 7 matrix values).   Detailed notes about distributions, sampling methods and other PyMC3 functions are available via the  help  function.  help(Normal) #try help(Model), help(Uniform) or help(basic_model)  Help on class Normal in module pymc3.distributions.continuous:\n\nclass Normal(pymc3.distributions.distribution.Continuous)\n |  Normal log-likelihood.\n |  \n |  .. math::\night\\}\n |  \n |  Parameters\n |  ----------\n |  mu : float\n |      Mean of the distribution.\n |  tau : float\n |      Precision of the distribution, which corresponds to\n |      :math:`1/\\sigma^2` (tau   0).\n |  sd : float\n |      Standard deviation of the distribution. Alternative parameterization.\n |  \n |  .. note::\n |  - :math:`E(X) = \\mu`\n |  - :math:`Var(X) = 1/        au`\n |  \n |  Method resolution order:\n |      Normal\n |      pymc3.distributions.distribution.Continuous\n |      pymc3.distributions.distribution.Distribution\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, mu=0.0, tau=None, sd=None, *args, **kwargs)\n |  \n |  logp(self, value)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __getnewargs__(self)\n |  \n |  default(self)\n |  \n |  get_test_val(self, val, defaults)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  dist(*args, **kwargs) from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __new__(cls, name, *args, **kwargs)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)  Having defined the priors, the next statement creates the expected value  mu  of the outcomes, specifying the linear relationship:  mu = alpha + beta * X  This creates a  deterministic  random variable, which implies that its value is  completely  determined by its parents' values. That is, there is no uncertainty beyond that which is inherent in the parents' values. Here,  mu  is just the sum of the intercept  alpha  and the product of the slope  beta  and the predictor variable, whatever their values may be. PyMC3 random variables and data can be arbitrarily added, subtracted, divided, multiplied together and indexed-into to create new random variables. This allows for great model expressivity. Many common mathematical functions like  sum ,  sin ,  exp  and linear algebra functions like  dot  (for inner product) and  inv  (for inverse) are also provided.   The final line of the model, defines  Y_obs , the sampling distribution of the outcomes in the dataset.  Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)  This is a special case of a stochastic variable that we call an  observed stochastic , and represents the data likelihood of the model. It is identical to a standard stochastic, except that its  observed  argument, which passes the data two the variable, indicates that the values for this variable were observed, and should not be changed by any fitting algorithm applied to the model. The data can be passed in the form of either a  numpy.ndarray  or  pandas.DataFrame  object.  Notice that, unlike for the priors of the model, the parameters for the normal distribution of  Y_obs  are not fixed values, but rather are the deterministic object  mu  and the stochastic  sigma . This creates parent-child relationships between the likelihood and these two variables.  Model fitting  Having completely specified our model, the next step is to obtain posterior estimates for the unknown variables in the model. Ideally, we could calculate the posterior analytically, but for most non-trivial models, this is not feasible. We will consider two approaches, whose appropriateness depends on the structure of the model and the goals of the analysis: finding the  maximum a posteriori  (MAP) point using optimization methods, and computing summaries based on samples drawn from the posterior distribution using Markov Chain Monte Carlo (MCMC) sampling methods.  Maximum a posteriori methods  The  maximum a posteriori (MAP)  estimate for a model, is the mode of the posterior distribution and is generally found using numercal optimization methods. This is often fast and easy to do, but only gives a point estimate for the parameters and can be biased if the mode isn't representative of the distribution. PyMC3 provides this functionality with the  find_MAP  function.  Below we find the MAP for our original model. The MAP is returned as a parameter  point , which is always represented by a Python dictionary of variable names to NumPy arrays of parameter values.   from pymc3 import find_MAP\n\nmap_estimate = find_MAP(model=basic_model)\n\nprint(map_estimate)  {'sigma': array(1.1211859035810363), 'beta': array([ 1.4679294 ,  0.29358588]), 'alpha': array(1.0136583610288663)}  By default, this uses Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) algorithm to find the maximum of the log-posterior but also allows selection of other optimization algorithms from the  scipy.optimize  module. For example, below we use Powell's method to find the MAP.  from scipy import optimize\n\nmap_estimate = find_MAP(model=basic_model, fmin=optimize.fmin_powell)\n\nprint(map_estimate)  {'sigma': array(1.1214023015580523), 'beta': array([ 1.1284382,  1.9605768]), 'alpha': array(1.015718140050394)}  It is important to note that the MAP estimate is not always reasonable, especially if the mode is at an extreme. This can be a subtle issue; with high dimensional posteriors, one can have areas of extremely high density but low total probability because the volume is very small. This will often occur in hierarchical models with the variance parameter for the random effect. If the individual group means are all the same, the posterior will have near infinite density if the scale parameter for the group means is almost zero, even though the probability of such a small scale parameter will be small since the group means must be extremely close together.   Most techniques for finding the MAP estimate also only find a  local  optimium (which is often good enough), but can fail badly for multimodal posteriors if the different modes are meaningfully different.  Sampling methods  Though finding the MAP is a fast and easy way of obtaining estimates of the unknown model parameters, it is limited because there is no associated estimate of uncertainty produced with the MAP estimates. Instead, a simulation-based approach such as Markov chain Monte Carlo (MCMC) can be used to obtain a Markov chain of values that, given the satisfaction of certain conditions, are indistinguishable from samples from the posterior distribution.   To conduct MCMC sampling to generate posterior samples in PyMC3, we specify a  step method  object that corresponds to a particular MCMC algorithm, such as Metropolis, Slice sampling, or the No-U-Turn Sampler (NUTS). PyMC3's  step_methods  submodule contains the following samplers:  NUTS ,  Metropolis ,  Slice ,  HamiltonianMC , and  BinaryMetropolis .  Gradient-based sampling methods  PyMC3 has the standard sampling algorithms like adaptive Metropolis-Hastings and adaptive slice sampling, but PyMC3's most capable step method is the No-U-Turn Sampler. NUTS is especially useful on models that have many continuous parameters, a situatiuon where other MCMC algorithms work very slowly. It takes advantage of information about where regions of higher probability are, based on the gradient of the log posterior-density. This helps them achieve dramatically faster convergence on large problems than traditional sampling methods achieve. PyMC3 relies on Theano to analytically compute model gradients via automatic differentation of the computational of the posterior density. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo. For random variables that are undifferentiable (namely, discrete variables) NUTS and HMC cannot be used, but they may still be used on the differentiable variables in a model that contains undifferentiable variables.   Both NUTS and HMC require a scaling matrix parameter, which is analogous to the variance parameter for the jump proposal distribution in Metropolis-Hastings, althrough HMC and NUTS use it somewhat differently. The matrix gives the rough shape of the distribution so that NUTS does not make jumps that are too large in some directions and too small in other directions. It is important to set this scaling parameter to a reasonable value to facilitate efficient sampling. This is especially true for models that have many unobserved stochastic random variables or models with highly non-normal posterior distributions. Poor scaling parameters will slow down NUTS significantly, sometimes almost stopping it completely. A reasonable starting point for sampling can also be important for efficient sampling, but not as often.  Fortunately NUTS can often make good guesses for the scaling parameters. If you pass a point in parameter space (as a dictionary of variable names to parameter values, the same format as returned by  find_MAP ) to HMC or NUTS, they will look at the local curvature of the log posterior-density (the diagonal of the Hessian matrix) at that point to make a guess for a good scaling vector, which often results in a good value. The MAP estimate is often a good point to use to initiate sampling. It is also possible to supply your own vector or scaling matrix to HMC/NUTS, though this is a more advanced use. If you wish to modify a Hessian at a specific point to use as your scaling matrix or vector, you can use  find_hessian  or  find_hessian_diag .  For our linear regression example in  basic_model , we will use NUTS to sample 500 draws from the posterior using the MAP as the starting point and scaling point. This must also be performed inside the context of the model.  from pymc3 import NUTS, sample\n\nwith basic_model:\n\n    # obtain starting values via MAP\n    start = find_MAP(fmin=optimize.fmin_powell)\n\n    # instantiate sampler\n    step = NUTS(scaling=start) \n\n    # draw 500 posterior samples\n    trace = sample(500, step, start=start)    [-----------------100%-----------------] 500 of 500 complete in 3.0 sec\n\n/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)  The  sample  function returns a  trace  object that can be queried in a similar way to a  dict  containing a map from variable names to  numpy.array s. The first dimension of the array is the sampling index and the later dimensions match the shape of the variable. We can see the last 5 values for the  alpha  variable as follows  trace['alpha'][-5:]  array([ 0.56381045,  0.80554633,  0.96843812,  1.03525559,  1.2003014 ])  Posterior analysis  PyMC3  provides plotting and summarization functions for inspecting the sampling output. A simple posterior plot can be created using  traceplot .  from pymc3 import traceplot\n\ntraceplot(trace);   The left column consists of a smoothed histogram (using kernel density estimation) of the marginal posteriors of each stochastic random variable while the right column contains the samples of the Markov chain plotted in sequential order. The  beta  variable, being vector-valued, produces two histograms and two sample traces, corresponding to both predictor coefficients.  In addition,  summary  provides a text-based output of common posterior statistics:  from pymc3 import summary\n\nsummary(trace)  alpha:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.976            0.237            0.014            [0.516, 1.387]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.537          0.818          0.974          1.142          1.409\n\n\nbeta:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.638            2.089            0.177            [-3.268, 5.015]\n  -0.250           10.300           0.876            [-23.525, 17.747]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -2.876         0.275          1.491          2.951          6.163\n  -22.137        -7.277         0.509          6.698          23.299\n\n\nsigma:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.151            0.094            0.004            [0.958, 1.311]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.987          1.081          1.147          1.217          1.353", 
            "title": "A Motivating Example: Linear Regression"
        }, 
        {
            "location": "/getting_started/#case-study-1-stochastic-volatility", 
            "text": "We present a case study of stochastic volatility, time varying stock market volatility, to illustrate PyMC3's use in addressing a more realistic problem. The distribution of market returns is highly non-normal, which makes sampling the volatlities significantly more difficult. This example has 400+ parameters so using common sampling algorithms like Metropolis-Hastings would get bogged down, generating highly autocorrelated samples. Instead, we use NUTS, which is dramatically more efficient.  The Model  Asset prices have time-varying volatility (variance of day over day  returns ). In some periods, returns are highly variable, while in others they are very stable. Stochastic volatility models address this with a latent volatility variable, which changes over time. The following model is similar to the one described in the NUTS paper (Hoffman 2014, p. 21).  $$\\begin{aligned} \n  \\sigma  \\sim exp(50) \\\\\n  \\nu  \\sim exp(.1) \\\\\n  s_i  \\sim \\mathcal{N}(s_{i-1}, \\sigma^{-2}) \\\\\n  log(y_i)  \\sim t(\\nu, 0, exp(-2 s_i))\n\\end{aligned}$$  Here,  $y$  is the daily return series which is modeled with a Student-t distribution with an unknown degrees of freedom parameter, and a scale parameter determined by a latent process  $s$ . The individual  $s_i$  are the individual daily log volatilities in the latent log volatility process.   The Data  Our data consist of the last 400 daily returns of the S P 500.  n = 400\nreturns = np.genfromtxt( data/SP500.csv )[-n:]\npl.plot(returns);   Model Specification  As with the linear regession example, specifying the model in PyMC3 mirrors its statistical specification. This model employs several new distributions: the  Exponential  distribution for the  $ \\nu $  and  $\\sigma$  priors, the student-t ( T ) distribution for distribution of returns, and the  GaussianRandomWalk  for the prior for the latent volatilities.     It is easier to sample the scale of the log volatility process innovations,  $\\sigma$ , on a log scale, so we create it using the model's  TransformedVar  method and use the appropriate transformation,  logtransform , as an argument.  TransformedVar  creates one variable in the transformed space and one in the normal space, whereby the one in the transformed space (here  $\\text{log}(\\sigma) $ ) is the one over which sampling will occur, and the one in the normal space is used throughout the rest of the model. The required arguments for  TransformedVar  are a variable name, a distribution and a transformation to use.  Although, unlike model specifiation in PyMC2, we do not typically provide starting points for variables at the model specification stage, we can also provide an initial value for any distribution (called a \"test value\") using the  testval  argument. This overrides the default test value for the distribution (usually the mean, median or mode of the distribution), and is most often useful if some values are illegal and we want to ensure we select a legal one. The test values for the distributions are also used as a starting point for sampling and optimization by default, though this is easily overriden.   The vector of latent volatilities  s  is given a prior distribution by  GaussianRandomWalk . As its name suggests GaussianRandomWalk is a vector valued distribution where the values of the vector form a random normal walk of length n, as specified by the  shape  argument. The scale of the innovations of the random walk,  sigma , is specified in terms of the precision of the normally distributed innovations and can be a scalar or vector.   from pymc3 import Exponential, T, logtransform, exp, Deterministic\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nwith Model() as sp500_model:\n\n    nu = Exponential('nu', 1./10, testval=.1)\n\n    sigma, log_sigma = sp500_model.TransformedVar('sigma', Exponential.dist(1./.02, testval=.1),\n                                            logtransform)\n\n    s = GaussianRandomWalk('s', sigma**-2, shape=n)\n\n    volatility_process = Deterministic('volatility_process', exp(-2*s))\n\n    r = T('r', nu, lam=volatility_process, observed=returns)  Notice that we transform the log volatility process  s  into the volatility process by  exp(-2*s) . Here,  exp  is a Theano function, rather than the corresponding function in NumPy; Theano provides a large subset of the mathematical functions that NumPy does.  Also note that we have declared the  Model  name  sp500_model  in the first occurrence of the context manager, rather than splitting it into two lines, as we did for the first example.  Fitting  Before we draw samples from the posterior, it is prudent to find a decent starting valuwa by finding a point of relatively high probability. For this model, the full  maximum a posteriori  (MAP) point over all variables is degenerate and has infinite density. But, if we fix  log_sigma  and  nu  it is no longer degenerate, so we find the MAP with respect only to the volatility process  s  keeping  log_sigma  and  nu  constant at their default values (remember that we set  testval=.1  for  sigma ). We use the Limited-memory BFGS (L-BFGS) optimizer, which is provided by the  scipy.optimize  package, as it is more efficient for high dimensional functions and we have 400 stochastic random variables (mostly from  s ).  To do the sampling, we do a short initial run to put us in a volume of high probability, then start again at the new starting point.  trace[-1]  gives us the last point in the sampling trace. NUTS will recalculate the scaling parameters based on the new point, and in this case it leads to faster sampling due to better scaling.  import scipy\nwith sp500_model:\n    start = find_MAP(vars=[s], fmin=scipy.optimize.fmin_l_bfgs_b)\n\n    step = NUTS(scaling=start)\n    trace = sample(50, step, progressbar=False)\n\n    # Start next run at the last sampled position.\n    step = NUTS(scaling=trace[-1], gamma=.25)\n    trace = sample(400, step, start=trace[-1])   [-----------------100%-----------------] 400 of 400 complete in 32.0 sec  We can check our samples by looking at the traceplot for  nu  and  log_sigma .  #figsize(12,6)\ntraceplot(trace, [nu, log_sigma]);   Finally we plot the distribution of volatility paths by plotting many of our sampled volatility paths on the same graph. Each is rendered partially transparent (via the  alpha  argument in Matplotlib's  plot  function) so the regions where many paths overlap are shaded more darkly.  pl.title(str(volatility_process));\npl.plot(trace[volatility_process][::10].T,'b', alpha=.03);\npl.xlabel('time');\npl.ylabel('log volatility');", 
            "title": "Case study 1: Stochastic volatility"
        }, 
        {
            "location": "/getting_started/#case-study-2-occupancy-estimation", 
            "text": "Ecologists often use survey data to make inferences regarding the abundance and distribution of plants and animals. Such data are often  zero-inflated , whereby there are more zeros observed than you would expect if the data were distributed according to some common distribution. This is sometimes due to habitat heterogeneity, which causes areas of low quality to be unoccupied by a particular species. However, some sites may be unoccupied simply due to chance.  Here is an example of such data; each element in the array (n=100) represents a count of a particular species among a set of sites. The data are clearly zero-inflated:  y = np.array([0, 2, 1, 0, 4, 2, 0, 0, 4, 0, 0, 0, 0, 0, 3, 0, 0, 6, 0, 0, 0, 2, 1,\n       2, 0, 0, 0, 1, 0, 0, 0, 4, 2, 0, 0, 0, 1, 0, 2, 4, 0, 0, 1, 0, 0, 0,\n       0, 0, 2, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0,\n       0, 0, 3, 0, 2, 0, 1, 2, 2, 2, 2, 3, 0, 0, 0, 0, 1, 0, 3, 1, 0, 0, 0,\n       0, 0, 2, 0, 0, 1, 0, 0])\n\npl.hist(y, bins=range(7));   One approach for dealing with excess zeros is to use a  mixture model . The mixture model contains two components: one which models the count data without inflated zeros (here, an abundance model), and another that accounts for the occurrence of excess zeros (here, a habitat suitability model). In this model the, abundance component is conditional on the habitat being suitable. Suitability is a binary variable, which indicates suitability ( $z=1$ ) with some probability  $p$  and unsuitability ( $z=0$ ) with probability  $1-p$ . If it is a suitable habitat then the abundance is modeled according to a Poisson distributtion with mean and varaince  $\\theta$ , whereas unsuitable patches always have zero aundance.  $$\\begin{aligned}\n  p  \\sim Beta(1,1) \\\\\n  \\theta  \\sim Unif(0,100) \\\\\n  z_i  \\sim \\text{Bernoulli}(p) \\\\\n  (y_i|z_i=1)  \\sim \\text{Poisson}(\\theta) \\\\ \n  (y_i|z_i=0)  = 0\n\\end{aligned}$$  PyMC3 includes a  ZeroInflatedPoisson  distribution class among its standard distributions, which takes a conditional mean parameter as well as an array of indicators for the excess zeros. Since we do not know which zeros are excess  a priori , this array is modeled as a latent variable using a Bernoulli distribution, with a hyperparameter representing the occupancy rate.  from pymc3 import Beta, Bernoulli, ZeroInflatedPoisson, Uniform, Poisson\n\nwith Model() as zip_model:\n\n    # Estimated occupancy\n    p = Beta('p', 1, 1)\n\n    # Latent variable for occupancy\n    z = Bernoulli('z', p, shape=y.shape)\n\n    # Estimated mean count\n    theta = Uniform('theta', 0, 100)\n\n    # Poisson likelihood\n    yd = ZeroInflatedPoisson('y', theta, z, observed=y)  Notice that since the latent occupancy indicators are discrete, we cannot use a gradient-based MCMC step method like HMC or NUTS for this variable. Instead, we will sample using a  BinaryMetropolis  sampler that proposes only binary values at each iteration for  z ; for the continuous-valued parameters,  theta  and  p  we will use a standard  Metropolis  sampler.   We sample with both samplers at once by passing them to  sample  in a list. Each new sample is generated by first applying  step1  then  step2 .  from pymc3 import Metropolis, BinaryMetropolis, sample \n\nwith zip_model:\n\n    start = {'p': 0.5, 'z': (y   0), 'theta': 5, 'yd_missing': np.array([1,1])}\n\n\n    step1 = Metropolis([theta, p])\n\n    step2 = BinaryMetropolis([z])\n\n    trace = sample(10000, [step1, step2], start)   [-----------------100%-----------------] 10000 of 10000 complete in 5.4 sec  The resulting posteriors for the unknown parameters suggest an occupancy rate in the neighborhood of 0.3 to 0.4, and an expected count (conditional on occupancy) of just over 2.  traceplot(trace[5000:], vars=['p', 'theta']);", 
            "title": "Case study 2: Occupancy estimation"
        }, 
        {
            "location": "/getting_started/#arbitrary-deterministics", 
            "text": "Due to its reliance on Theano, PyMC3 provides many mathematical functions and operators for transforming random variables into new random variables. However, the library of functions in Theano is not exhaustive, therefore Theano and PyMC3 provide functionality for creating arbitrary Theano functions in pure Python, and including these functions in PyMC models. This is supported with the  as_op  function decorator.  Theano needs to know the types of the inputs and outputs of a function, which are specified for  as_op  by  itypes  for inputs and  otypes  for outputs. The Theano documentation includes  an overview of the available types .  import theano.tensor as T \nfrom theano.compile.ops import as_op\n\n@as_op(itypes=[T.lscalar], otypes=[T.lscalar])\ndef crazy_modulo3(value):\n    if value   0: \n        return value % 3\n    else :\n        return (-value + 1) % 3\n\nwith Model() as model_deterministic:\n    a = Poisson('a', 1)\n    b = crazy_modulo3(a)  An important drawback of this approach is that it is not possible for  theano  to inspect these functions in order to compute the gradient required for the Hamiltonian-based samplers. Therefore, it is not possible to use the HMC or NUTS samplers for a model that uses such an operator. However, it is possible to add a gradient if we inherit from  theano.Op  instead of using  as_op . The PyMC example set includes  a more elaborate example of the usage of  as_op .", 
            "title": "Arbitrary deterministics"
        }, 
        {
            "location": "/getting_started/#arbitrary-distributions", 
            "text": "Similarly, the library of statistical distributions in PyMC3 is not exhaustive, but PyMC allows for the creation of user-defined functions for an arbitrary probability distribution. For simple statistical distributions, the  DensityDist  function takes as an argument any function that calculates a log-probability  $log(p(x))$ . This function may employ other random variables in its calculation. Here is an example inspired by a blog post by Jake Vanderplas on which priors to use for a linear regression (Vanderplas, 2014).   import theano.tensor as T\nfrom pymc3 import DensityDist\n\nwith Model() as model:\n    alpha = Uniform('intercept', -100, 100)\n\n    # Create custom densities\n    beta = DensityDist('beta', lambda value: -1.5 * T.log(1 + value**2), testval=0)\n    eps = DensityDist('eps', lambda value: -T.log(T.abs_(value)), testval=1)\n\n    # Create likelihood\n    like = Normal('y_est', mu=alpha + beta * X, sd=eps, observed=Y)  For more complex distributions, one can create a subclass of  Continuous  or  Discrete  and provide the custom  logp  function, as required. This is how the built-in distributions in PyMC are specified. As an example, fields like psychology and astrophysics have complex likelihood functions for a particular process that may require numerical approximation. In these cases, it is impossible to write the function in terms of predefined theano operators and we must use a custom theano operator using  as_op  or inheriting from  theano.Op .   Implementing the  beta  variable above as a  Continuous  subclass is shown below, along with a sub-function using the  as_op  decorator, though this is not strictly necessary.  from pymc3.distributions import Continuous\n\nclass Beta(Continuous):\n    def __init__(self, mu, *args, **kwargs):\n        super(Beta, self).__init__(*args, **kwargs)\n        self.mu = mu\n        self.mode = mu\n\n    def logp(self, value):\n        mu = self.mu\n        return beta_logp(value - mu)\n\n@as_op(itypes=[T.dscalar], otypes=[T.dscalar])\ndef beta_logp(value):\n    return -1.5 * np.log(1 + (value)**2)\n\n\nwith Model() as model:\n    beta = Beta('slope', mu=0, testval=0)", 
            "title": "Arbitrary distributions"
        }, 
        {
            "location": "/getting_started/#generalized-linear-models", 
            "text": "Generalized Linear Models (GLMs) are a class of flexible models that are widely used to estimate regression relationships between a single outcome variable and one or multiple predictors. Because these models are so common,  PyMC3  offers a  glm  submodule that allows flexible creation of various GLMs with an intuitive  R -like syntax that is implemented via the  patsy  module.  The  glm  submodule requires data to be included as a  pandas   DataFrame . Hence, for our linear regression example:  # Convert X and Y to a pandas DataFrame\nimport pandas \ndf = pandas.DataFrame({'x1': X1, 'x2': X2, 'y': Y})  The model can then be very concisely specified in one line of code.  from pymc3.glm import glm\n\nwith Model() as model_glm:\n    glm('y ~ x1 + x2', df)  The error distribution, if not specified via the  family  argument, is assumed to be normal. In the case of logistic regression, this can be modified by passing in a  Binomial  family object.  from pymc3.glm.families import Binomial\n\ndf_logistic = pandas.DataFrame({'x1': X1, 'x2': X2, 'y': Y   0})\n\nwith Model() as model_glm_logistic:\n    glm('y ~ x1 + x2', df_logistic, family=Binomial())", 
            "title": "Generalized Linear Models"
        }, 
        {
            "location": "/getting_started/#backends", 
            "text": "PyMC3  has support for different ways to store samples during and after sampling, called backends, including in-memory (default), text file, and SQLite. These can be found in  pymc.backends :  By default, an in-memory  ndarray  is used but if the samples would get too large to be held in memory we could use the  sqlite  backend:  from pymc3.backends import SQLite\n\nwith model_glm_logistic:\n    backend = SQLite('trace.sqlite')\n    trace = sample(5000, Metropolis(), trace=backend)   [-----------------100%-----------------] 5000 of 5000 complete in 3.8 sec  summary(trace, vars=['x1', 'x2'])  x1:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  -11.926          6.125            0.610            [-20.747, -1.430]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -20.409        -16.261        -14.169        -4.829         -1.018\n\n\nx2:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  59.498           30.716           3.059            [5.914, 102.835]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  5.031          22.983         70.320         80.770         102.425  The stored trace can then later be loaded using the  load  command:  from pymc3.backends.sqlite import load\n\nwith basic_model:\n    trace_loaded = load('trace.sqlite')\n\ntrace_loaded   MultiTrace: 1 chains, 5000 iterations, 4 variables   More information about  backends  can be found in the docstring of  pymc.backends .", 
            "title": "Backends"
        }, 
        {
            "location": "/getting_started/#references", 
            "text": "Patil, A., D. Huard and C.J. Fonnesbeck. (2010) PyMC: Bayesian Stochastic Modelling in Python. Journal of Statistical Software, 35(4), pp. 1-81  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. (2012) \u201cTheano: new features and speed improvements\u201d. NIPS 2012 deep learning workshop.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010) \u201cTheano: A CPU and GPU Math Expression Compiler\u201d. Proceedings of the Python for Scientific Computing Conference (SciPy) 2010. June 30 - July 3, Austin, TX  Lunn, D.J., Thomas, A., Best, N., and Spiegelhalter, D. (2000) WinBUGS -- a Bayesian modelling framework: concepts, structure, and extensibility. Statistics and Computing, 10:325--337.  Neal, R.M. Slice sampling. Annals of Statistics. (2003). doi:10.2307/3448413.  van Rossum, G. The Python Library Reference Release 2.6.5., (2010). URL http://docs.python.org/library/.  Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987) \u201cHybrid Monte Carlo\u201d, Physics Letters, vol. 195, pp. 216-222.  Stan Development Team. (2014). Stan: A C++ Library for Probability and Sampling, Version 2.5.0.   http://mc-stan.org.   Gamerman, D. Markov Chain Monte Carlo: statistical simulation for Bayesian inference. Chapman and Hall, 1997.  Hoffman, M. D.,   Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research, 30.  Vanderplas, Jake. \"Frequentism and Bayesianism IV: How to be a Bayesian in Python.\" Pythonic Perambulations. N.p., 14 Jun 2014. Web. 27 May. 2015.  https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ .", 
            "title": "References"
        }, 
        {
            "location": "/BEST/", 
            "text": "Bayesian Estimation Supersedes the T-Test\n\nThis model replicates the example used in:\nKruschke, John. (2012) Bayesian estimation supersedes the t test. Journal of Experimental Psychology: General.\n\nThe original pymc2 implementation was written by Andrew Straw and can be found here: https://github.com/strawlab/best\n\nPorted to PyMC3 by Thomas Wiecki (c) 2015.\n\n\n\nimport numpy as np\nimport pymc3 as pm\n\ndrug = (101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n        96,103,124,101,101,100,101,101,104,100,101)\nplacebo = (99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n           101,100,99,101,100,102,99,100,99)\n\ny1 = np.array(drug)\ny2 = np.array(placebo)\ny = np.concatenate((y1, y2))\n\nmu_m = np.mean( y )\nmu_p = 0.000001 * 1/np.std(y)**2\n\nsigma_low = np.std(y)/1000\nsigma_high = np.std(y)*1000\n\nwith pm.Model() as model:\n    group1_mean = pm.Normal('group1_mean', mu=mu_m, tau=mu_p, testval=y1.mean())\n    group2_mean = pm.Normal('group2_mean', mu=mu_m, tau=mu_p, testval=y2.mean())\n    group1_std = pm.Uniform('group1_std', lower=sigma_low, upper=sigma_high, testval=y1.std())\n    group2_std = pm.Uniform('group2_std', lower=sigma_low, upper=sigma_high, testval=y2.std())\n    nu = pm.Exponential('nu_minus_one', 1/29.) + 1\n\n    lam1 = group1_std**-2\n    lam2 = group2_std**-2\n\n    group1 = pm.T('drug', nu=nu, mu=group1_mean, lam=lam1, observed=y1)\n    group2 = pm.T('placebo', nu=nu, mu=group2_mean, lam=lam2, observed=y2)\n\n    diff_of_means = pm.Deterministic('difference of means', group1_mean - group2_mean)\n    diff_of_stds = pm.Deterministic('difference of stds', group1_std - group2_std)\n    effect_size = pm.Deterministic('effect size', diff_of_means / pm.sqrt((group1_std**2 + group2_std**2) / 2))\n\n    step = pm.NUTS()\n    trace = pm.sample(5000, step)\n\n\n\n\n [-----------------100%-----------------] 5000 of 5000 complete in 29.0 sec\n\n\n\n%matplotlib inline\n\n\n\n\npm.traceplot(trace[1000:]);\n\n\n\n\n\n\npm.plots.summary(trace[1000:])\n\n\n\n\ngroup1_mean:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  101.546          0.385            0.007            [100.745, 102.275]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  100.771        101.300        101.549        101.789        102.323\n\n\ngroup2_mean:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  100.526          0.212            0.005            [100.095, 100.912]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  100.101        100.383        100.532        100.667        100.922\n\n\ngroup1_std:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  2.060            0.422            0.010            [1.266, 2.882]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  1.349          1.756          2.027          2.315          2.998\n\n\ngroup2_std:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.053            0.213            0.004            [0.689, 1.481]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.705          0.905          1.029          1.184          1.524\n\n\nnu_minus_one:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.848            0.486            0.014            [0.033, 1.789]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.156          0.498          0.772          1.098          2.026\n\n\ndifference of means:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.020            0.441            0.008            [0.153, 1.838]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.168          0.731          1.010          1.320          1.861\n\n\ndifference of stds:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.007            0.435            0.008            [0.197, 1.899]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.230          0.699          0.989          1.287          1.946\n\n\neffect size:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.643            0.302            0.006            [0.080, 1.256]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.092          0.438          0.628          0.838          1.280", 
            "title": "BEST"
        }, 
        {
            "location": "/stochastic_volatility/", 
            "text": "Stochastic Volatility model\n\n\nimport numpy as np\nimport pymc3 as pm\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nfrom scipy.sparse import csc_matrix\nfrom scipy import optimize\n\n%pylab inline\n\n\n\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n:0: FutureWarning: IPython widgets are experimental and may change in the future.\n\n\n\nAsset prices have time-varying volatility (variance of day over day \nreturns\n). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, Hoffman (2011) p21.\n\n\n$$ \\sigma \\sim Exponential(50) $$\n\n\n$$ \\nu \\sim Exponential(.1) $$\n\n\n$$ s_i \\sim Normal(s_{i-1}, \\sigma^{-2}) $$\n\n\n$$ log(\\frac{y_i}{y_{i-1}}) \\sim t(\\nu, 0, exp(-2 s_i)) $$\n\n\nHere, \n$y$\n is the daily return series and \n$s$\n is the latent log volatility process.\n\n\nBuild Model\n\n\nFirst we load some daily returns of the S\nP 500.\n\n\nn = 400\nreturns = np.genfromtxt(\ndata/SP500.csv\n)[-n:]\nreturns[:5]\n\n\n\n\narray([-0.00637 , -0.004045, -0.02547 ,  0.005102, -0.047733])\n\n\n\nplt.plot(returns)\n\n\n\n\n[\nmatplotlib.lines.Line2D at 0xaeaec1cc\n]\n\n\n\n\n\nSpecifying the model in pymc3 mirrors its statistical specification. \n\n\nmodel = pm.Model()\nwith model:\n    sigma = pm.Exponential('sigma', 1./.02, testval=.1)\n\n    nu = pm.Exponential('nu', 1./10)\n    s = GaussianRandomWalk('s', sigma**-2, shape=n)\n\n    r = pm.T('r', nu, lam=pm.exp(-2*s), observed=returns)\n\n\n\n\nFit Model\n\n\nFor this model, the full maximum a posteriori (MAP) point is degenerate and has infinite density. However, if we fix \nlog_sigma\n and \nnu\n it is no longer degenerate, so we find the MAP with respect to the volatility process, 's', keeping \nlog_sigma\n and \nnu\n constant at their default values. \n\n\nWe use L-BFGS because it is more efficient for high dimensional functions (\ns\n has n elements).\n\n\nwith model:\n    start = pm.find_MAP(vars=[s], fmin=optimize.fmin_l_bfgs_b)\n\n\n\n\nWe do a short initial run to get near the right area, then start again using a new Hessian at the new starting point to get faster sampling due to better scaling. We do a short run since this is an interactive example.\n\n\nwith model:\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start, gamma=.25)\n    start2 = pm.sample(100, step, start=start)[-1]\n\n    # Start next run at the last sampled position.\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start2, gamma=.55)\n    trace = pm.sample(2000, step, start=start2)\n\n\n\n\n [-----------------100%-----------------] 2001 of 2000 complete in 368.9 sec\n\n/usr/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)\n\n\n\nfigsize(12,6)\npm.traceplot(trace, model.vars[:-1]);\n\n\n\n\n\n\nfigsize(12,6)\ntitle(str(s))\nplot(trace[s][::10].T,'b', alpha=.03);\nxlabel('time')\nylabel('log volatility')\n\n\n\n\nmatplotlib.text.Text at 0xac04168c\n\n\n\n\n\n\nLooking at the returns over time and overlaying the estimated standard deviation we can see how the model tracks the volatility over time.\n\n\nplot(returns)\nplot(np.exp(trace[s][::10].T), 'r', alpha=.03);\nsd = np.exp(trace[s].T)\nplot(-np.exp(trace[s][::10].T), 'r', alpha=.03);\nxlabel('time')\nylabel('returns')\n\n\n\n\nmatplotlib.text.Text at 0xaab3854c\n\n\n\n\n\n\nReferences\n\n\n\n\nHoffman \n Gelman. (2011). \nThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\n.", 
            "title": "Stochastic Volatility"
        }, 
        {
            "location": "/stochastic_volatility/#stochastic-volatility-model", 
            "text": "import numpy as np\nimport pymc3 as pm\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nfrom scipy.sparse import csc_matrix\nfrom scipy import optimize\n\n%pylab inline  Populating the interactive namespace from numpy and matplotlib\n\n\n:0: FutureWarning: IPython widgets are experimental and may change in the future.  Asset prices have time-varying volatility (variance of day over day  returns ). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, Hoffman (2011) p21.  $$ \\sigma \\sim Exponential(50) $$  $$ \\nu \\sim Exponential(.1) $$  $$ s_i \\sim Normal(s_{i-1}, \\sigma^{-2}) $$  $$ log(\\frac{y_i}{y_{i-1}}) \\sim t(\\nu, 0, exp(-2 s_i)) $$  Here,  $y$  is the daily return series and  $s$  is the latent log volatility process.", 
            "title": "Stochastic Volatility model"
        }, 
        {
            "location": "/stochastic_volatility/#build-model", 
            "text": "First we load some daily returns of the S P 500.  n = 400\nreturns = np.genfromtxt( data/SP500.csv )[-n:]\nreturns[:5]  array([-0.00637 , -0.004045, -0.02547 ,  0.005102, -0.047733])  plt.plot(returns)  [ matplotlib.lines.Line2D at 0xaeaec1cc ]   Specifying the model in pymc3 mirrors its statistical specification.   model = pm.Model()\nwith model:\n    sigma = pm.Exponential('sigma', 1./.02, testval=.1)\n\n    nu = pm.Exponential('nu', 1./10)\n    s = GaussianRandomWalk('s', sigma**-2, shape=n)\n\n    r = pm.T('r', nu, lam=pm.exp(-2*s), observed=returns)", 
            "title": "Build Model"
        }, 
        {
            "location": "/stochastic_volatility/#fit-model", 
            "text": "For this model, the full maximum a posteriori (MAP) point is degenerate and has infinite density. However, if we fix  log_sigma  and  nu  it is no longer degenerate, so we find the MAP with respect to the volatility process, 's', keeping  log_sigma  and  nu  constant at their default values.   We use L-BFGS because it is more efficient for high dimensional functions ( s  has n elements).  with model:\n    start = pm.find_MAP(vars=[s], fmin=optimize.fmin_l_bfgs_b)  We do a short initial run to get near the right area, then start again using a new Hessian at the new starting point to get faster sampling due to better scaling. We do a short run since this is an interactive example.  with model:\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start, gamma=.25)\n    start2 = pm.sample(100, step, start=start)[-1]\n\n    # Start next run at the last sampled position.\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start2, gamma=.55)\n    trace = pm.sample(2000, step, start=start2)   [-----------------100%-----------------] 2001 of 2000 complete in 368.9 sec\n\n/usr/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)  figsize(12,6)\npm.traceplot(trace, model.vars[:-1]);   figsize(12,6)\ntitle(str(s))\nplot(trace[s][::10].T,'b', alpha=.03);\nxlabel('time')\nylabel('log volatility')  matplotlib.text.Text at 0xac04168c    Looking at the returns over time and overlaying the estimated standard deviation we can see how the model tracks the volatility over time.  plot(returns)\nplot(np.exp(trace[s][::10].T), 'r', alpha=.03);\nsd = np.exp(trace[s].T)\nplot(-np.exp(trace[s][::10].T), 'r', alpha=.03);\nxlabel('time')\nylabel('returns')  matplotlib.text.Text at 0xaab3854c", 
            "title": "Fit Model"
        }, 
        {
            "location": "/stochastic_volatility/#references", 
            "text": "Hoffman   Gelman. (2011).  The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo .", 
            "title": "References"
        }, 
        {
            "location": "/hierarchical/", 
            "text": "%pylab --no-import-all inline\n\nfrom pymc3 import *\nimport theano.tensor as T\nfrom numpy import random, sum as nsum, ones, concatenate, newaxis, dot, arange\nimport numpy as np\n\nrandom.seed(1)\n\nn_groups = 1\nno_pergroup = 30\nn_observed = no_pergroup * n_groups\nn_group_predictors = 1\nn_predictors = 3\n\ngroup = concatenate([[i] * no_pergroup for i in range(n_groups)])\ngroup_predictors = random.normal(size=(n_groups, n_group_predictors))  # random.normal(size = (n_groups, n_group_predictors))\npredictors = random.normal(size=(n_observed, n_predictors))\n\ngroup_effects_a = random.normal(size=(n_group_predictors, n_predictors))\neffects_a = random.normal(\n    size=(n_groups, n_predictors)) + dot(group_predictors, group_effects_a)\n\ny = nsum(\n    effects_a[group, :] * predictors, 1) + random.normal(size=(n_observed))\n\n\nmodel = Model()\nwith model:\n\n    # m_g ~ N(0, .1)\n    group_effects = Normal(\n        \ngroup_effects\n, 0, .1, shape=(1, n_group_predictors, n_predictors))\n\n    # sg ~ Uniform(.05, 10)\n    sg = Uniform(\nsg\n, .0, 10, testval=2.)\n\n\n    # m ~ N(mg * pg, sg)\n    effects = Normal(\neffects\n,\n                     sum(group_predictors[:, :, newaxis] *\n                     group_effects, 1), sg ** -2,\n                     shape=(n_groups, n_predictors))\n\n    s = Uniform(\ns\n, .01, 10, shape=n_groups)\n\n    g = T.constant(group)\n\n    # y ~ Normal(m[g] * p, s)\n    yd = Normal('y', sum(effects[g] * predictors, 1), s[g] ** -2, observed=y)\n\n    #start = find_MAP()\n    #h = find_hessian(start)\n\n\n    #step = Metropolis()\n    #step = Slice()\n    step = NUTS()\n\n\n\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n/usr/local/Cellar/python3/3.4.2_1/Frameworks/Python.framework/Versions/3.4/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)\n\n\n\nn_predictors\n\n\n\n\n3\n\n\n\ngroup\n\n\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0])\n\n\n\nn_groups\n\n\n\n\n1\n\n\n\nimport pymc3 as pm\npm.debug.eval_univariate?\n\n\n\n\nmodel.free_RVs\n\n\n\n\n[group_effects, sg_interval, effects, s_interval]\n\n\n\nlp = lambda x, y: model.logp({'group_effects': [[[0, 0, 0]]], 'sg': x, 'effects':[[y, y, y]], 's': [.05]})\nlp = np.vectorize(lp)\n\n\n\n\nimport seaborn as sns\n\n\n\n\nx, y = np.mgrid[0.001:0.1:.001, -.05:.05:.001]\nplt.contourf(x, y, lp(x, y))\n\n\n\n\n---------------------------------------------------------------------------\n\nTypeError                                 Traceback (most recent call last)\n\n\nipython-input-9-12de12a989b6\n in \nmodule\n()\n      1 x, y = np.mgrid[0.001:0.1:.001, -.05:.05:.001]\n----\n 2 plt.contourf(x, y, lp(x, y))\n\n\n/usr/local/lib/python3.4/site-packages/numpy/lib/function_base.py in __call__(self, *args, **kwargs)\n   1698             vargs.extend([kwargs[_n] for _n in names])\n   1699 \n-\n 1700         return self._vectorize_call(func=func, args=vargs)\n   1701 \n   1702     def _get_ufunc_and_otypes(self, func, args):\n\n\n/usr/local/lib/python3.4/site-packages/numpy/lib/function_base.py in _vectorize_call(self, func, args)\n   1761             _res = func()\n   1762         else:\n-\n 1763             ufunc, otypes = self._get_ufunc_and_otypes(func=func, args=args)\n   1764 \n   1765             # Convert args to object arrays first\n\n\n/usr/local/lib/python3.4/site-packages/numpy/lib/function_base.py in _get_ufunc_and_otypes(self, func, args)\n   1723             # arrays (the input values are not checked to ensure this)\n   1724             inputs = [asarray(_a).flat[0] for _a in args]\n-\n 1725             outputs = func(*inputs)\n   1726 \n   1727             # Performance note: profiling indicates that -- for simple\n\n\n\nipython-input-7-d3b3653500d1\n in \nlambda\n(x, y)\n----\n 1 lp = lambda x, y: model.logp({'group_effects': [[[0, 0, 0]]], 'sg': x, 'effects':[[y, y, y]], 's': [.05]})\n      2 lp = np.vectorize(lp)\n\n\n/Users/fonnescj/GitHub/pymc3/pymc3/model.py in __call__(self, *args, **kwargs)\n    323     def __call__(self, *args, **kwargs):\n    324         point = Point(model=self.model, *args, **kwargs)\n--\n 325         return self.f(**point)\n    326 \n    327 compilef = fastfn\n\n\n/usr/local/lib/python3.4/site-packages/theano/compile/function_module.py in __call__(self, *args, **kwargs)\n    579                     raise TypeError(\"Missing required input: %s\" %\n    580                                     getattr(self.inv_finder[c], 'variable',\n--\n 581                                             self.inv_finder[c]))\n    582                 if c.provided \n 1:\n    583                     raise TypeError(\"Multiple values for input: %s\" %\n\n\nTypeError: Missing required input: sg_interval\n\n\n\nbest of 3: 6.94 s per loop\n\n\ntr = sample(3e3, step, model=model)\n\n\n\n\n [-----------------100%-----------------] 3000 of 3000 complete in 33.2 sec\n\n\n\ntr.varnames\n\n\n\n\n['group_effects', 'sg_interval', 'effects', 's_interval', 'sg', 's']\n\n\n\ntraceplot(tr, vars=['s_interval', 's'])\n\n\n\n\narray([[\nmatplotlib.axes._subplots.AxesSubplot object at 0x1157a6b00\n,\n        \nmatplotlib.axes._subplots.AxesSubplot object at 0x115814240\n],\n       [\nmatplotlib.axes._subplots.AxesSubplot object at 0x1158556a0\n,\n        \nmatplotlib.axes._subplots.AxesSubplot object at 0x1158a1940\n]], dtype=object)\n\n\n\n\n\n%prun -q -D sample_nuts.prof sample(3e3, step, model=model)\n\n\n\n\n [-----------------100%-----------------] 3000 of 3000 complete in 74.1 sec \n*** Profile stats marshalled to file u'sample_nuts.prof'.\n\n\n\n%prun -q -D sample.prof sample(3e3, step, start, model=model)\n\n\n\n\n [-----------------100%-----------------] 3000 of 3000 complete in 80.0 sec \n*** Profile stats marshalled to file u'sample.prof'.\n\n\n\ntraceplot(trace);", 
            "title": "Hierarchical model"
        }, 
        {
            "location": "/GLM-linear/", 
            "text": "The Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nAuthor: Thomas Wiecki\n\n\nThis tutorial appeared as a post in a small series on Bayesian GLMs on my blog:\n\n\n\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nThis world is far from Normal(ly distributed): Robust Regression in PyMC3\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\nIn this blog post I will talk about:\n\n\n\n\nHow the Bayesian Revolution in many scientific disciplines is hindered by poor usability of current Probabilistic Programming languages.\n\n\nA gentle introduction to Bayesian linear regression and how it differs from the frequentist approach.\n\n\nA preview of \nPyMC3\n (currently in alpha) and its new GLM submodule I wrote to allow creation and estimation of Bayesian GLMs as easy as frequentist GLMs in R.\n\n\n\n\nReady? Lets get started!\n\n\nThere is a huge paradigm shift underway in many scientific disciplines: The Bayesian Revolution. \n\n\nWhile the theoretical benefits of Bayesian over Frequentist stats have been discussed at length elsewhere (see \nFurther Reading\n below), there is a major obstacle that hinders wider adoption -- \nusability\n (this is one of the reasons DARPA wrote out a huge grant to \nimprove Probabilistic Programming\n). \n\n\nThis is mildly ironic because the beauty of Bayesian statistics is their generality. Frequentist stats have a bazillion different tests for every different scenario. In Bayesian land you define your model exactly as you think is appropriate and hit the \nInference Button(TM)\n (i.e. running the magical MCMC sampling algorithm).\n\n\nYet when I ask my colleagues why they use frequentist stats (even though they would like to use Bayesian stats) the answer is that software packages like SPSS or R make it very easy to run all those individuals tests with a single command (and more often then not, they don't know the exact model and inference method being used).\n\n\nWhile there are great Bayesian software packages like \nJAGS\n, \nBUGS\n, \nStan\n and \nPyMC\n, they are written for Bayesians statisticians who know very well what model they want to build. \n\n\nUnfortunately, \n\"the vast majority of statistical analysis is not performed by statisticians\"\n -- so what we really need are tools for \nscientists\n and not for statisticians.\n\n\nIn the interest of putting my code where my mouth is I wrote a submodule for the upcoming \nPyMC3\n that makes construction of Bayesian Generalized Linear Models (GLMs) as easy as Frequentist ones in R.\n\n\nLinear Regression\n\n\nWhile future blog posts will explore more complex models, I will start here with the simplest GLM -- linear regression.\nIn general, frequentists think about Linear Regression as follows:\n\n\n$$ Y = X\\beta + \\epsilon $$\n\n\nwhere \n$Y$\n is the output we want to predict (or \ndependent\n variable), \n$X$\n is our predictor (or \nindependent\n variable), and \n$\\beta$\n are the coefficients (or parameters) of the model we want to estimate. \n$\\epsilon$\n is an error term which is assumed to be normally distributed. \n\n\nWe can then use Ordinary Least Squares or Maximum Likelihood to find the best fitting \n$\\beta$\n.\n\n\nProbabilistic Reformulation\n\n\nBayesians take a probabilistic view of the world and express this model in terms of probability distributions. Our above linear regression can be rewritten to yield:\n\n\n$$ Y \\sim \\mathcal{N}(X \\beta, \\sigma^2) $$\n\n\nIn words, we view \n$Y$\n as a random variable (or random vector) of which each element (data point) is distributed according to a Normal distribution. The mean of this normal distribution is provided by our linear predictor with variance \n$\\sigma^2$\n.\n\n\nWhile this is essentially the same model, there are two critical advantages of Bayesian estimation:\n\n\n\n\nPriors: We can quantify any prior knowledge we might have by placing priors on the paramters. For example, if we think that \n$\\sigma$\n is likely to be small we would choose a prior with more probability mass on low values.\n\n\nQuantifying uncertainty: We do not get a single estimate of \n$\\beta$\n as above but instead a complete posterior distribution about how likely different values of \n$\\beta$\n are. For example, with few data points our uncertainty in \n$\\beta$\n will be very high and we'd be getting very wide posteriors.\n\n\n\n\nBayesian GLMs in PyMC3\n\n\nWith the new GLM module in PyMC3 it is very easy to build this and much more complex models.\n\n\nFirst, lets import the required modules.\n\n\n%matplotlib inline\n\nfrom pymc3 import  *\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nGenerating data\n\n\nCreate some toy data to play around with and scatter-plot it. \n\n\nEssentially we are creating a regression line defined by intercept and slope and add data points by sampling from a Normal with the mean set to the regression line.\n\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\ndata = dict(x=x, y=y)\n\n\n\n\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x, y, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);\n\n\n\n\n\n\nEstimating the model\n\n\nLets fit a Bayesian linear regression model to this data. As you can see, model specifications in \nPyMC3\n are wrapped in a \nwith\n statement. \n\n\nHere we use the awesome new \nNUTS sampler\n (our Inference Button) to draw 2000 posterior samples.\n\n\nwith Model() as model: # model specifications in PyMC3 are wrapped in a with-statement\n    # Define priors\n    sigma = HalfCauchy('sigma', beta=10, testval=1.)\n    intercept = Normal('Intercept', 0, sd=20)\n    x_coeff = Normal('x', 0, sd=20)\n\n    # Define likelihood\n    likelihood = Normal('y', mu=intercept + x_coeff * x, \n                        sd=sigma, observed=y)\n\n    # Inference!\n    start = find_MAP() # Find starting value by optimization\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, start=start, progressbar=False) # draw 2000 posterior samples using NUTS sampling\n\n\n\n\nThis should be fairly readable for people who know probabilistic programming. However, would my non-statistican friend know what all this does? Moreover, recall that this is an extremely simple model that would be one line in R. Having multiple, potentially transformed regressors, interaction terms or link-functions would also make this much more complex and error prone. \n\n\nThe new \nglm()\n function instead takes a \nPatsy\n linear model specifier from which it creates a design matrix. \nglm()\n then adds random variables for each of the coefficients and an appopriate likelihood to the model. \n\n\nwith Model() as model:\n    # specify glm and pass in data. The resulting linear model, its likelihood and \n    # and all its parameters are automatically added to our model.\n    glm.glm('y ~ x', data)\n    start = find_MAP()\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, progressbar=False) # draw 2000 posterior samples using NUTS sampling\n\n\n\n\nMuch shorter, but this code does the exact same thing as the above model specification (you can change priors and everything else too if we wanted). \nglm()\n parses the \nPatsy\n model string, adds random variables for each regressor (\nIntercept\n and slope \nx\n in this case), adds a likelihood (by default, a Normal is chosen), and all other variables (\nsigma\n). Finally, \nglm()\n then initializes the parameters to a good starting point by estimating a frequentist linear model using \nstatsmodels\n.\n\n\nIf you are not familiar with R's syntax, \n'y ~ x'\n specifies that we have an output variable \ny\n that we want to estimate as a linear function of \nx\n.\n\n\nAnalyzing the model\n\n\nBayesian inference does not give us only one best fitting line (as maximum likelihood does) but rather a whole posterior distribution of likely parameters. Lets plot the posterior distribution of our parameters and the individual samples we drew.\n\n\nplt.figure(figsize=(7, 7))\ntraceplot(trace[100:])\nplt.tight_layout();\n\n\n\n\nmatplotlib.figure.Figure at 0x7f574ca72b10\n\n\n\n\n\n\nThe left side shows our marginal posterior -- for each parameter value on the x-axis we get a probability on the y-axis that tells us how likely that parameter value is.\n\n\nThere are a couple of things to see here. The first is that our sampling chains for the individual parameters (left side) seem well converged and stationary (there are no large drifts or other odd patterns).\n\n\nSecondly, the maximum posterior estimate of each variable (the peak in the left side distributions) is very close to the true parameters used to generate the data (\nx\n is the regression coefficient and \nsigma\n is the standard deviation of our normal).\n\n\nIn the GLM we thus do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. Here we are using the \nglm.plot_posterior_predictive()\n convenience function for this.\n\n\nplt.figure(figsize=(7, 7))\nplt.plot(x, y, 'x', label='data')\nglm.plot_posterior_predictive(trace, samples=100, \n                              label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, label='true regression line', lw=3., c='y')\n\nplt.title('Posterior predictive regression lines')\nplt.legend(loc=0)\nplt.xlabel('x')\nplt.ylabel('y');\n\n\n\n\n\n\nAs you can see, our estimated regression lines are very similar to the true regression line. But since we only have limited data we have \nuncertainty\n in our estimates, here expressed by the variability of the lines.\n\n\nSummary\n\n\n\n\nUsability is currently a huge hurdle for wider adoption of Bayesian statistics.\n\n\nPyMC3\n allows GLM specification with convenient syntax borrowed from R.\n\n\nPosterior predictive plots allow us to evaluate fit and our uncertainty in it.\n\n\n\n\nFurther reading\n\n\nThis is the first post of a small series on Bayesian GLMs I am preparing. Next week I will describe how the Student T distribution can be used to perform robust linear regression.\n\n\nThen there are also other good resources on Bayesian statistics:\n\n\n\n\nThe excellent book \nDoing Bayesian Data Analysis by John Kruschke\n.\n\n\nAndrew Gelman's blog\n\n\nBaeu Cronins blog post on Probabilistic Programming", 
            "title": "Linear Regression"
        }, 
        {
            "location": "/GLM-linear/#the-inference-button-bayesian-glms-made-easy-with-pymc3", 
            "text": "Author: Thomas Wiecki  This tutorial appeared as a post in a small series on Bayesian GLMs on my blog:   The Inference Button: Bayesian GLMs made easy with PyMC3  This world is far from Normal(ly distributed): Robust Regression in PyMC3  The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3   In this blog post I will talk about:   How the Bayesian Revolution in many scientific disciplines is hindered by poor usability of current Probabilistic Programming languages.  A gentle introduction to Bayesian linear regression and how it differs from the frequentist approach.  A preview of  PyMC3  (currently in alpha) and its new GLM submodule I wrote to allow creation and estimation of Bayesian GLMs as easy as frequentist GLMs in R.   Ready? Lets get started!  There is a huge paradigm shift underway in many scientific disciplines: The Bayesian Revolution.   While the theoretical benefits of Bayesian over Frequentist stats have been discussed at length elsewhere (see  Further Reading  below), there is a major obstacle that hinders wider adoption --  usability  (this is one of the reasons DARPA wrote out a huge grant to  improve Probabilistic Programming ).   This is mildly ironic because the beauty of Bayesian statistics is their generality. Frequentist stats have a bazillion different tests for every different scenario. In Bayesian land you define your model exactly as you think is appropriate and hit the  Inference Button(TM)  (i.e. running the magical MCMC sampling algorithm).  Yet when I ask my colleagues why they use frequentist stats (even though they would like to use Bayesian stats) the answer is that software packages like SPSS or R make it very easy to run all those individuals tests with a single command (and more often then not, they don't know the exact model and inference method being used).  While there are great Bayesian software packages like  JAGS ,  BUGS ,  Stan  and  PyMC , they are written for Bayesians statisticians who know very well what model they want to build.   Unfortunately,  \"the vast majority of statistical analysis is not performed by statisticians\"  -- so what we really need are tools for  scientists  and not for statisticians.  In the interest of putting my code where my mouth is I wrote a submodule for the upcoming  PyMC3  that makes construction of Bayesian Generalized Linear Models (GLMs) as easy as Frequentist ones in R.", 
            "title": "The Inference Button: Bayesian GLMs made easy with PyMC3"
        }, 
        {
            "location": "/GLM-linear/#linear-regression", 
            "text": "While future blog posts will explore more complex models, I will start here with the simplest GLM -- linear regression.\nIn general, frequentists think about Linear Regression as follows:  $$ Y = X\\beta + \\epsilon $$  where  $Y$  is the output we want to predict (or  dependent  variable),  $X$  is our predictor (or  independent  variable), and  $\\beta$  are the coefficients (or parameters) of the model we want to estimate.  $\\epsilon$  is an error term which is assumed to be normally distributed.   We can then use Ordinary Least Squares or Maximum Likelihood to find the best fitting  $\\beta$ .", 
            "title": "Linear Regression"
        }, 
        {
            "location": "/GLM-linear/#probabilistic-reformulation", 
            "text": "Bayesians take a probabilistic view of the world and express this model in terms of probability distributions. Our above linear regression can be rewritten to yield:  $$ Y \\sim \\mathcal{N}(X \\beta, \\sigma^2) $$  In words, we view  $Y$  as a random variable (or random vector) of which each element (data point) is distributed according to a Normal distribution. The mean of this normal distribution is provided by our linear predictor with variance  $\\sigma^2$ .  While this is essentially the same model, there are two critical advantages of Bayesian estimation:   Priors: We can quantify any prior knowledge we might have by placing priors on the paramters. For example, if we think that  $\\sigma$  is likely to be small we would choose a prior with more probability mass on low values.  Quantifying uncertainty: We do not get a single estimate of  $\\beta$  as above but instead a complete posterior distribution about how likely different values of  $\\beta$  are. For example, with few data points our uncertainty in  $\\beta$  will be very high and we'd be getting very wide posteriors.", 
            "title": "Probabilistic Reformulation"
        }, 
        {
            "location": "/GLM-linear/#bayesian-glms-in-pymc3", 
            "text": "With the new GLM module in PyMC3 it is very easy to build this and much more complex models.  First, lets import the required modules.  %matplotlib inline\n\nfrom pymc3 import  *\n\nimport numpy as np\nimport matplotlib.pyplot as plt  Generating data  Create some toy data to play around with and scatter-plot it.   Essentially we are creating a regression line defined by intercept and slope and add data points by sampling from a Normal with the mean set to the regression line.  size = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\ndata = dict(x=x, y=y)  fig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x, y, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);   Estimating the model  Lets fit a Bayesian linear regression model to this data. As you can see, model specifications in  PyMC3  are wrapped in a  with  statement.   Here we use the awesome new  NUTS sampler  (our Inference Button) to draw 2000 posterior samples.  with Model() as model: # model specifications in PyMC3 are wrapped in a with-statement\n    # Define priors\n    sigma = HalfCauchy('sigma', beta=10, testval=1.)\n    intercept = Normal('Intercept', 0, sd=20)\n    x_coeff = Normal('x', 0, sd=20)\n\n    # Define likelihood\n    likelihood = Normal('y', mu=intercept + x_coeff * x, \n                        sd=sigma, observed=y)\n\n    # Inference!\n    start = find_MAP() # Find starting value by optimization\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, start=start, progressbar=False) # draw 2000 posterior samples using NUTS sampling  This should be fairly readable for people who know probabilistic programming. However, would my non-statistican friend know what all this does? Moreover, recall that this is an extremely simple model that would be one line in R. Having multiple, potentially transformed regressors, interaction terms or link-functions would also make this much more complex and error prone.   The new  glm()  function instead takes a  Patsy  linear model specifier from which it creates a design matrix.  glm()  then adds random variables for each of the coefficients and an appopriate likelihood to the model.   with Model() as model:\n    # specify glm and pass in data. The resulting linear model, its likelihood and \n    # and all its parameters are automatically added to our model.\n    glm.glm('y ~ x', data)\n    start = find_MAP()\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, progressbar=False) # draw 2000 posterior samples using NUTS sampling  Much shorter, but this code does the exact same thing as the above model specification (you can change priors and everything else too if we wanted).  glm()  parses the  Patsy  model string, adds random variables for each regressor ( Intercept  and slope  x  in this case), adds a likelihood (by default, a Normal is chosen), and all other variables ( sigma ). Finally,  glm()  then initializes the parameters to a good starting point by estimating a frequentist linear model using  statsmodels .  If you are not familiar with R's syntax,  'y ~ x'  specifies that we have an output variable  y  that we want to estimate as a linear function of  x .  Analyzing the model  Bayesian inference does not give us only one best fitting line (as maximum likelihood does) but rather a whole posterior distribution of likely parameters. Lets plot the posterior distribution of our parameters and the individual samples we drew.  plt.figure(figsize=(7, 7))\ntraceplot(trace[100:])\nplt.tight_layout();  matplotlib.figure.Figure at 0x7f574ca72b10    The left side shows our marginal posterior -- for each parameter value on the x-axis we get a probability on the y-axis that tells us how likely that parameter value is.  There are a couple of things to see here. The first is that our sampling chains for the individual parameters (left side) seem well converged and stationary (there are no large drifts or other odd patterns).  Secondly, the maximum posterior estimate of each variable (the peak in the left side distributions) is very close to the true parameters used to generate the data ( x  is the regression coefficient and  sigma  is the standard deviation of our normal).  In the GLM we thus do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. Here we are using the  glm.plot_posterior_predictive()  convenience function for this.  plt.figure(figsize=(7, 7))\nplt.plot(x, y, 'x', label='data')\nglm.plot_posterior_predictive(trace, samples=100, \n                              label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, label='true regression line', lw=3., c='y')\n\nplt.title('Posterior predictive regression lines')\nplt.legend(loc=0)\nplt.xlabel('x')\nplt.ylabel('y');   As you can see, our estimated regression lines are very similar to the true regression line. But since we only have limited data we have  uncertainty  in our estimates, here expressed by the variability of the lines.", 
            "title": "Bayesian GLMs in PyMC3"
        }, 
        {
            "location": "/GLM-linear/#summary", 
            "text": "Usability is currently a huge hurdle for wider adoption of Bayesian statistics.  PyMC3  allows GLM specification with convenient syntax borrowed from R.  Posterior predictive plots allow us to evaluate fit and our uncertainty in it.   Further reading  This is the first post of a small series on Bayesian GLMs I am preparing. Next week I will describe how the Student T distribution can be used to perform robust linear regression.  Then there are also other good resources on Bayesian statistics:   The excellent book  Doing Bayesian Data Analysis by John Kruschke .  Andrew Gelman's blog  Baeu Cronins blog post on Probabilistic Programming", 
            "title": "Summary"
        }, 
        {
            "location": "/GLM-robust/", 
            "text": "This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n\n\nAuthor: Thomas Wiecki\n\n\nThis tutorial first appeard as a post in small series on Bayesian GLMs on my blog:\n\n\n\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nThis world is far from Normal(ly distributed): Robust Regression in PyMC3\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\nIn this blog post I will write about:\n\n\n\n\nHow a few outliers can largely affect the fit of linear regression models.\n\n\nHow replacing the normal likelihood with Student T distribution produces robust regression.\n\n\nHow this can easily be done with \nPyMC3\n and its new \nglm\n module by passing a \nfamily\n object.\n\n\n\n\nThis is the second part of a series on Bayesian GLMs (click \nhere for part I about linear regression\n). In this prior post I described how minimizing the squared distance of the regression line is the same as maximizing the likelihood of a Normal distribution with the mean coming from the regression line. This latter probabilistic expression allows us to easily formulate a Bayesian linear regression model.\n\n\nThis worked splendidly on simulated data. The problem with simulated data though is that it's, well, simulated. In the real world things tend to get more messy and assumptions like normality are easily violated by a few outliers. \n\n\nLets see what happens if we add some outliers to our simulated data from the last post.\n\n\nAgain, import our modules.\n\n\n%matplotlib inline\n\nimport pymc3 as pm\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport theano\n\n\n\n\nCreate some toy data but also add some outliers.\n\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\n# Add outliers\nx_out = np.append(x, [.1, .15, .2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = dict(x=x_out, y=y_out)\n\n\n\n\nPlot the data together with the true regression line (the three points in the upper left corner are the outliers we added).\n\n\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x_out, y_out, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);\n\n\n\n\n\n\nRobust Regression\n\n\nLets see what happens if we estimate our Bayesian linear regression model using the \nglm()\n function as before. This function takes a \nPatsy\n string to describe the linear model and adds a Normal likelihood by default. \n\n\nwith pm.Model() as model:\n    pm.glm.glm('y ~ x', data)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(2000, step, progressbar=False)\n\n\n\n\n/home/wiecki/envs/pymc3/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *\n\n\n\nTo evaluate the fit, I am plotting the posterior predictive regression lines by taking regression parameters from the posterior distribution and plotting a regression line for each (this is all done inside of \nplot_posterior_predictive()\n).\n\n\nplt.subplot(111, xlabel='x', ylabel='y', \n            title='Posterior predictive regression lines')\nplt.plot(x_out, y_out, 'x', label='data')\npm.glm.plot_posterior_predictive(trace, samples=100, \n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\n\nplt.legend(loc=0);\n\n\n\n\n\n\nAs you can see, the fit is quite skewed and we have a fair amount of uncertainty in our estimate as indicated by the wide range of different posterior predictive regression lines. Why is this? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.\n\n\nA Frequentist would estimate a \nRobust Regression\n and use a non-quadratic distance measure to evaluate the fit.\n\n\nBut what's a Bayesian to do? Since the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the \nStudent T distribution\n which has heavier tails as shown next (I read about this trick in \n\"The Kruschke\"\n, aka the puppy-book; but I think \nGelman\n was the first to formulate this).\n\n\nLets look at those two distributions to get a feel for them.\n\n\nnormal_dist = pm.Normal.dist(mu=0, sd=1)\nt_dist = pm.T.dist(mu=0, lam=1, nu=1)\nx_eval = np.linspace(-8, 8, 300)\nplt.plot(x_eval, theano.tensor.exp(normal_dist.logp(x_eval)).eval(), label='Normal', lw=2.)\nplt.plot(x_eval, theano.tensor.exp(t_dist.logp(x_eval)).eval(), label='Student T', lw=2.)\nplt.xlabel('x')\nplt.ylabel('Probability density')\nplt.legend();\n\n\n\n\n\n\nAs you can see, the probability of values far away from the mean (0 in this case) are much more likely under the \nT\n distribution than under the Normal distribution.\n\n\nTo define the usage of a T distribution in \nPyMC3\n we can pass a family object -- \nT\n -- that specifies that our data is Student T-distributed (see \nglm.families\n for more choices). Note that this is the same syntax as \nR\n and \nstatsmodels\n use.\n\n\nwith pm.Model() as model_robust:\n    family = pm.glm.families.T()\n    pm.glm.glm('y ~ x', data, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace_robust = pm.sample(2000, step, progressbar=False)\n\nplt.figure(figsize=(5, 5))\nplt.plot(x_out, y_out, 'x')\npm.glm.plot_posterior_predictive(trace_robust,\n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\nplt.legend();\n\n\n\n\n\n\nThere, much better! The outliers are barely influencing our estimation at all because our likelihood function assumes that outliers are much more probable than under the Normal distribution.\n\n\nSummary\n\n\n\n\nPyMC3\n's \nglm()\n function allows you to pass in a \nfamily\n object that contains information about the likelihood.\n\n\nBy changing the likelihood from a Normal distribution to a Student T distribution -- which has more mass in the tails -- we can perform \nRobust Regression\n.\n\n\n\n\nThe next post will be about logistic regression in PyMC3 and what the posterior and oatmeal have in common.\n\n\nExtensions\n: \n\n\n\n\nThe Student-T distribution has, besides the mean and variance, a third parameter called \ndegrees of freedom\n that describes how much mass should be put into the tails. Here it is set to 1 which gives maximum mass to the tails (setting this to infinity results in a Normal distribution!). One could easily place a prior on this rather than fixing it which I leave as an exercise for the reader ;).\n\n\nT distributions can be used as priors as well. I will show this in a future post on hierarchical GLMs.\n\n\nHow do we test if our data is normal or violates that assumption in an important way? Check out this \ngreat blog post\n by Allen Downey.", 
            "title": "Robust Regression"
        }, 
        {
            "location": "/GLM-robust/#this-world-is-far-from-normally-distributed-bayesian-robust-regression-in-pymc3", 
            "text": "Author: Thomas Wiecki  This tutorial first appeard as a post in small series on Bayesian GLMs on my blog:   The Inference Button: Bayesian GLMs made easy with PyMC3  This world is far from Normal(ly distributed): Robust Regression in PyMC3  The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3   In this blog post I will write about:   How a few outliers can largely affect the fit of linear regression models.  How replacing the normal likelihood with Student T distribution produces robust regression.  How this can easily be done with  PyMC3  and its new  glm  module by passing a  family  object.   This is the second part of a series on Bayesian GLMs (click  here for part I about linear regression ). In this prior post I described how minimizing the squared distance of the regression line is the same as maximizing the likelihood of a Normal distribution with the mean coming from the regression line. This latter probabilistic expression allows us to easily formulate a Bayesian linear regression model.  This worked splendidly on simulated data. The problem with simulated data though is that it's, well, simulated. In the real world things tend to get more messy and assumptions like normality are easily violated by a few outliers.   Lets see what happens if we add some outliers to our simulated data from the last post.  Again, import our modules.  %matplotlib inline\n\nimport pymc3 as pm\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport theano  Create some toy data but also add some outliers.  size = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\n# Add outliers\nx_out = np.append(x, [.1, .15, .2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = dict(x=x_out, y=y_out)  Plot the data together with the true regression line (the three points in the upper left corner are the outliers we added).  fig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x_out, y_out, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);", 
            "title": "This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3"
        }, 
        {
            "location": "/GLM-robust/#robust-regression", 
            "text": "Lets see what happens if we estimate our Bayesian linear regression model using the  glm()  function as before. This function takes a  Patsy  string to describe the linear model and adds a Normal likelihood by default.   with pm.Model() as model:\n    pm.glm.glm('y ~ x', data)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(2000, step, progressbar=False)  /home/wiecki/envs/pymc3/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *  To evaluate the fit, I am plotting the posterior predictive regression lines by taking regression parameters from the posterior distribution and plotting a regression line for each (this is all done inside of  plot_posterior_predictive() ).  plt.subplot(111, xlabel='x', ylabel='y', \n            title='Posterior predictive regression lines')\nplt.plot(x_out, y_out, 'x', label='data')\npm.glm.plot_posterior_predictive(trace, samples=100, \n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\n\nplt.legend(loc=0);   As you can see, the fit is quite skewed and we have a fair amount of uncertainty in our estimate as indicated by the wide range of different posterior predictive regression lines. Why is this? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.  A Frequentist would estimate a  Robust Regression  and use a non-quadratic distance measure to evaluate the fit.  But what's a Bayesian to do? Since the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the  Student T distribution  which has heavier tails as shown next (I read about this trick in  \"The Kruschke\" , aka the puppy-book; but I think  Gelman  was the first to formulate this).  Lets look at those two distributions to get a feel for them.  normal_dist = pm.Normal.dist(mu=0, sd=1)\nt_dist = pm.T.dist(mu=0, lam=1, nu=1)\nx_eval = np.linspace(-8, 8, 300)\nplt.plot(x_eval, theano.tensor.exp(normal_dist.logp(x_eval)).eval(), label='Normal', lw=2.)\nplt.plot(x_eval, theano.tensor.exp(t_dist.logp(x_eval)).eval(), label='Student T', lw=2.)\nplt.xlabel('x')\nplt.ylabel('Probability density')\nplt.legend();   As you can see, the probability of values far away from the mean (0 in this case) are much more likely under the  T  distribution than under the Normal distribution.  To define the usage of a T distribution in  PyMC3  we can pass a family object --  T  -- that specifies that our data is Student T-distributed (see  glm.families  for more choices). Note that this is the same syntax as  R  and  statsmodels  use.  with pm.Model() as model_robust:\n    family = pm.glm.families.T()\n    pm.glm.glm('y ~ x', data, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace_robust = pm.sample(2000, step, progressbar=False)\n\nplt.figure(figsize=(5, 5))\nplt.plot(x_out, y_out, 'x')\npm.glm.plot_posterior_predictive(trace_robust,\n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\nplt.legend();   There, much better! The outliers are barely influencing our estimation at all because our likelihood function assumes that outliers are much more probable than under the Normal distribution.", 
            "title": "Robust Regression"
        }, 
        {
            "location": "/GLM-robust/#summary", 
            "text": "PyMC3 's  glm()  function allows you to pass in a  family  object that contains information about the likelihood.  By changing the likelihood from a Normal distribution to a Student T distribution -- which has more mass in the tails -- we can perform  Robust Regression .   The next post will be about logistic regression in PyMC3 and what the posterior and oatmeal have in common.  Extensions :    The Student-T distribution has, besides the mean and variance, a third parameter called  degrees of freedom  that describes how much mass should be put into the tails. Here it is set to 1 which gives maximum mass to the tails (setting this to infinity results in a Normal distribution!). One could easily place a prior on this rather than fixing it which I leave as an exercise for the reader ;).  T distributions can be used as priors as well. I will show this in a future post on hierarchical GLMs.  How do we test if our data is normal or violates that assumption in an important way? Check out this  great blog post  by Allen Downey.", 
            "title": "Summary"
        }, 
        {
            "location": "/GLM-hierarchical/", 
            "text": "The best of both worlds: Hierarchical Linear Regression in PyMC3\n\n\nAuthors: Danne Elbers, Thomas Wiecki\n\n\nToday's blog post is co-written by \nDanne Elbers\n who is doing her masters thesis with me on computational psychiatry using Bayesian modeling. This post also borrows heavily from a \nNotebook\n by \nChris Fonnesbeck\n.\n\n\nThe power of Bayesian modelling really clicked for me when I was first introduced to hierarchical modelling. In this blog post we will:\n\n\n\n\nprovide and intuitive explanation of hierarchical/multi-level Bayesian modeling;\n\n\nshow how this type of model can easily be built and estimated in \nPyMC3\n;\n\n\ndemonstrate the advantage of using hierarchical Bayesian modelling as opposed to non-hierarchical Bayesian modelling by comparing the two;\n\n\nvisualize the \"shrinkage effect\" (explained below); and\n\n\nhighlight connections to the frequentist version of this model.\n\n\n\n\nHaving multiple sets of related measurements comes up all the time. In mathematical psychology, for example, you test multiple subjects on the same task. We then want to estimate a computational/mathematical model that describes the behavior on the task by a set of parameters. We could thus fit a model to each subject individually, assuming they share no similarities; or, pool all the data and estimate one model assuming all subjects are identical. Hierarchical modeling allows the best of both worlds by modeling subjects' similarities but also allowing estimiation of individual parameters. As an aside, software from our lab, \nHDDM\n, allows hierarchical Bayesian estimation of a widely used decision making model in psychology. In this blog post, however, we will use a more classical example of \nhierarchical linear regression\n to predict radon levels in houses.\n\n\nThis is the 3rd blog post on the topic of Bayesian modeling in PyMC3, see here for the previous two:\n\n\n\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n  \n\n\n\n\nThe data set\n\n\nGelman et al.'s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all counties of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil.\nHere we'll investigate this differences and try to make predictions of radonlevels in different counties based on the county itself and the presence of a basement. In this example we'll look at Minnesota, a state that contains 85 counties in which different measurements are taken, ranging from 2 to 116 measurements per county. \n\n\n\n\nFirst, we'll load the data: \n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('data/radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\nn_counties = len(data.county.unique())\n\n\n\n\nThe relevant part of the data we will model looks as follows:\n\n\ndata[['county', 'log_radon', 'floor']].head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ncounty\n\n      \nlog_radon\n\n      \nfloor\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n AITKIN\n\n      \n 0.832909\n\n      \n 1\n\n    \n\n    \n\n      \n1\n\n      \n AITKIN\n\n      \n 0.832909\n\n      \n 0\n\n    \n\n    \n\n      \n2\n\n      \n AITKIN\n\n      \n 1.098612\n\n      \n 0\n\n    \n\n    \n\n      \n3\n\n      \n AITKIN\n\n      \n 0.095310\n\n      \n 0\n\n    \n\n    \n\n      \n4\n\n      \n  ANOKA\n\n      \n 1.163151\n\n      \n 0\n\n    \n\n  \n\n\n\n\n\n\n\nAs you can see, we have multiple \nradon\n measurements (log-converted to be on the real line) -- one row for each house -- in a \ncounty\n and whether the house has a basement (\nfloor\n == 0) or not (\nfloor\n == 1). We are interested in whether having a basement increases the \nradon\n measured in the house. \n\n\nThe Models\n\n\nPooling of measurements\n\n\nNow you might say: \"That's easy! I'll just pool all my data and estimate one big regression to asses the influence of a basement across all counties\". In math-speak that model would be:\n\n\n$$radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon$$\n \n\n\nWhere \n$i$\n represents the measurement, \n$c$\n the county and floor contains a 0 or 1 if the house has a basement or not, respectively. If you need a refresher on Linear Regressions in \nPyMC\n, check out my \nprevious blog post\n. Critically, we are only estimating \none\n intercept and \none\n slope for all measurements over all counties pooled together as illustrated in the graphic below (\n$\\theta$\n represents \n$(\\alpha, \\beta)$\n in our case and \n$y_i$\n are the measurements of the \n$i$\nth county).\n\n\n\n\nUnpooled measurements: separate regressions\n\n\nBut what if we are interested in whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say \"OK then, I'll just estimate \n$n$\n (number of counties) different regressions -- one for each county\". In math-speak that model would be:\n\n\n$$radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c$$\n\n\nNote that we added the subindex \n$c$\n so we are estimating \n$n$\n different \n$\\alpha$\ns and \n$\\beta$\ns -- one for each county.\n\n\n\n\nThis is the extreme opposite model; where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever. As we show below, this type of model can be very noisy when we have little data per county, as is the case in this data set.\n\n\nPartial pooling: Hierarchical Regression aka, the best of both worlds\n\n\nFortunately, there is a middle ground to both of these extremes. Specifically, we may assume that while \n$\\alpha$\ns and \n$\\beta$\ns are different for each county as in the unpooled case, the coefficients all share similarity. We can model this by assuming that each individual coefficient comes from a common group distribution:\n\n\n$$\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$\n\n\n$$\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)$$\n\n\nWe thus assume the intercepts \n$\\alpha$\n and slopes \n$\\beta$\n to come from a normal distribution centered around their respective group mean \n$\\mu$\n with a certain standard deviation \n$\\sigma^2$\n, the values (or rather posteriors) of which we also estimate. That's why this is called a multilevel, hierarchical or partial-pooling modeling.\n\n\n\n\nHow do we estimate such a complex model you might ask? Well, that's the beauty of Probabilistic Programming -- we just formulate the model we want and press our \nInference Button(TM)\n. \n\n\n(Note that the above is not a complete Bayesian model specification as we haven't defined priors or hyperpriors (i.e. priors for the group distribution, \n$\\mu$\n and \n$\\sigma$\n). These will be used in the model implementation below but only distract here.)\n\n\nProbabilistic Programming\n\n\nUnpooled/non-hierarchical model\n\n\nTo really highlight the effect of the hierarchical linear regression we'll first estimate the non-hierarchical, unpooled Bayesian model from above (separate regressions). For each county we estimate a completely separate model. As we have no prior information on what the intercept or regressions could be, we will be using a normal distribution centered around 0 with a wide standard-deviation to describe the intercept and regressions. We'll assume the measurements are normally distributed with noise \n$\\epsilon$\n on which we place a uniform distribution.  \n\n\nindiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.ix[data.county == county_name]\n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n\n    with pm.Model() as individual_model:\n        # Intercept prior (variance == sd**2)\n        a = pm.Normal('alpha', mu=0, sd=100**2)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sd=100**2)\n\n        # Model error prior\n        eps = pm.Uniform('eps', lower=0, upper=100)\n\n        # Linear model\n        radon_est = a + b * c_floor_measure\n\n        # Data likelihood\n        radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        step = pm.NUTS()\n        trace = pm.sample(2000, step=step, progressbar=False)\n\n    # keep trace for later analysis\n    indiv_traces[county_name] = trace\n\n\n\n\nHierarchical Model\n\n\nInstead of creating models separatley, the hierarchical model creates group parameters that consider the countys not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county's \n$\\alpha$\n and \n$\\beta$\n.\n\n\nwith pm.Model() as hierarchical_model:\n    # Hyperpriors for group nodes\n    mu_a = pm.Normal('mu_alpha', mu=0., sd=100**2)\n    sigma_a = pm.Uniform('sigma_alpha', lower=0, upper=100)\n    mu_b = pm.Normal('mu_beta', mu=0., sd=100**2)\n    sigma_b = pm.Uniform('sigma_beta', lower=0, upper=100)\n\n    # Intercept for each county, distributed around group mean mu_a\n    # Above we just set mu and sd to a fixed value while here we\n    # plug in a common group distribution for all a and b (which are\n    # vectors of length n_counties).\n    a = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=n_counties)\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=n_counties)\n\n    # Model error\n    eps = pm.Uniform('eps', lower=0, upper=100)\n\n    # Model prediction of radon level\n    # a[county_idx] translates to a[0, 0, 0, 1, 1, ...],\n    # we thus link multiple household measures of a county\n    # to its coefficients.\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n\n    # Data likelihood\n    radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=data.log_radon)\n\n\n\n\n# Inference button (TM)!\nwith hierarchical_model:\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    hierarchical_trace = pm.sample(2000, step, start=start, progressbar=False)\n\n\n\n\n# Plotting the hierarchical model trace -its found values- from 500 iterations onwards (right side plot) \n# and its accumulated marginal values (left side plot) \npm.traceplot(hierarchical_trace[500:]);\n\n\n\n\n\n\nThe marginal posteriors in the left column are highly informative. \nmu_a\n tells us the group mean (log) radon levels. \nmu_b\n tells us that having no basement decreases radon levels significantly (no mass above zero). We can also see by looking at the marginals for \na\n that there is quite some differences in radon levels between counties (each 'rainbow' color corresponds to a single county); the different widths are related to how much confidence we have in each paramter estimate -- the more measurements per county, the higher our confidence will be.\n\n\nPosterior Predictive Check\n\n\nThe Root Mean Square Deviation\n\n\nTo find out which of the models explains the data better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.\n\n\nWhen computing the RMSD (code not shown) we get the following result:\n\n\n\n\nindividual/non-hierarchical model: 0.13\n\n\nhierarchical model: 0.08\n\n\n\n\nAs can be seen above the hierarchical model performs better than the non-hierarchical model in predicting the radon values. Following this, we'll plot some examples of county's showing the actual radon measurements, the hierarchial predictions and the non-hierarchical predictions. \n\n\nselection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.ix[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][500::10], indiv_traces[c]['beta'][500::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.1)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][500::10].mean() + indiv_traces[c]['beta'][500::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][500::10][z], hierarchical_trace['beta'][500::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.1)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][500::10][z].mean() + hierarchical_trace['beta'][500::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'no basement'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')\n\n\n\n\n\n\nIn the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.\n\n\nWhen looking at the county 'CASS' we see that the non-hierarchical estimation is strongly biased: as this county's data contains only households with a basement the estimated regression produces the non-sensical result of a giant negative slope meaning that we would expect negative radon levels in a house without basement!\n\n\nMoreover, in the example county's 'CROW WING' and 'FREEBORN' the non-hierarchical model appears to react more strongly than the hierarchical model to the existance of outliers in the dataset ('CROW WING': no basement upper right. 'FREEBORN': basement upper left). Assuming that there should be a higher amount of radon gas measurable in households with basements opposed to those without, the county 'CROW WING''s non-hierachical model seems off. Having the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa.\n\n\nShrinkage\n\n\nShrinkage describes the process by which our estimates are \"pulled\" towards the group-mean as a result of the common group distribution -- county-coefficients very far away from the group mean have very low probability under the normality assumption, moving them closer to the group mean gives them higher probability. In the non-hierachical model every county is allowed to differ completely from the others by just using each county's data, resulting in a model more prone to outliers (as shown above). \n\n\nhier_a = hierarchical_trace['alpha'][500:].mean(axis=0)\nhier_b = hierarchical_trace['beta'][500:].mean(axis=0)\nindv_a = [indiv_traces[c]['alpha'][500:].mean() for c in county_names]\nindv_b = [indiv_traces[c]['beta'][500:].mean() for c in county_names]\n\n\n\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, xlabel='Intercept', ylabel='Floor Measure', \n                     title='Hierarchical vs. Non-hierarchical Bayes', \n                     xlim=(0, 3), ylim=(-3, 3))\n\nax.scatter(indv_a,indv_b, s=26, alpha=0.4, label = 'non-hierarchical')\nax.scatter(hier_a,hier_b, c='red', s=26, alpha=0.4, label = 'hierarchical')\nfor i in range(len(indv_b)):  \n    ax.arrow(indv_a[i], indv_b[i], hier_a[i] - indv_a[i], hier_b[i] - indv_b[i], \n             fc=\nk\n, ec=\nk\n, length_includes_head=True, alpha=0.4, head_width=.04)\nax.legend();\n\n\n\n\n\n\nIn the shrinkage plot above we show the coefficients of each county's non-hierarchical posterior mean (blue) and the hierarchical posterior mean (red). To show the effect of shrinkage on a single coefficient-pair (alpha and beta) we connect the blue and red points belonging to the same county by an arrow. Some non-hierarchical posteriors are so far out that we couldn't display them in this plot (it makes the axes too wide). Interestingly, all hierarchical posteriors of the floor-measure seem to be around -0.6 indicating that having a basement in almost all county's is a clear indicator for heightened radon levels. The intercept (which we take for type of soil) appears to differ among countys. This information would have been difficult to find if we had only used the non-hierarchial model.\n\n\nCritically, many effects that look quite large and significant in the non-hiearchical model actually turn out to be much smaller when we take the group distribution into account (this point can also well be seen in plot \nIn[12]\n in \nChris' NB\n). Shrinkage can thus be viewed as a form of smart regularization that helps reduce false-positives!\n\n\nConnections to Frequentist statistics\n\n\nThis type of hierarchical, partial pooling model is known as a \nrandom effects model\n in frequentist terms. Interestingly, if we placed uniform priors on the group mean and variance in the above model, the resulting Bayesian model would be equivalent to a random effects model. One might imagine that the difference between a model with uniform or wide normal hyperpriors should not have a huge impact. However, \nGelman says\n encourages use of weakly-informative priors (like we did above) over flat priors.\n\n\nSummary\n\n\nIn this post, co-authored by Danne Elbers, we showed how a multi-level hierarchical Bayesian model gives the best of both worlds when we have multiple sets of measurements we expect to have similarity. The naive approach either pools all data together and ignores the individual differences, or treats each set as completely separate leading to noisy estimates, as shown above. By assuming that each individual data set (each county in our case) is distributed according to a group distribution -- which we simultaneously estimate -- we benefit from increased statistical power and smart regularization via the shrinkage effect. Probabilistic Programming in \nPyMC\n then makes Bayesian estimation of this model trivial.\n\n\nAs a follow-up we could also include other states into our model. For this we could add yet another layer to the hierarchy where each state is pooled at the country level. Finally, readers of my blog will notice that we didn't use \nglm()\n here as it does not play nice with hierarchical models yet.\n\n\nReferences\n\n\n\n\nThe underlying Notebook of this blog post\n\n\nBlog post: \nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nBlog post: \nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n  \n\n\nChris Fonnesbeck repo containing a more extensive analysis\n\n\nBlog post: \nShrinkage in multi-level hierarchical models\n by John Kruschke\n\n\nGelman, A.; Carlin; Stern; and Rubin, D., 2007, \"Replication data for: Bayesian Data Analysis, Second Edition\", \n\n\nGelman, A., \n Hill, J. (2006). \nData Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.\n\n\nGelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432\u2013435.\n\n\n\n\nAcknowledgements\n\n\nThanks to \nImri Sofer\n for feedback and teaching us about the connections to random-effects models and \nDan Dillon\n for useful comments on an earlier draft.", 
            "title": "Hierarchical linear regression"
        }, 
        {
            "location": "/GLM-hierarchical/#the-best-of-both-worlds-hierarchical-linear-regression-in-pymc3", 
            "text": "Authors: Danne Elbers, Thomas Wiecki  Today's blog post is co-written by  Danne Elbers  who is doing her masters thesis with me on computational psychiatry using Bayesian modeling. This post also borrows heavily from a  Notebook  by  Chris Fonnesbeck .  The power of Bayesian modelling really clicked for me when I was first introduced to hierarchical modelling. In this blog post we will:   provide and intuitive explanation of hierarchical/multi-level Bayesian modeling;  show how this type of model can easily be built and estimated in  PyMC3 ;  demonstrate the advantage of using hierarchical Bayesian modelling as opposed to non-hierarchical Bayesian modelling by comparing the two;  visualize the \"shrinkage effect\" (explained below); and  highlight connections to the frequentist version of this model.   Having multiple sets of related measurements comes up all the time. In mathematical psychology, for example, you test multiple subjects on the same task. We then want to estimate a computational/mathematical model that describes the behavior on the task by a set of parameters. We could thus fit a model to each subject individually, assuming they share no similarities; or, pool all the data and estimate one model assuming all subjects are identical. Hierarchical modeling allows the best of both worlds by modeling subjects' similarities but also allowing estimiation of individual parameters. As an aside, software from our lab,  HDDM , allows hierarchical Bayesian estimation of a widely used decision making model in psychology. In this blog post, however, we will use a more classical example of  hierarchical linear regression  to predict radon levels in houses.  This is the 3rd blog post on the topic of Bayesian modeling in PyMC3, see here for the previous two:   The Inference Button: Bayesian GLMs made easy with PyMC3  This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3", 
            "title": "The best of both worlds: Hierarchical Linear Regression in PyMC3"
        }, 
        {
            "location": "/GLM-hierarchical/#the-data-set", 
            "text": "Gelman et al.'s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all counties of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil.\nHere we'll investigate this differences and try to make predictions of radonlevels in different counties based on the county itself and the presence of a basement. In this example we'll look at Minnesota, a state that contains 85 counties in which different measurements are taken, ranging from 2 to 116 measurements per county.    First, we'll load the data:   %matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('data/radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\nn_counties = len(data.county.unique())  The relevant part of the data we will model looks as follows:  data[['county', 'log_radon', 'floor']].head()   \n   \n     \n       \n       county \n       log_radon \n       floor \n     \n   \n   \n     \n       0 \n        AITKIN \n        0.832909 \n        1 \n     \n     \n       1 \n        AITKIN \n        0.832909 \n        0 \n     \n     \n       2 \n        AITKIN \n        1.098612 \n        0 \n     \n     \n       3 \n        AITKIN \n        0.095310 \n        0 \n     \n     \n       4 \n         ANOKA \n        1.163151 \n        0 \n     \n      As you can see, we have multiple  radon  measurements (log-converted to be on the real line) -- one row for each house -- in a  county  and whether the house has a basement ( floor  == 0) or not ( floor  == 1). We are interested in whether having a basement increases the  radon  measured in the house.", 
            "title": "The data set"
        }, 
        {
            "location": "/GLM-hierarchical/#the-models", 
            "text": "", 
            "title": "The Models"
        }, 
        {
            "location": "/GLM-hierarchical/#pooling-of-measurements", 
            "text": "Now you might say: \"That's easy! I'll just pool all my data and estimate one big regression to asses the influence of a basement across all counties\". In math-speak that model would be:  $$radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon$$    Where  $i$  represents the measurement,  $c$  the county and floor contains a 0 or 1 if the house has a basement or not, respectively. If you need a refresher on Linear Regressions in  PyMC , check out my  previous blog post . Critically, we are only estimating  one  intercept and  one  slope for all measurements over all counties pooled together as illustrated in the graphic below ( $\\theta$  represents  $(\\alpha, \\beta)$  in our case and  $y_i$  are the measurements of the  $i$ th county).", 
            "title": "Pooling of measurements"
        }, 
        {
            "location": "/GLM-hierarchical/#unpooled-measurements-separate-regressions", 
            "text": "But what if we are interested in whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say \"OK then, I'll just estimate  $n$  (number of counties) different regressions -- one for each county\". In math-speak that model would be:  $$radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c$$  Note that we added the subindex  $c$  so we are estimating  $n$  different  $\\alpha$ s and  $\\beta$ s -- one for each county.   This is the extreme opposite model; where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever. As we show below, this type of model can be very noisy when we have little data per county, as is the case in this data set.", 
            "title": "Unpooled measurements: separate regressions"
        }, 
        {
            "location": "/GLM-hierarchical/#partial-pooling-hierarchical-regression-aka-the-best-of-both-worlds", 
            "text": "Fortunately, there is a middle ground to both of these extremes. Specifically, we may assume that while  $\\alpha$ s and  $\\beta$ s are different for each county as in the unpooled case, the coefficients all share similarity. We can model this by assuming that each individual coefficient comes from a common group distribution:  $$\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$  $$\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)$$  We thus assume the intercepts  $\\alpha$  and slopes  $\\beta$  to come from a normal distribution centered around their respective group mean  $\\mu$  with a certain standard deviation  $\\sigma^2$ , the values (or rather posteriors) of which we also estimate. That's why this is called a multilevel, hierarchical or partial-pooling modeling.   How do we estimate such a complex model you might ask? Well, that's the beauty of Probabilistic Programming -- we just formulate the model we want and press our  Inference Button(TM) .   (Note that the above is not a complete Bayesian model specification as we haven't defined priors or hyperpriors (i.e. priors for the group distribution,  $\\mu$  and  $\\sigma$ ). These will be used in the model implementation below but only distract here.)", 
            "title": "Partial pooling: Hierarchical Regression aka, the best of both worlds"
        }, 
        {
            "location": "/GLM-hierarchical/#probabilistic-programming", 
            "text": "", 
            "title": "Probabilistic Programming"
        }, 
        {
            "location": "/GLM-hierarchical/#unpoolednon-hierarchical-model", 
            "text": "To really highlight the effect of the hierarchical linear regression we'll first estimate the non-hierarchical, unpooled Bayesian model from above (separate regressions). For each county we estimate a completely separate model. As we have no prior information on what the intercept or regressions could be, we will be using a normal distribution centered around 0 with a wide standard-deviation to describe the intercept and regressions. We'll assume the measurements are normally distributed with noise  $\\epsilon$  on which we place a uniform distribution.    indiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.ix[data.county == county_name]\n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n\n    with pm.Model() as individual_model:\n        # Intercept prior (variance == sd**2)\n        a = pm.Normal('alpha', mu=0, sd=100**2)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sd=100**2)\n\n        # Model error prior\n        eps = pm.Uniform('eps', lower=0, upper=100)\n\n        # Linear model\n        radon_est = a + b * c_floor_measure\n\n        # Data likelihood\n        radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        step = pm.NUTS()\n        trace = pm.sample(2000, step=step, progressbar=False)\n\n    # keep trace for later analysis\n    indiv_traces[county_name] = trace", 
            "title": "Unpooled/non-hierarchical model"
        }, 
        {
            "location": "/GLM-hierarchical/#hierarchical-model", 
            "text": "Instead of creating models separatley, the hierarchical model creates group parameters that consider the countys not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county's  $\\alpha$  and  $\\beta$ .  with pm.Model() as hierarchical_model:\n    # Hyperpriors for group nodes\n    mu_a = pm.Normal('mu_alpha', mu=0., sd=100**2)\n    sigma_a = pm.Uniform('sigma_alpha', lower=0, upper=100)\n    mu_b = pm.Normal('mu_beta', mu=0., sd=100**2)\n    sigma_b = pm.Uniform('sigma_beta', lower=0, upper=100)\n\n    # Intercept for each county, distributed around group mean mu_a\n    # Above we just set mu and sd to a fixed value while here we\n    # plug in a common group distribution for all a and b (which are\n    # vectors of length n_counties).\n    a = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=n_counties)\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=n_counties)\n\n    # Model error\n    eps = pm.Uniform('eps', lower=0, upper=100)\n\n    # Model prediction of radon level\n    # a[county_idx] translates to a[0, 0, 0, 1, 1, ...],\n    # we thus link multiple household measures of a county\n    # to its coefficients.\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n\n    # Data likelihood\n    radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=data.log_radon)  # Inference button (TM)!\nwith hierarchical_model:\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    hierarchical_trace = pm.sample(2000, step, start=start, progressbar=False)  # Plotting the hierarchical model trace -its found values- from 500 iterations onwards (right side plot) \n# and its accumulated marginal values (left side plot) \npm.traceplot(hierarchical_trace[500:]);   The marginal posteriors in the left column are highly informative.  mu_a  tells us the group mean (log) radon levels.  mu_b  tells us that having no basement decreases radon levels significantly (no mass above zero). We can also see by looking at the marginals for  a  that there is quite some differences in radon levels between counties (each 'rainbow' color corresponds to a single county); the different widths are related to how much confidence we have in each paramter estimate -- the more measurements per county, the higher our confidence will be.", 
            "title": "Hierarchical Model"
        }, 
        {
            "location": "/GLM-hierarchical/#posterior-predictive-check", 
            "text": "", 
            "title": "Posterior Predictive Check"
        }, 
        {
            "location": "/GLM-hierarchical/#the-root-mean-square-deviation", 
            "text": "To find out which of the models explains the data better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.  When computing the RMSD (code not shown) we get the following result:   individual/non-hierarchical model: 0.13  hierarchical model: 0.08   As can be seen above the hierarchical model performs better than the non-hierarchical model in predicting the radon values. Following this, we'll plot some examples of county's showing the actual radon measurements, the hierarchial predictions and the non-hierarchical predictions.   selection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.ix[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][500::10], indiv_traces[c]['beta'][500::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.1)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][500::10].mean() + indiv_traces[c]['beta'][500::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][500::10][z], hierarchical_trace['beta'][500::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.1)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][500::10][z].mean() + hierarchical_trace['beta'][500::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'no basement'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')   In the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.  When looking at the county 'CASS' we see that the non-hierarchical estimation is strongly biased: as this county's data contains only households with a basement the estimated regression produces the non-sensical result of a giant negative slope meaning that we would expect negative radon levels in a house without basement!  Moreover, in the example county's 'CROW WING' and 'FREEBORN' the non-hierarchical model appears to react more strongly than the hierarchical model to the existance of outliers in the dataset ('CROW WING': no basement upper right. 'FREEBORN': basement upper left). Assuming that there should be a higher amount of radon gas measurable in households with basements opposed to those without, the county 'CROW WING''s non-hierachical model seems off. Having the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa.", 
            "title": "The Root Mean Square Deviation"
        }, 
        {
            "location": "/GLM-hierarchical/#shrinkage", 
            "text": "Shrinkage describes the process by which our estimates are \"pulled\" towards the group-mean as a result of the common group distribution -- county-coefficients very far away from the group mean have very low probability under the normality assumption, moving them closer to the group mean gives them higher probability. In the non-hierachical model every county is allowed to differ completely from the others by just using each county's data, resulting in a model more prone to outliers (as shown above).   hier_a = hierarchical_trace['alpha'][500:].mean(axis=0)\nhier_b = hierarchical_trace['beta'][500:].mean(axis=0)\nindv_a = [indiv_traces[c]['alpha'][500:].mean() for c in county_names]\nindv_b = [indiv_traces[c]['beta'][500:].mean() for c in county_names]  fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, xlabel='Intercept', ylabel='Floor Measure', \n                     title='Hierarchical vs. Non-hierarchical Bayes', \n                     xlim=(0, 3), ylim=(-3, 3))\n\nax.scatter(indv_a,indv_b, s=26, alpha=0.4, label = 'non-hierarchical')\nax.scatter(hier_a,hier_b, c='red', s=26, alpha=0.4, label = 'hierarchical')\nfor i in range(len(indv_b)):  \n    ax.arrow(indv_a[i], indv_b[i], hier_a[i] - indv_a[i], hier_b[i] - indv_b[i], \n             fc= k , ec= k , length_includes_head=True, alpha=0.4, head_width=.04)\nax.legend();   In the shrinkage plot above we show the coefficients of each county's non-hierarchical posterior mean (blue) and the hierarchical posterior mean (red). To show the effect of shrinkage on a single coefficient-pair (alpha and beta) we connect the blue and red points belonging to the same county by an arrow. Some non-hierarchical posteriors are so far out that we couldn't display them in this plot (it makes the axes too wide). Interestingly, all hierarchical posteriors of the floor-measure seem to be around -0.6 indicating that having a basement in almost all county's is a clear indicator for heightened radon levels. The intercept (which we take for type of soil) appears to differ among countys. This information would have been difficult to find if we had only used the non-hierarchial model.  Critically, many effects that look quite large and significant in the non-hiearchical model actually turn out to be much smaller when we take the group distribution into account (this point can also well be seen in plot  In[12]  in  Chris' NB ). Shrinkage can thus be viewed as a form of smart regularization that helps reduce false-positives!", 
            "title": "Shrinkage"
        }, 
        {
            "location": "/GLM-hierarchical/#connections-to-frequentist-statistics", 
            "text": "This type of hierarchical, partial pooling model is known as a  random effects model  in frequentist terms. Interestingly, if we placed uniform priors on the group mean and variance in the above model, the resulting Bayesian model would be equivalent to a random effects model. One might imagine that the difference between a model with uniform or wide normal hyperpriors should not have a huge impact. However,  Gelman says  encourages use of weakly-informative priors (like we did above) over flat priors.", 
            "title": "Connections to Frequentist statistics"
        }, 
        {
            "location": "/GLM-hierarchical/#summary", 
            "text": "In this post, co-authored by Danne Elbers, we showed how a multi-level hierarchical Bayesian model gives the best of both worlds when we have multiple sets of measurements we expect to have similarity. The naive approach either pools all data together and ignores the individual differences, or treats each set as completely separate leading to noisy estimates, as shown above. By assuming that each individual data set (each county in our case) is distributed according to a group distribution -- which we simultaneously estimate -- we benefit from increased statistical power and smart regularization via the shrinkage effect. Probabilistic Programming in  PyMC  then makes Bayesian estimation of this model trivial.  As a follow-up we could also include other states into our model. For this we could add yet another layer to the hierarchy where each state is pooled at the country level. Finally, readers of my blog will notice that we didn't use  glm()  here as it does not play nice with hierarchical models yet.", 
            "title": "Summary"
        }, 
        {
            "location": "/GLM-hierarchical/#references", 
            "text": "The underlying Notebook of this blog post  Blog post:  The Inference Button: Bayesian GLMs made easy with PyMC3  Blog post:  This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3     Chris Fonnesbeck repo containing a more extensive analysis  Blog post:  Shrinkage in multi-level hierarchical models  by John Kruschke  Gelman, A.; Carlin; Stern; and Rubin, D., 2007, \"Replication data for: Bayesian Data Analysis, Second Edition\",   Gelman, A.,   Hill, J. (2006).  Data Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.  Gelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432\u2013435.", 
            "title": "References"
        }, 
        {
            "location": "/GLM-hierarchical/#acknowledgements", 
            "text": "Thanks to  Imri Sofer  for feedback and teaching us about the connections to random-effects models and  Dan Dillon  for useful comments on an earlier draft.", 
            "title": "Acknowledgements"
        }, 
        {
            "location": "/pmf-pymc/", 
            "text": "Probabilistic Matrix Factorization for Making Personalized Recommendations\n\n\nThe model discussed in this analysis was developed by Ruslan Salakhutdinov and Andriy Mnih. All of the code and supporting text, when not referenced, is the original work of \nMack Sweeney\n.\n\n\nMotivation\n\n\nSay I download a handbook of a hundred jokes, and I'd like to know very quickly which ones will be my favorite. So maybe I read a few, I laugh, I read a few more, I stop laughing, and I indicate on a scale of -10 to 10 how funny I thought each joke was. Maybe I do this for 5 jokes out of the 100. Now I go to the back of the book, and there's a little program included for calculating my preferences for all the other jokes. I enter in my preference numbers and shazam! The program spits out a list of all 100 jokes, sorted in the order I'll like them. That certainly would be nice. Today we'll write a program that does exactly this.\n\n\nWe'll start out by getting some intuition for how our model will work. Then we'll formalize our intuition. Afterwards, we'll examine the dataset we are going to use. Once we have some notion of what our data looks like, we'll define some baseline methods for predicting preferences for jokes. Following that, we'll look at Probabilistic Matrix Factorization (PMF), which is a more sophisticated Bayesian method for predicting preferences. Having detailed the PMF model, we'll use PyMC3 for MAP estimation and MCMC inference. Finally, we'll compare the results obtained with PMF to those obtained from our baseline methods and discuss the outcome.\n\n\nIntuition\n\n\nNormally if we want recommendations for something, we try to find people who are similar to us and ask their opinions. If Bob, Alice, and Monty are all similar to me, and they all like knock-knock jokes, I'll probably like knock-knock jokes. Now this isn't always true. It depends on what we consider to be \"similar\". In order to get the best bang for our buck, we really want to look for people who have the most similar sense of humor. Humor being a complex beast, we'd probably like to break it down into something more understandable. We might try to characterize each joke in terms of various factors. Perhaps jokes can be dry, sarcastic, crude, sexual, political, etc. Now imagine we go through our handbook of jokes and assign each joke a rating in each of the categories. How dry is it? How sarcastic is it? How much does it use sexual innuendos? Perhaps we use numbers between 0 and 1 for each category. Intuitively, we might call this the joke's humor profile.\n\n\nNow let's suppose we go back to those 5 jokes we rated. At this point, we can get a richer picture of our own preferences by looking at the humor profiles of each of the jokes we liked and didn't like. Perhaps we take the averages across the 5 humor profiles and call this our ideal type of joke. In other words, we have computed some notion of our inherent \npreferences\n for various types of jokes. Suppose Bob, Alice, and Monty all do the same. Now we can compare our preferences and determine how similar each of us really are. I might find that Bob is the most similar and the other two are still more similar than other people, but not as much as Bob. So I want recommendations from all three people, but when I make my final decision, I'm going to put more weight on Bob's recommendation than those I get from Alice and Monty.\n\n\nWhile the above procedure sounds fairly effective as is, it also reveals an unexpected additional source of information. If we rated a particular joke highly, and we know its humor profile, we can compare with the profiles of other jokes. If we find one with very close numbers, it is probable we'll also enjoy this joke. Both this approach and the one above are commonly known as \nneighborhood approaches\n. Techniques that leverage both of these approaches simultaneously are often called \ncollaborative filtering\n \n[1]\n. The first approach we talked about uses user-user similarity, while the second uses item-item similarity. Ideally, we'd like to use both sources of information. The idea is we have a lot of items available to us, and we'd like to work together with others to filter the list of items down to those we'll each like best. My list should have the items I'll like best at the top and those I'll like least at the bottom. Everyone else wants the same. If I get together with a bunch of other people, we all read 5 jokes, and we have some efficient computational process to determine similarity, we can very quickly order the jokes to our liking.\n\n\nFormalization\n\n\nLet's take some time to make the intuitive notions we've been discussing more concrete. We have a set of \n$M$\n jokes, or \nitems\n (\n$M = 100$\n in our example above). We also have \n$N$\n people, whom we'll call \nusers\n of our recommender system. For each item, we'd like to find a \n$D$\n dimensional factor composition (humor profile above) to describe the item. Ideally, we'd like to do this without actually going through and manually labeling all of the jokes. Manual labeling would be both slow and error-prone, as different people will likely label jokes differently. So we model each joke as a \n$D$\n dimensional vector, which is its latent factor composition. Furthermore, we expect each user to have some preferences, but without our manual labeling and averaging procedure, we have to rely on the latent factor compositions to learn \n$D$\n dimensional latent preference vectors for each user. The only thing we get to observe is the \n$N \\times M$\n ratings matrix \n$R$\n provided by the users. Entry \n$R_{ij}$\n is the rating user \n$i$\n gave to item \n$j$\n. Many of these entries may be missing, since most users will not have rated all 100 jokes. Our goal is to fill in the missing values with predicted ratings based on the latent variables \n$U$\n and \n$V$\n. We denote the predicted ratings by \n$R_{ij}^*$\n. We also define an indicator matrix \n$I$\n, with entry \n$I_{ij} = 0$\n if \n$R_{ij}$\n is missing and \n$I_{ij} = 1$\n otherwise.\n\n\nSo we have an \n$N \\times D$\n matrix of user preferences which we'll call \n$U$\n and an \n$M \\times D$\n factor composition matrix we'll call \n$V$\n. We also have a \n$N \\times M$\n rating matrix we'll call \n$R$\n. We can think of each row \n$U_i$\n as indications of how much each user prefers each of the \n$D$\n latent factors. Each row \n$V_j$\n can be thought of as how much each item can be described by each of the latent factors. In order to make a recommendation, we need a suitable prediction function which maps a user preference vector \n$U_i$\n and an item latent factor vector \n$V_j$\n to a predicted ranking. The choice of this prediction function is an important modeling decision, and a variety of prediction functions have been used. Perhaps the most common is the dot product of the two vectors, \n$U_i \\cdot V_j$\n \n[1]\n.\n\n\nTo better understand CF techniques, let us explore a particular example. Imagine we are seeking to recommend jokes using a model which infers five latent factors, \n$V_j$\n, for \n$j = 1,2,3,4,5$\n. In reality, the latent factors are often unexplainable in a straightforward manner, and most models make no attempt to understand what information is being captured by each factor.  However, for the purposes of explanation, let us assume the five latent factors might end up capturing the humor profile we were discussing above. So our five latent factors are: dry, sarcastic, crude, sexual, and political. Then for a particular user \n$i$\n, imagine we infer a preference vector \n$U_i = \n0.2, 0.1, 0.3, 0.1, 0.3\n$\n. Also, for a particular item \n$j$\n, we infer these values for the latent factors: \n$V_j = \n0.5, 0.5, 0.25, 0.8, 0.9\n$\n. Using the dot product as the prediction function, we would calculate 0.575 as the ranking for that item, which is more or less a neutral preference given our -10 to 10 rating scale.\n\n\n$$0.2 \\times 0.5 + 0.1 \\times 0.5 + 0.3 \\times 0.25 + 0.1 \\times 0.8 + 0.3\n\\times 0.9 = 0.575$$\n\n\nData\n\n\nThe \nv1 Jester dataset\n provides something very much like the handbook of jokes we have been discussing. The original version of this dataset was constructed in conjunction with the development of the \nEigentaste recommender system\n \n[2]\n. At this point in time, v1 contains over 4.1 million continuous ratings in the range [-10, 10] of 100 jokes from 73,421 users. These ratings were collected between Apr. 1999 and May 2003. In order to reduce the training time of the model for illustrative purposes, 1,000 users who have rated all 100 jokes will be selected randomly. We will implement a model that is suitable for collaborative filtering on this data and evaluate it in terms of root mean squared error (RMSE) to validate the results.\n\n\nLet's begin by exploring our data. We want to get a general feel for what it looks like and a sense for what sort of patterns it might contain.\n\n\n% matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv('data/jester-dataset-v1-dense-first-1000.csv')\ndata.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \n1\n\n      \n2\n\n      \n3\n\n      \n4\n\n      \n5\n\n      \n6\n\n      \n7\n\n      \n8\n\n      \n9\n\n      \n10\n\n      \n...\n\n      \n91\n\n      \n92\n\n      \n93\n\n      \n94\n\n      \n95\n\n      \n96\n\n      \n97\n\n      \n98\n\n      \n99\n\n      \n100\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n 4.08\n\n      \n-0.29\n\n      \n 6.36\n\n      \n 4.37\n\n      \n-2.38\n\n      \n-9.66\n\n      \n-0.73\n\n      \n-5.34\n\n      \n 8.88\n\n      \n 9.22\n\n      \n...\n\n      \n 2.82\n\n      \n-4.95\n\n      \n-0.29\n\n      \n 7.86\n\n      \n-0.19\n\n      \n-2.14\n\n      \n 3.06\n\n      \n 0.34\n\n      \n-4.32\n\n      \n 1.07\n\n    \n\n    \n\n      \n1\n\n      \n-6.17\n\n      \n-3.54\n\n      \n 0.44\n\n      \n-8.50\n\n      \n-7.09\n\n      \n-4.32\n\n      \n-8.69\n\n      \n-0.87\n\n      \n-6.65\n\n      \n-1.80\n\n      \n...\n\n      \n-3.54\n\n      \n-6.89\n\n      \n-0.68\n\n      \n-2.96\n\n      \n-2.18\n\n      \n-3.35\n\n      \n 0.05\n\n      \n-9.08\n\n      \n-5.05\n\n      \n-3.45\n\n    \n\n    \n\n      \n2\n\n      \n 6.84\n\n      \n 3.16\n\n      \n 9.17\n\n      \n-6.21\n\n      \n-8.16\n\n      \n-1.70\n\n      \n 9.27\n\n      \n 1.41\n\n      \n-5.19\n\n      \n-4.42\n\n      \n...\n\n      \n 7.23\n\n      \n-1.12\n\n      \n-0.10\n\n      \n-5.68\n\n      \n-3.16\n\n      \n-3.35\n\n      \n 2.14\n\n      \n-0.05\n\n      \n 1.31\n\n      \n 0.00\n\n    \n\n    \n\n      \n3\n\n      \n-3.79\n\n      \n-3.54\n\n      \n-9.42\n\n      \n-6.89\n\n      \n-8.74\n\n      \n-0.29\n\n      \n-5.29\n\n      \n-8.93\n\n      \n-7.86\n\n      \n-1.60\n\n      \n...\n\n      \n 4.37\n\n      \n-0.29\n\n      \n 4.17\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-3.40\n\n      \n-4.95\n\n    \n\n    \n\n      \n4\n\n      \n 1.31\n\n      \n 1.80\n\n      \n 2.57\n\n      \n-2.38\n\n      \n 0.73\n\n      \n 0.73\n\n      \n-0.97\n\n      \n 5.00\n\n      \n-7.23\n\n      \n-1.36\n\n      \n...\n\n      \n 1.46\n\n      \n 1.70\n\n      \n 0.29\n\n      \n-3.30\n\n      \n 3.45\n\n      \n 5.44\n\n      \n 4.08\n\n      \n 2.48\n\n      \n 4.51\n\n      \n 4.66\n\n    \n\n  \n\n\n\n\n5 rows \u00d7 100 columns\n\n\n\n\n\n# Extract the ratings from the DataFrame\nall_ratings = np.ndarray.flatten(data.values)\nratings = pd.Series(all_ratings)\n\n# Plot histogram and density.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\nratings.plot(kind='density', ax=ax1, grid=False)\nax1.set_ylim(0, 0.08)\nax1.set_xlim(-11, 11)\n\n# Plot histogram\nratings.plot(kind='hist', ax=ax2, bins=20, grid=False)\nax2.set_xlim(-11, 11)\nplt.show()\n\n\n\n\n\n\nratings.describe()\n\n\n\n\ncount    100000.000000\nmean          0.996219\nstd           5.265215\nmin          -9.950000\n25%          -2.860000\n50%           1.650000\n75%           5.290000\nmax           9.420000\ndtype: float64\n\n\n\nThis must be a decent batch of jokes. From our exploration above, we know most ratings are in the range -1 to 10, and positive ratings are more likely than negative ratings. Let's look at the means for each joke to see if we have any particularly good (or bad) humor here.\n\n\njoke_means = data.mean(axis=0)\njoke_means.plot(kind='bar', grid=False, figsize=(16, 6),\n                title=\nMean Ratings for All 100 Jokes\n)\n\n\n\n\nmatplotlib.axes._subplots.AxesSubplot at 0x7fc46a3c5fd0\n\n\n\n\n\n\nWhile the majority of the jokes generally get positive feedback from users, there are definitely a few that stand out as poor humor. Let's take a look at the worst and best joke, just for fun.\n\n\nimport os\n\n# Worst and best joke?\nworst_joke_id = joke_means.argmin()\nbest_joke_id = joke_means.argmax()\n\n# Let's see for ourselves. Load the jokes.\njoke_dir = 'data/jokes'\nfiles = [os.path.join(joke_dir, fname) for fname in os.listdir(joke_dir)]\njokes = [fname for fname in files if fname.endswith('txt')]\nnums = [filter(lambda c: c.isdigit(), fname) for fname in jokes]\njoke_dict = {k: v for k, v in zip(nums, jokes)}\n\ndef read_joke(joke_id):\n    fname = joke_dict[joke_id]\n    with open(fname) as f:\n        return f.read()\n\nprint 'The worst joke:\\n---------------\\n%s\\n' % read_joke(worst_joke_id)\nprint 'The best joke:\\n--------------\\n%s' % read_joke(best_joke_id)\n\n\n\n\nThe worst joke:\n---------------\nA Joke \nHow many teddybears does it take to change a lightbulb?\nIt takes only one teddybear, but it takes a whole lot of lightbulbs.\n\nThe best joke:\n--------------\nA Joke \nA radio conversation of a US naval ship with Canadian authorities ...\n\nAmericans: Please divert your course 15 degrees to the North to avoid a collision.\nCanadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.\nAmericans: This is the Captain of a US Navy ship.  I say again, divert YOUR course.\nCanadians: No. I say again, you divert YOUR course.\nAmericans: This is the aircraft carrier USS LINCOLN, the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers, three cruisers and numerous support vessels. I demand that you change your course 15 degrees north, that's ONE FIVE DEGREES NORTH, or counter-measures will be undertaken to ensure the safety of this ship.\nCanadians: This is a lighthouse. Your call.\n\n\n\nMake sense to me. We now know there are definite popularity differences between the jokes. Some of them are simply funnier than others, and some are downright lousy. Looking at the joke means allowed us to discover these general trends. Perhaps there are similar trends across users. It might be the case that some users are simply more easily humored than others. Let's take a look.\n\n\nuser_means = data.mean(axis=1)\nfig, ax = plt.subplots(figsize=(16, 6))\nuser_means.plot(kind='bar', grid=False, ax=ax,\n                title=\nMean Ratings for All 1000 Users\n)\nax.set_xticklabels('')  # 1000 labels is nonsensical\nfig.show()\n\n\n\n\n\n\nWe see even more significant trends here. Some users rate nearly everything highly, and some (though not as many) rate nearly everything negatively. These observations will come in handy when considering models to use for predicting user preferences on unseen jokes.\n\n\nMethods\n\n\nHaving explored the data, we're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read.\n\n\nBaselines\n\n\nEvery good analysis needs some kind of baseline methods to compare against. It's difficult to claim we've produced good results if we have no reference point for what defines \"good\". We'll define three very simple baseline methods and find the RMSE using these methods. Our goal will be to obtain lower RMSE scores with whatever model we produce.\n\n\nUniform Random Baseline\n\n\nOur first baseline is about as dead stupid as you can get. Every place we see a missing value in \n$R$\n, we'll simply fill it with a number drawn uniformly at random in the range [-10, 10]. We expect this method to do the worst by far.\n\n\n$$R_{ij}^* \\sim Uniform$$\n\n\nGlobal Mean Baseline\n\n\nThis method is only slightly better than the last. Wherever we have a missing value, we'll fill it in with the mean of all observed ratings.\n\n\n$$\\text{global_mean} = \\frac{1}{N \\times M} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij}(R_{ij})$$\n\n\n$$R_{ij}^* = \\text{global_mean}$$\n\n\nMean of Means Baseline\n\n\nNow we're going to start getting a bit smarter. We imagine some users might be easily amused, and inclined to rate all jokes more highly. Other users might be the opposite. Additionally, some jokes might simply be more witty than others, so all users might rate some jokes more highly than others in general. We can clearly see this in our graph of the joke means above. We'll attempt to capture these general trends through per-user and per-joke rating means. We'll also incorporate the global mean to smooth things out a bit. So if we see a missing value in cell \n$R_{ij}$\n, we'll average the global mean with the mean of \n$U_i$\n and the mean of \n$V_j$\n and use that value to fill it in.\n\n\n$$\\text{user_means} = \\frac{1}{M} \\sum_{j=1}^M I_{ij}(R_{ij})$$\n\n\n$$\\text{joke_means} = \\frac{1}{N} \\sum_{i=1}^N I_{ij}(R_{ij})$$\n\n\n$$R_{ij}^* = \\frac{1}{3} \\left(\\text{user_means}_i + \\text{ joke_means}_j + \\text{ global_mean} \\right)$$\n\n\nfrom collections import OrderedDict\n\n\n# Create a base class with scaffolding for our 3 baselines.\n\ndef split_title(title):\n    \nChange \nBaselineMethod\n to \nBaseline Method\n.\n\n    words = []\n    tmp = [title[0]]\n    for c in title[1:]:\n        if c.isupper():\n            words.append(''.join(tmp))\n            tmp = [c]\n        else:\n            tmp.append(c)\n    words.append(''.join(tmp))\n    return ' '.join(words)\n\n\nclass Baseline(object):\n    \nCalculate baseline predictions.\n\n\n    def __init__(self, train_data):\n        \nSimple heuristic-based transductive learning to fill in missing\n        values in data matrix.\n\n        self.predict(train_data.copy())\n\n    def predict(self, train_data):\n        raise NotImplementedError(\n            'baseline prediction not implemented for base class')\n\n    def rmse(self, test_data):\n        \nCalculate root mean squared error for predictions on test data.\n\n        return rmse(test_data, self.predicted)\n\n    def __str__(self):\n        return split_title(self.__class__.__name__)\n\n\n\n# Implement the 3 baselines.\n\nclass UniformRandomBaseline(Baseline):\n    \nFill missing values with uniform random values.\n\n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        pmin, pmax = masked_train.min(), masked_train.max()\n        N = nan_mask.sum()\n        train_data[nan_mask] = np.random.uniform(pmin, pmax, N)\n        self.predicted = train_data\n\n\nclass GlobalMeanBaseline(Baseline):\n    \nFill in missing values using the global mean.\n\n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        train_data[nan_mask] = train_data[~nan_mask].mean()\n        self.predicted = train_data\n\n\nclass MeanOfMeansBaseline(Baseline):\n    \nFill in missing values using mean of user/item/global means.\n\n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        global_mean = masked_train.mean()\n        user_means = masked_train.mean(axis=1)\n        item_means = masked_train.mean(axis=0)\n        self.predicted = train_data.copy()\n        n, m = train_data.shape\n        for i in xrange(n):\n            for j in xrange(m):\n                if np.ma.isMA(item_means[j]):\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i]))\n                else:\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i], item_means[j]))\n\n\nbaseline_methods = OrderedDict()\nbaseline_methods['ur'] = UniformRandomBaseline\nbaseline_methods['gm'] = GlobalMeanBaseline\nbaseline_methods['mom'] = MeanOfMeansBaseline\n\n\n\n\nProbabilistic Matrix Factorization\n\n\nProbabilistic Matrix Factorization (PMF)\n [3] is a probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective. The ratings \n$R$\n are modeled as draws from a Gaussian distribution.  The mean for \n$R_{ij}$\n is \n$U_i V_j^T$\n. The precision \n$\\alpha$\n is a fixed parameter that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance. Complexity is controlled by placing zero-mean spherical Gaussian priors on \n$U$\n and \n$V$\n. In other words, each row of \n$U$\n is drawn from a multivariate Gaussian with mean \n$\\mu = 0$\n and precision which is some multiple of the identity matrix \n$I$\n. Those multiples are \n$\\alpha_U$\n for \n$U$\n and \n$\\alpha_V$\n for \n$V$\n. So our model is defined by:\n\n\n$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n\n\n\\begin{equation}\nP(R \\given U, V, \\alpha^2) = \n    \\prod_{i=1}^N \\prod_{j=1}^M\n        \\left[ \\mathcal{N}(R_{ij} \\given U_i V_j^T, \\alpha^{-1}) \\right]^{I_{ij}}\n\\end{equation}\n\n\n\\begin{equation}\nP(U \\given \\alpha_U^2) =\n    \\prod_{i=1}^N \\mathcal{N}(U_i \\given 0, \\alpha_U^{-1} \\boldsymbol{I})\n\\end{equation}\n\n\n\\begin{equation}\nP(V \\given \\alpha_U^2) =\n    \\prod_{j=1}^M \\mathcal{N}(V_j \\given 0, \\alpha_V^{-1} \\boldsymbol{I})\n\\end{equation}\n\n\nGiven small precision parameters, the priors on \n$U$\n and \n$V$\n ensure our latent variables do not grow too far from 0. This prevents overly strong user preferences and item factor compositions from being learned. This is commonly known as complexity control, where the complexity of the model here is measured by the magnitude of the latent variables. Controlling complexity like this helps prevent overfitting, which allows the model to generalize better for unseen data. We must also choose an appropriate \n$\\alpha$\n value for the normal distribution for \n$R$\n. So the challenge becomes choosing appropriate values for \n$\\alpha_U$\n, \n$\\alpha_V$\n, and \n$\\alpha$\n. This challenge can be tackled with the soft weight-sharing methods discussed by \nNowland and Hinton, 1992\n [4]. However, for the purposes of this analysis, we will stick to using point estimates obtained from our data.\n\n\nimport time\nimport logging\nimport pymc3 as pm\nimport theano\nimport scipy as sp\n\n\n# Enable on-the-fly graph computations, but ignore \n# absence of intermediate test values.\ntheano.config.compute_test_value = 'ignore'\n\n# Set up logging.\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n\nclass PMF(object):\n    \nProbabilistic Matrix Factorization model using pymc3.\n\n\n    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(-10, 10)):\n        \nBuild the Probabilistic Matrix Factorization model using pymc3.\n\n        :param np.ndarray train: The training data to use for learning the model.\n        :param int dim: Dimensionality of the model; number of latent factors.\n        :param int alpha: Fixed precision for the likelihood function.\n        :param float std: Amount of noise to use for model initialization.\n        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n            These bounds will simply be used to cap the estimates produced for R.\n\n        \n\n        self.dim = dim\n        self.alpha = alpha\n        self.std = np.sqrt(1.0 / alpha)\n        self.bounds = bounds\n        self.data = train.copy()\n        n, m = self.data.shape\n\n        # Perform mean value imputation\n        nan_mask = np.isnan(self.data)\n        self.data[nan_mask] = self.data[~nan_mask].mean()\n\n        # Low precision reflects uncertainty; prevents overfitting.\n        # Set to the mean variance across users and items.\n        self.alpha_u = 1 / self.data.var(axis=1).mean()\n        self.alpha_v = 1 / self.data.var(axis=0).mean()\n\n        # Specify the model.\n        logging.info('building the PMF model')\n        with pm.Model() as pmf:\n            U = pm.MvNormal(\n                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n            V = pm.MvNormal(\n                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n            R = pm.Normal(\n                'R', mu=theano.tensor.dot(U, V.T), tau=self.alpha * np.ones((n, m)),\n                observed=self.data)\n\n        logging.info('done building the PMF model') \n        self.model = pmf\n\n    def __str__(self):\n        return self.name\n\n\n\n\n\nWe'll also need functions for calculating the MAP and performing sampling on our PMF model. When the observation noise variance \n$\\alpha$\n and the prior variances \n$\\alpha_U$\n and \n$\\alpha_V$\n are all kept fixed, maximizing the log posterior is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms.\n\n\n$$\nE = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - U_i V_j^T)^2 +\n    \\frac{\\lambda_U}{2} \\sum_{i=1}^N \\|U\\|_{Fro}^2 +\n    \\frac{\\lambda_V}{2} \\sum_{j=1}^M \\|V\\|_{Fro}^2,\n$$\n\n\nwhere \n$\\lambda_U = \\alpha_U / \\alpha$\n, \n$\\lambda_V = \\alpha_V / \\alpha$\n, and \n$\\|\\cdot\\|_{Fro}^2$\n denotes the Frobenius norm [3]. Minimizing this objective function gives a local minimum, which is essentially a maximum a posteriori (MAP) estimate. While it is possible to use a fast Stochastic Gradient Descent procedure to find this MAP, we'll be finding it using the utilities built into \npymc3\n. In particular, we'll use \nfind_MAP\n with Powell optimization (\nscipy.optimize.fmin_powell\n). Having found this MAP estimate, we can use it as our starting point for MCMC sampling.\n\n\nSince it is a reasonably complex model, we expect the MAP estimation to take some time. So let's save it after we've found it. Note that we define a function for finding the MAP below, assuming it will receive a namespace with some variables in it. Then we attach that function to the PMF class, where it will have such a namespace after initialization. The PMF class is defined in pieces this way so I can say a few things between each piece to make it clearer.\n\n\ntry:\n    import ujson as json\nexcept ImportError:\n    import json\n\n\n# First define functions to save our MAP estimate after it is found.\n# We adapt these from `pymc3`'s `backends` module, where the original\n# code is used to save the traces from MCMC samples.\ndef save_np_vars(vars, savedir):\n    \nSave a dictionary of numpy variables to `savedir`. We assume\n    the directory does not exist; an OSError will be raised if it does.\n    \n\n    logging.info('writing numpy vars to directory: %s' % savedir)\n    os.mkdir(savedir)\n    shapes = {}\n    for varname in vars:\n        data = vars[varname]\n        var_file = os.path.join(savedir, varname + '.txt')\n        np.savetxt(var_file, data.reshape(-1, data.size))\n        shapes[varname] = data.shape\n\n        ## Store shape information for reloading.\n        shape_file = os.path.join(savedir, 'shapes.json')\n        with open(shape_file, 'w') as sfh:\n            json.dump(shapes, sfh)\n\n\ndef load_np_vars(savedir):\n    \nLoad numpy variables saved with `save_np_vars`.\n\n    shape_file = os.path.join(savedir, 'shapes.json')\n    with open(shape_file, 'r') as sfh:\n        shapes = json.load(sfh)\n\n    vars = {}\n    for varname, shape in shapes.items():\n        var_file = os.path.join(savedir, varname + '.txt')\n        vars[varname] = np.loadtxt(var_file).reshape(shape)\n\n    return vars\n\n\n# Now define the MAP estimation infrastructure.\ndef _map_dir(self):\n    basename = 'pmf-map-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _find_map(self):\n    \nFind mode of posterior using Powell optimization.\n\n    tstart = time.time()\n    with self.model:\n        logging.info('finding PMF MAP using Powell optimization...')\n        self._map = pm.find_MAP(fmin=sp.optimize.fmin_powell, disp=True)\n\n    elapsed = int(time.time() - tstart)\n    logging.info('found PMF MAP in %d seconds' % elapsed)\n\n    # This is going to take a good deal of time to find, so let's save it.\n    save_np_vars(self._map, self.map_dir)\n\ndef _load_map(self):\n    self._map = load_np_vars(self.map_dir)\n\ndef _map(self):\n    try:\n        return self._map\n    except:\n        if os.path.isdir(self.map_dir):\n            self.load_map()\n        else:\n            self.find_map()\n        return self._map\n\n\n# Update our class with the new MAP infrastructure.\nPMF.find_map = _find_map\nPMF.load_map = _load_map\nPMF.map_dir = property(_map_dir)\nPMF.map = property(_map)\n\n\n\n\nSo now our PMF class has a \nmap\n \nproperty\n which will either be found using Powell optimization or loaded from a previous optimization. Once we have the MAP, we can use it as a starting point for our MCMC sampler. We'll need a sampling function in order to draw MCMC samples to approximate the posterior distribution of the PMF model.\n\n\n# Draw MCMC samples.\ndef _trace_dir(self):\n    basename = 'pmf-mcmc-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _draw_samples(self, nsamples=1000, njobs=2):\n    # First make sure the trace_dir does not already exist.\n    if os.path.isdir(self.trace_dir):\n        raise OSError(\n            'trace directory %s already exists. Please move or delete.' % self.trace_dir)\n    start = self.map  # use our MAP as the starting point\n    with self.model:\n        logging.info('drawing %d samples using %d jobs' % (nsamples, njobs))\n        step = pm.NUTS(scaling=start)\n        backend = pm.backends.Text(self.trace_dir)\n        logging.info('backing up trace to directory: %s' % self.trace_dir)\n        self.trace = pm.sample(nsamples, step, start=start, njobs=njobs, trace=backend)\n\ndef _load_trace(self):\n    with self.model:\n        self.trace = pm.backends.text.load(self.trace_dir)\n\n\n# Update our class with the sampling infrastructure.\nPMF.trace_dir = property(_trace_dir)\nPMF.draw_samples = _draw_samples\nPMF.load_trace = _load_trace\n\n\n\n\nWe could define some kind of default trace property like we did for the MAP, but that would mean using possibly nonsensical values for \nnsamples\n and \nnjobs\n. Better to leave it as a non-optional call to \ndraw_samples\n. Finally, we'll need a function to make predictions using our inferred values for \n$U$\n and \n$V$\n. For user \n$i$\n and joke \n$j$\n, a prediction is generated by drawing from \n$\\mathcal{N}(U_i V_j^T, \\alpha)$\n. To generate predictions from the sampler, we generate an \n$R$\n matrix for each \n$U$\n and \n$V$\n sampled, then we combine these by averaging over the \n$K$\n samples.\n\n\n\\begin{equation}\nP(R_{ij}^* \\given R, \\alpha, \\alpha_U, \\alpha_V) \\approx\n    \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(U_i V_j^T, \\alpha)\n\\end{equation}\n\n\nWe'll want to inspect the individual \n$R$\n matrices before averaging them for diagnostic purposes. So we'll write code for the averaging piece during evaluation. The function below simply draws an \n$R$\n matrix given a \n$U$\n and \n$V$\n and the fixed \n$\\alpha$\n stored in the PMF object.\n\n\ndef _predict(self, U, V):\n    \nEstimate R from the given values of U and V.\n\n    R = np.dot(U, V.T)\n    n, m = R.shape\n    sample_R = np.array([\n        [np.random.normal(R[i,j], self.std) for j in xrange(m)]\n        for i in xrange(n)\n    ])\n\n    # bound ratings\n    low, high = self.bounds\n    sample_R[sample_R \n low] = low\n    sample_R[sample_R \n high] = high\n    return sample_R\n\n\nPMF.predict = _predict\n\n\n\n\nOne final thing to note: the dot products in this model are often constrained using a logistic function \n$g(x) = 1/(1 + exp(-x))$\n, that bounds the predictions to the range [0, 1]. To facilitate this bounding, the ratings are also mapped to the range [0, 1] using \n$t(x) = (x + min) / range$\n. The authors of PMF also introduced a constrained version which performs better on users with less ratings [3]. Both models are generally improvements upon the basic model presented here. However, in the interest of time and space, these will not be implemented here.\n\n\nEvaluation\n\n\nMetrics\n\n\nIn order to understand how effective our models are, we'll need to be able to evaluate them. We'll be evaluating in terms of root mean squared error (RMSE), which looks like this:\n\n\n\\begin{equation}\nRMSE = \\sqrt{ \\frac{ \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - R_{ij}^*)^2 }\n                   { \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} } }\n\\end{equation}\n\n\nIn this case, the RMSE can be thought of as the standard deviation of our predictions from the actual user preferences.\n\n\n# Define our evaluation function.\ndef rmse(test_data, predicted):\n    \nCalculate root mean squared error.\n    Ignoring missing values in the test data.\n    \n\n    I = ~np.isnan(test_data)   # indicator for missing values\n    N = I.sum()                # number of non-missing values\n    sqerror = abs(test_data - predicted) ** 2  # squared error array\n    mse = sqerror[I].sum() / N                 # mean squared error\n    return np.sqrt(mse)                        # RMSE\n\n\n\n\nTraining Data vs. Test Data\n\n\nThe next thing we need to do is split our data into a training set and a test set. Matrix factorization techniques use \ntransductive learning\n rather than inductive learning. So we produce a test set by taking a random sample of the cells in the full \n$N \\times M$\n data matrix. The values selected as test samples are replaced with \nnan\n values in a copy of the original data matrix to produce the training set. Since we'll be producing random splits, let's also write out the train/test sets generated. This will allow us to replicate our results. We'd like to be able to idenfity which split is which, so we'll take a hash of the indices selected for testing and use that to save the data.\n\n\nimport hashlib\n\n\n# Define a function for splitting train/test data.\ndef split_train_test(data, percent_test=10):\n    \nSplit the data into train/test sets.\n    :param int percent_test: Percentage of data to use for testing. Default 10.\n    \n\n    n, m = data.shape             # # users, # jokes\n    N = n * m                     # # cells in matrix\n    test_size = N / percent_test  # use 10% of data as test set\n    train_size = N - test_size    # and remainder for training\n\n    # Prepare train/test ndarrays.\n    train = data.copy().values\n    test = np.ones(data.shape) * np.nan\n\n    # Draw random sample of training data to use for testing.\n    tosample = np.where(~np.isnan(train))       # ignore nan values in data\n    idx_pairs = zip(tosample[0], tosample[1])   # tuples of row/col index pairs\n    indices = np.arange(len(idx_pairs))         # indices of index pairs\n    sample = np.random.choice(indices, replace=False, size=test_size)\n\n    # Transfer random sample from train set to test set.\n    for idx in sample:\n        idx_pair = idx_pairs[idx]\n        test[idx_pair] = train[idx_pair]  # transfer to test set\n        train[idx_pair] = np.nan          # remove from train set\n\n    # Verify everything worked properly\n    assert(np.isnan(train).sum() == test_size)\n    assert(np.isnan(test).sum() == train_size)\n\n    # Finally, hash the indices and save the train/test sets.\n    index_string = ''.join(map(str, np.sort(sample)))\n    name = hashlib.sha1(index_string).hexdigest()\n    savedir = os.path.join('data', name)\n    save_np_vars({'train': train, 'test': test}, savedir)\n\n    # Return train set, test set, and unique hash of indices.\n    return train, test, name\n\n\ndef load_train_test(name):\n    \nLoad the train/test sets.\n\n    savedir = os.path.join('data', name)\n    vars = load_np_vars(savedir)\n    return vars['train'], vars['test']\n\n# train, test, name = split_train_test(data)\n\n\n\n\nIn order to facilitate reproducibility, I've produced a train/test split using the code above which we'll now use for all the evaluations below.\n\n\ntrain, test = load_train_test('6bb8d06c69c0666e6da14c094d4320d115f1ffc8')\n\n\n\n\nResults\n\n\n# Let's see the results:\nbaselines = {}\nfor name in baseline_methods:\n    Method = baseline_methods[name]\n    method = Method(train)\n    baselines[name] = method.rmse(test)\n    print '%s RMSE:\\t%.5f' % (method, baselines[name])\n\n\n\n\n\nUniform Random Baseline RMSE:   7.77062\nGlobal Mean Baseline RMSE:  5.25004\nMean Of Means Baseline RMSE:    4.79832\n\n\n\nAs expected: the uniform random baseline is the worst by far, the global mean baseline is next best, and the mean of means method is our best baseline. Now let's see how PMF stacks up.\n\n\n# We use a fixed precision for the likelihood.\n# This reflects uncertainty in the dot product.\n# We choose 2 in the footsteps Salakhutdinov\n# Mnihof.\nALPHA = 2\n\n# The dimensionality D; the number of latent factors.\n# We can adjust this higher to try to capture more subtle\n# characteristics of each joke. However, the higher it is,\n# the more expensive our inference procedures will be.\n# Specifically, we have D(N + M) latent variables. For our\n# Jester dataset, this means we have D(1100), so for 5\n# dimensions, we are sampling 5500 latent variables.\nDIM = 5\n\n\npmf = PMF(train, DIM, ALPHA, std=0.05)\n\n\n\n\nINFO:root:building the PMF model\nINFO:root:done building the PMF model\n\n\n\nPredictions Using MAP\n\n\n# Find MAP for PMF.\npmf.find_map()\n# pmf.load_map()\n\n\n\n\nINFO:root:finding MAP using Powell optimization...\nINFO:root:found MAP in 2575 seconds\n\n\nOptimization terminated successfully.\n         Current function value: 1553644.881552\n         Iterations: 33\n         Function evaluations: 1644948\n\n\n\nExcellent. The first thing we want to do is make sure the MAP estimate we obtained is reasonable. We can do this by computing RMSE on the predicted ratings obtained from the MAP values of \n$U$\n and \n$V$\n. First we define a function for generating the predicted ratings \n$R$\n from \n$U$\n and \n$V$\n. We ensure the actual rating bounds are enforced by setting all values below -10 to -10 and all values above 10 to 10. Finally, we compute RMSE for both the training set and the test set. We expect the test RMSE to be higher. The difference between the two gives some idea of how much we have overfit. Some difference is always expected, but a very low RMSE on the training set with a high RMSE on the test set is a definite sign of overfitting.\n\n\ndef eval_map(pmf_model, train, test):\n    U = pmf_model.map['U']\n    V = pmf_model.map['V']\n\n    # Make predictions and calculate RMSE on train \n test sets.\n    predictions = pmf_model.predict(U, V)\n    train_rmse = rmse(train, predictions)\n    test_rmse = rmse(test, predictions)\n    overfit = test_rmse - train_rmse\n\n    # Print report.\n    print 'PMF MAP training RMSE: %.5f' % train_rmse\n    print 'PMF MAP testing RMSE:  %.5f' % test_rmse\n    print 'Train/test difference: %.5f' % overfit\n\n    return test_rmse\n\n\n# Add eval function to PMF class.\nPMF.eval_map = eval_map\n\n\n\n\n# Evaluate PMF MAP estimates.\npmf_map_rmse = pmf.eval_map(train, test)\npmf_improvement = baselines['mom'] - pmf_map_rmse\nprint 'PMF MAP Improvement:   %.5f' % pmf_improvement\n\n\n\n\nPMF MAP training RMSE: 4.00824\nPMF MAP testing RMSE:  4.02974\nTrain/test difference: 0.02150\nPMF MAP Improvement:   0.76858\n\n\n\nSo we see a pretty nice improvement here when compared to our best baseline, which was the mean of means method. We also have a fairly small difference in the RMSE values between the train and the test sets. This indicates that the point estimates for \n$\\alpha_U$\n and \n$\\alpha_V$\n that we calculated from our data are doing a good job of controlling model complexity. Now let's see if we can improve our estimates by approximating our posterior distribution with MCMC sampling. We'll draw 1000 samples and back them up using the \npymc3.backend.Text\n backend.\n\n\nPredictions using MCMC\n\n\n# Draw MCMC samples.\npmf.draw_samples(5000, njobs=3)\n\n# uncomment to load previous trace rather than drawing new samples.\n# pmf.load_trace()\n\n\n\n\nINFO:root:drawing 5000 samples using 3 jobs\n/home/mack/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *\nINFO:root:backing up trace to directory: data/pmf-mcmc-d5\n\n\n [-----------------100%-----------------] 5001 of 5000 complete in 7506.2 sec\n\n\n\nDiagnostics and Posterior Predictive Check\n\n\nThe next step is to check how many samples we should discard as burn-in. Normally, we'd do this using a traceplot to get some idea of where the sampled variables start to converge. In this case, we have high-dimensional samples, so we need to find a way to approximate them. One way was proposed by \nSalakhutdinov and Mnih, p.886\n. We can calculate the Frobenius norms of \n$U$\n and \n$V$\n at each step and monitor those for convergence. This essentially gives us some idea when the average magnitude of the latent variables is stabilizing. The equations for the Frobenius norms of \n$U$\n and \n$V$\n are shown below. We will use \nnumpy\n's \nlinalg\n package to calculate these.\n\n\n$$\n\\|U\\|_{Fro}^2 = \\sqrt{\\sum_{i=1}^N \\sum_{d=1}^D |U_{id}|^2}, \\hspace{40pt}\n\\|V\\|_{Fro}^2 = \\sqrt{\\sum_{j=1}^M \\sum_{d=1}^D |V_{jd}|^2}\n$$\n\n\ndef _norms(pmf_model, monitor=('U', 'V'), ord='fro'):\n    \nReturn norms of latent variables at each step in the\n    sample trace. These can be used to monitor convergence\n    of the sampler.\n    \n\n    monitor = ('U', 'V')\n    norms = {var: [] for var in monitor}\n    for sample in pmf_model.trace:\n        for var in monitor:\n            norms[var].append(np.linalg.norm(sample[var], ord))\n    return norms\n\n\ndef _traceplot(pmf_model):\n    \nPlot Frobenius norms of U and V as a function of sample #.\n\n    trace_norms = pmf_model.norms()\n    u_series = pd.Series(trace_norms['U'])\n    v_series = pd.Series(trace_norms['V'])\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    u_series.plot(kind='line', ax=ax1, grid=False,\n                  title=\n$\\|U\\|_{Fro}^2$ at Each Sample\n)\n    v_series.plot(kind='line', ax=ax2, grid=False,\n                  title=\n$\\|V\\|_{Fro}^2$ at Each Sample\n)\n    ax1.set_xlabel(\nSample Number\n)\n    ax2.set_xlabel(\nSample Number\n)\n\n\nPMF.norms = _norms\nPMF.traceplot = _traceplot\n\n\n\n\npmf.traceplot()\n\n\n\n\n\n\nIt appears we get convergence of \n$U$\n and \n$V$\n after about 200 samples. When testing for convergence, we also want to see convergence of the particular statistics we are looking for, since different characteristics of the posterior may converge at different rates. Let's also do a traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample.\n\n\ndef _running_rmse(pmf_model, test_data, train_data, burn_in=0, plot=True):\n    \nCalculate RMSE for each step of the trace to monitor convergence.\n    \n\n    burn_in = burn_in if len(pmf_model.trace) \n= burn_in else 0\n    results = {'per-step-train': [], 'running-train': [],\n               'per-step-test': [], 'running-test': []}\n    R = np.zeros(test_data.shape)\n    for cnt, sample in enumerate(pmf_model.trace[burn_in:]):\n        sample_R = pmf_model.predict(sample['U'], sample['V'])\n        R += sample_R\n        running_R = R / (cnt + 1)\n        results['per-step-train'].append(rmse(train_data, sample_R))\n        results['running-train'].append(rmse(train_data, running_R))\n        results['per-step-test'].append(rmse(test_data, sample_R))\n        results['running-test'].append(rmse(test_data, running_R))\n\n    results = pd.DataFrame(results)\n\n    if plot:\n        results.plot(\n            kind='line', grid=False, figsize=(15, 7),\n            title='Per-step and Running RMSE From Posterior Predictive')\n\n    # Return the final predictions, and the RMSE calculations\n    return running_R, results\n\n\nPMF.running_rmse = _running_rmse\n\n\n\n\npredicted, results = pmf.running_rmse(test, train, burn_in=200)\n\n\n\n\n\n\n# And our final RMSE?\nfinal_test_rmse = results['running-test'].values[-1]\nfinal_train_rmse = results['running-train'].values[-1]\nprint 'Posterior predictive train RMSE: %.5f' % final_train_rmse\nprint 'Posterior predictive test RMSE:  %.5f' % final_test_rmse\nprint 'Train/test difference:           %.5f' % (final_test_rmse - final_train_rmse)\nprint 'Improvement from MAP:            %.5f' % (pmf_map_rmse - final_test_rmse)\nprint 'Improvement from Mean of Means:  %.5f' % (baselines['mom'] - final_test_rmse)\n\n\n\n\nPosterior predictive train RMSE: 3.92230\nPosterior predictive test RMSE:  4.18027\nTrain/test difference:           0.25797\nImprovement from MAP:            -0.15052\nImprovement from Mean of Means:  0.61806\n\n\n\nWe have some interesting results here. As expected, our MCMC sampler provides lower error on the training set. However, it seems it does so at the cost of overfitting the data. This results in a decrease in test RMSE as compared to the MAP, even though it is still much better than our best baseline. So why might this be the case? Recall that we used point estimates for our precision paremeters \n$\\alpha_U$\n and \n$\\alpha_V$\n and we chose a fixed precision \n$\\alpha$\n. It is quite likely that by doing this, we constrained our posterior in a way that biased it towards the training data. In reality, the variance in the user ratings and the joke ratings is unlikely to be equal to the means of sample variances we used. Also, the most reasonable observation precision \n$\\alpha$\n is likely different as well.\n\n\nSummary of Results\n\n\nLet's summarize our results.\n\n\nsize = 100  # RMSE doesn't really change after 100th sample anyway.\nall_results = pd.DataFrame({\n    'uniform random': np.repeat(baselines['ur'], size),\n    'global means': np.repeat(baselines['gm'], size),\n    'mean of means': np.repeat(baselines['mom'], size),\n    'PMF MAP': np.repeat(pmf_map_rmse, size),\n    'PMF MCMC': results['running-test'][:size],\n})\nfig, ax = plt.subplots(figsize=(10, 5))\nall_results.plot(kind='line', grid=False, ax=ax,\n                 title='RMSE for all methods')\nax.set_xlabel(\nNumber of Samples\n)\nax.set_ylabel(\nRMSE\n)\n\n\n\n\nmatplotlib.text.Text at 0x7f5327451a10\n\n\n\n\n\n\nSummary\n\n\nWe set out to predict user preferences for unseen jokes. First we discussed the intuitive notion behind the user-user and item-item neighborhood approaches to collaborative filtering. Then we formalized our intuitions. With a firm understanding of our problem context, we moved on to exploring our subset of the Jester data. After discovering some general patterns, we defined three baseline methods: uniform random, global mean, and mean of means. With the goal of besting our baseline methods, we implemented the basic version of Probabilistic Matrix Factorization (PMF) using \npymc3\n.\n\n\nOur results demonstrate that the mean of means method is our best baseline on our prediction task. As expected, we are able to obtain a significant decrease in RMSE using the PMF MAP estimate obtained via Powell optimization. We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space using the Frobenius norms of the sampled variables. The traceplots using this method seem to indicate that our sampler converged to the posterior. Results using this posterior showed that attempting to improve the MAP estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters \n$\\alpha$\n, \n$\\alpha_U$\n, and \n$\\alpha_V$\n.\n\n\nAs a followup to this analysis, it would be interesting to also implement the logistic and constrained versions of PMF. We expect both models to outperform the basic PMF model. We could also implement the \nfully Bayesian version of PMF\n (BPMF), which places hyperpriors on the model parameters to automatically learn ideal mean and precision parameters for \n$U$\n and \n$V$\n. This would likely resolve the issue we faced in this analysis. We would expect BPMF to improve upon the MAP estimation produced here by learning more suitable hyperparameters and parameters. For a basic (but working!) implementation of BPMF in \npymc3\n, see \nthis gist\n.\n\n\nIf you made it this far, then congratulations! You now have some idea of how to build a basic recommender system. These same ideas and methods can be used on many different recommendation tasks. Items can be movies, products, advertisements, courses, or even other people. Any time you can build yourself a user-item matrix with user preferences in the cells, you can use these types of collaborative filtering algorithms to predict the missing values. If you want to learn more about recommender systems, the first reference is a good place to start.\n\n\nReferences\n\n\n\n\nY. Koren, R. Bell, and C. Volinsky, \u201cMatrix Factorization Techniques for Recommender Systems,\u201d Computer, vol. 42, no. 8, pp. 30\u201337, Aug. 2009.\n\n\nK. Goldberg, T. Roeder, D. Gupta, and C. Perkins, \u201cEigentaste: A constant time collaborative filtering algorithm,\u201d Information Retrieval, vol. 4, no. 2, pp. 133\u2013151, 2001.\n\n\nA. Mnih and R. Salakhutdinov, \u201cProbabilistic matrix factorization,\u201d in Advances in neural information processing systems, 2007, pp. 1257\u20131264.\n\n\nS. J. Nowlan and G. E. Hinton, \u201cSimplifying Neural Networks by Soft Weight-sharing,\u201d Neural Comput., vol. 4, no. 4, pp. 473\u2013493, Jul. 1992.\n\n\nR. Salakhutdinov and A. Mnih, \u201cBayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo,\u201d in Proceedings of the 25th International Conference on Machine Learning, New York, NY, USA, 2008, pp. 880\u2013887.", 
            "title": "Probabilistic Matrix Factorization"
        }, 
        {
            "location": "/pmf-pymc/#probabilistic-matrix-factorization-for-making-personalized-recommendations", 
            "text": "The model discussed in this analysis was developed by Ruslan Salakhutdinov and Andriy Mnih. All of the code and supporting text, when not referenced, is the original work of  Mack Sweeney .", 
            "title": "Probabilistic Matrix Factorization for Making Personalized Recommendations"
        }, 
        {
            "location": "/pmf-pymc/#motivation", 
            "text": "Say I download a handbook of a hundred jokes, and I'd like to know very quickly which ones will be my favorite. So maybe I read a few, I laugh, I read a few more, I stop laughing, and I indicate on a scale of -10 to 10 how funny I thought each joke was. Maybe I do this for 5 jokes out of the 100. Now I go to the back of the book, and there's a little program included for calculating my preferences for all the other jokes. I enter in my preference numbers and shazam! The program spits out a list of all 100 jokes, sorted in the order I'll like them. That certainly would be nice. Today we'll write a program that does exactly this.  We'll start out by getting some intuition for how our model will work. Then we'll formalize our intuition. Afterwards, we'll examine the dataset we are going to use. Once we have some notion of what our data looks like, we'll define some baseline methods for predicting preferences for jokes. Following that, we'll look at Probabilistic Matrix Factorization (PMF), which is a more sophisticated Bayesian method for predicting preferences. Having detailed the PMF model, we'll use PyMC3 for MAP estimation and MCMC inference. Finally, we'll compare the results obtained with PMF to those obtained from our baseline methods and discuss the outcome.", 
            "title": "Motivation"
        }, 
        {
            "location": "/pmf-pymc/#intuition", 
            "text": "Normally if we want recommendations for something, we try to find people who are similar to us and ask their opinions. If Bob, Alice, and Monty are all similar to me, and they all like knock-knock jokes, I'll probably like knock-knock jokes. Now this isn't always true. It depends on what we consider to be \"similar\". In order to get the best bang for our buck, we really want to look for people who have the most similar sense of humor. Humor being a complex beast, we'd probably like to break it down into something more understandable. We might try to characterize each joke in terms of various factors. Perhaps jokes can be dry, sarcastic, crude, sexual, political, etc. Now imagine we go through our handbook of jokes and assign each joke a rating in each of the categories. How dry is it? How sarcastic is it? How much does it use sexual innuendos? Perhaps we use numbers between 0 and 1 for each category. Intuitively, we might call this the joke's humor profile.  Now let's suppose we go back to those 5 jokes we rated. At this point, we can get a richer picture of our own preferences by looking at the humor profiles of each of the jokes we liked and didn't like. Perhaps we take the averages across the 5 humor profiles and call this our ideal type of joke. In other words, we have computed some notion of our inherent  preferences  for various types of jokes. Suppose Bob, Alice, and Monty all do the same. Now we can compare our preferences and determine how similar each of us really are. I might find that Bob is the most similar and the other two are still more similar than other people, but not as much as Bob. So I want recommendations from all three people, but when I make my final decision, I'm going to put more weight on Bob's recommendation than those I get from Alice and Monty.  While the above procedure sounds fairly effective as is, it also reveals an unexpected additional source of information. If we rated a particular joke highly, and we know its humor profile, we can compare with the profiles of other jokes. If we find one with very close numbers, it is probable we'll also enjoy this joke. Both this approach and the one above are commonly known as  neighborhood approaches . Techniques that leverage both of these approaches simultaneously are often called  collaborative filtering   [1] . The first approach we talked about uses user-user similarity, while the second uses item-item similarity. Ideally, we'd like to use both sources of information. The idea is we have a lot of items available to us, and we'd like to work together with others to filter the list of items down to those we'll each like best. My list should have the items I'll like best at the top and those I'll like least at the bottom. Everyone else wants the same. If I get together with a bunch of other people, we all read 5 jokes, and we have some efficient computational process to determine similarity, we can very quickly order the jokes to our liking.", 
            "title": "Intuition"
        }, 
        {
            "location": "/pmf-pymc/#formalization", 
            "text": "Let's take some time to make the intuitive notions we've been discussing more concrete. We have a set of  $M$  jokes, or  items  ( $M = 100$  in our example above). We also have  $N$  people, whom we'll call  users  of our recommender system. For each item, we'd like to find a  $D$  dimensional factor composition (humor profile above) to describe the item. Ideally, we'd like to do this without actually going through and manually labeling all of the jokes. Manual labeling would be both slow and error-prone, as different people will likely label jokes differently. So we model each joke as a  $D$  dimensional vector, which is its latent factor composition. Furthermore, we expect each user to have some preferences, but without our manual labeling and averaging procedure, we have to rely on the latent factor compositions to learn  $D$  dimensional latent preference vectors for each user. The only thing we get to observe is the  $N \\times M$  ratings matrix  $R$  provided by the users. Entry  $R_{ij}$  is the rating user  $i$  gave to item  $j$ . Many of these entries may be missing, since most users will not have rated all 100 jokes. Our goal is to fill in the missing values with predicted ratings based on the latent variables  $U$  and  $V$ . We denote the predicted ratings by  $R_{ij}^*$ . We also define an indicator matrix  $I$ , with entry  $I_{ij} = 0$  if  $R_{ij}$  is missing and  $I_{ij} = 1$  otherwise.  So we have an  $N \\times D$  matrix of user preferences which we'll call  $U$  and an  $M \\times D$  factor composition matrix we'll call  $V$ . We also have a  $N \\times M$  rating matrix we'll call  $R$ . We can think of each row  $U_i$  as indications of how much each user prefers each of the  $D$  latent factors. Each row  $V_j$  can be thought of as how much each item can be described by each of the latent factors. In order to make a recommendation, we need a suitable prediction function which maps a user preference vector  $U_i$  and an item latent factor vector  $V_j$  to a predicted ranking. The choice of this prediction function is an important modeling decision, and a variety of prediction functions have been used. Perhaps the most common is the dot product of the two vectors,  $U_i \\cdot V_j$   [1] .  To better understand CF techniques, let us explore a particular example. Imagine we are seeking to recommend jokes using a model which infers five latent factors,  $V_j$ , for  $j = 1,2,3,4,5$ . In reality, the latent factors are often unexplainable in a straightforward manner, and most models make no attempt to understand what information is being captured by each factor.  However, for the purposes of explanation, let us assume the five latent factors might end up capturing the humor profile we were discussing above. So our five latent factors are: dry, sarcastic, crude, sexual, and political. Then for a particular user  $i$ , imagine we infer a preference vector  $U_i =  0.2, 0.1, 0.3, 0.1, 0.3 $ . Also, for a particular item  $j$ , we infer these values for the latent factors:  $V_j =  0.5, 0.5, 0.25, 0.8, 0.9 $ . Using the dot product as the prediction function, we would calculate 0.575 as the ranking for that item, which is more or less a neutral preference given our -10 to 10 rating scale.  $$0.2 \\times 0.5 + 0.1 \\times 0.5 + 0.3 \\times 0.25 + 0.1 \\times 0.8 + 0.3\n\\times 0.9 = 0.575$$", 
            "title": "Formalization"
        }, 
        {
            "location": "/pmf-pymc/#data", 
            "text": "The  v1 Jester dataset  provides something very much like the handbook of jokes we have been discussing. The original version of this dataset was constructed in conjunction with the development of the  Eigentaste recommender system   [2] . At this point in time, v1 contains over 4.1 million continuous ratings in the range [-10, 10] of 100 jokes from 73,421 users. These ratings were collected between Apr. 1999 and May 2003. In order to reduce the training time of the model for illustrative purposes, 1,000 users who have rated all 100 jokes will be selected randomly. We will implement a model that is suitable for collaborative filtering on this data and evaluate it in terms of root mean squared error (RMSE) to validate the results.  Let's begin by exploring our data. We want to get a general feel for what it looks like and a sense for what sort of patterns it might contain.  % matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv('data/jester-dataset-v1-dense-first-1000.csv')\ndata.head()   \n   \n     \n       \n       1 \n       2 \n       3 \n       4 \n       5 \n       6 \n       7 \n       8 \n       9 \n       10 \n       ... \n       91 \n       92 \n       93 \n       94 \n       95 \n       96 \n       97 \n       98 \n       99 \n       100 \n     \n   \n   \n     \n       0 \n        4.08 \n       -0.29 \n        6.36 \n        4.37 \n       -2.38 \n       -9.66 \n       -0.73 \n       -5.34 \n        8.88 \n        9.22 \n       ... \n        2.82 \n       -4.95 \n       -0.29 \n        7.86 \n       -0.19 \n       -2.14 \n        3.06 \n        0.34 \n       -4.32 \n        1.07 \n     \n     \n       1 \n       -6.17 \n       -3.54 \n        0.44 \n       -8.50 \n       -7.09 \n       -4.32 \n       -8.69 \n       -0.87 \n       -6.65 \n       -1.80 \n       ... \n       -3.54 \n       -6.89 \n       -0.68 \n       -2.96 \n       -2.18 \n       -3.35 \n        0.05 \n       -9.08 \n       -5.05 \n       -3.45 \n     \n     \n       2 \n        6.84 \n        3.16 \n        9.17 \n       -6.21 \n       -8.16 \n       -1.70 \n        9.27 \n        1.41 \n       -5.19 \n       -4.42 \n       ... \n        7.23 \n       -1.12 \n       -0.10 \n       -5.68 \n       -3.16 \n       -3.35 \n        2.14 \n       -0.05 \n        1.31 \n        0.00 \n     \n     \n       3 \n       -3.79 \n       -3.54 \n       -9.42 \n       -6.89 \n       -8.74 \n       -0.29 \n       -5.29 \n       -8.93 \n       -7.86 \n       -1.60 \n       ... \n        4.37 \n       -0.29 \n        4.17 \n       -0.29 \n       -0.29 \n       -0.29 \n       -0.29 \n       -0.29 \n       -3.40 \n       -4.95 \n     \n     \n       4 \n        1.31 \n        1.80 \n        2.57 \n       -2.38 \n        0.73 \n        0.73 \n       -0.97 \n        5.00 \n       -7.23 \n       -1.36 \n       ... \n        1.46 \n        1.70 \n        0.29 \n       -3.30 \n        3.45 \n        5.44 \n        4.08 \n        2.48 \n        4.51 \n        4.66 \n     \n     5 rows \u00d7 100 columns   # Extract the ratings from the DataFrame\nall_ratings = np.ndarray.flatten(data.values)\nratings = pd.Series(all_ratings)\n\n# Plot histogram and density.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\nratings.plot(kind='density', ax=ax1, grid=False)\nax1.set_ylim(0, 0.08)\nax1.set_xlim(-11, 11)\n\n# Plot histogram\nratings.plot(kind='hist', ax=ax2, bins=20, grid=False)\nax2.set_xlim(-11, 11)\nplt.show()   ratings.describe()  count    100000.000000\nmean          0.996219\nstd           5.265215\nmin          -9.950000\n25%          -2.860000\n50%           1.650000\n75%           5.290000\nmax           9.420000\ndtype: float64  This must be a decent batch of jokes. From our exploration above, we know most ratings are in the range -1 to 10, and positive ratings are more likely than negative ratings. Let's look at the means for each joke to see if we have any particularly good (or bad) humor here.  joke_means = data.mean(axis=0)\njoke_means.plot(kind='bar', grid=False, figsize=(16, 6),\n                title= Mean Ratings for All 100 Jokes )  matplotlib.axes._subplots.AxesSubplot at 0x7fc46a3c5fd0    While the majority of the jokes generally get positive feedback from users, there are definitely a few that stand out as poor humor. Let's take a look at the worst and best joke, just for fun.  import os\n\n# Worst and best joke?\nworst_joke_id = joke_means.argmin()\nbest_joke_id = joke_means.argmax()\n\n# Let's see for ourselves. Load the jokes.\njoke_dir = 'data/jokes'\nfiles = [os.path.join(joke_dir, fname) for fname in os.listdir(joke_dir)]\njokes = [fname for fname in files if fname.endswith('txt')]\nnums = [filter(lambda c: c.isdigit(), fname) for fname in jokes]\njoke_dict = {k: v for k, v in zip(nums, jokes)}\n\ndef read_joke(joke_id):\n    fname = joke_dict[joke_id]\n    with open(fname) as f:\n        return f.read()\n\nprint 'The worst joke:\\n---------------\\n%s\\n' % read_joke(worst_joke_id)\nprint 'The best joke:\\n--------------\\n%s' % read_joke(best_joke_id)  The worst joke:\n---------------\nA Joke \nHow many teddybears does it take to change a lightbulb?\nIt takes only one teddybear, but it takes a whole lot of lightbulbs.\n\nThe best joke:\n--------------\nA Joke \nA radio conversation of a US naval ship with Canadian authorities ...\n\nAmericans: Please divert your course 15 degrees to the North to avoid a collision.\nCanadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.\nAmericans: This is the Captain of a US Navy ship.  I say again, divert YOUR course.\nCanadians: No. I say again, you divert YOUR course.\nAmericans: This is the aircraft carrier USS LINCOLN, the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers, three cruisers and numerous support vessels. I demand that you change your course 15 degrees north, that's ONE FIVE DEGREES NORTH, or counter-measures will be undertaken to ensure the safety of this ship.\nCanadians: This is a lighthouse. Your call.  Make sense to me. We now know there are definite popularity differences between the jokes. Some of them are simply funnier than others, and some are downright lousy. Looking at the joke means allowed us to discover these general trends. Perhaps there are similar trends across users. It might be the case that some users are simply more easily humored than others. Let's take a look.  user_means = data.mean(axis=1)\nfig, ax = plt.subplots(figsize=(16, 6))\nuser_means.plot(kind='bar', grid=False, ax=ax,\n                title= Mean Ratings for All 1000 Users )\nax.set_xticklabels('')  # 1000 labels is nonsensical\nfig.show()   We see even more significant trends here. Some users rate nearly everything highly, and some (though not as many) rate nearly everything negatively. These observations will come in handy when considering models to use for predicting user preferences on unseen jokes.", 
            "title": "Data"
        }, 
        {
            "location": "/pmf-pymc/#methods", 
            "text": "Having explored the data, we're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read.", 
            "title": "Methods"
        }, 
        {
            "location": "/pmf-pymc/#baselines", 
            "text": "Every good analysis needs some kind of baseline methods to compare against. It's difficult to claim we've produced good results if we have no reference point for what defines \"good\". We'll define three very simple baseline methods and find the RMSE using these methods. Our goal will be to obtain lower RMSE scores with whatever model we produce.  Uniform Random Baseline  Our first baseline is about as dead stupid as you can get. Every place we see a missing value in  $R$ , we'll simply fill it with a number drawn uniformly at random in the range [-10, 10]. We expect this method to do the worst by far.  $$R_{ij}^* \\sim Uniform$$  Global Mean Baseline  This method is only slightly better than the last. Wherever we have a missing value, we'll fill it in with the mean of all observed ratings.  $$\\text{global_mean} = \\frac{1}{N \\times M} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij}(R_{ij})$$  $$R_{ij}^* = \\text{global_mean}$$  Mean of Means Baseline  Now we're going to start getting a bit smarter. We imagine some users might be easily amused, and inclined to rate all jokes more highly. Other users might be the opposite. Additionally, some jokes might simply be more witty than others, so all users might rate some jokes more highly than others in general. We can clearly see this in our graph of the joke means above. We'll attempt to capture these general trends through per-user and per-joke rating means. We'll also incorporate the global mean to smooth things out a bit. So if we see a missing value in cell  $R_{ij}$ , we'll average the global mean with the mean of  $U_i$  and the mean of  $V_j$  and use that value to fill it in.  $$\\text{user_means} = \\frac{1}{M} \\sum_{j=1}^M I_{ij}(R_{ij})$$  $$\\text{joke_means} = \\frac{1}{N} \\sum_{i=1}^N I_{ij}(R_{ij})$$  $$R_{ij}^* = \\frac{1}{3} \\left(\\text{user_means}_i + \\text{ joke_means}_j + \\text{ global_mean} \\right)$$  from collections import OrderedDict\n\n\n# Create a base class with scaffolding for our 3 baselines.\n\ndef split_title(title):\n     Change  BaselineMethod  to  Baseline Method . \n    words = []\n    tmp = [title[0]]\n    for c in title[1:]:\n        if c.isupper():\n            words.append(''.join(tmp))\n            tmp = [c]\n        else:\n            tmp.append(c)\n    words.append(''.join(tmp))\n    return ' '.join(words)\n\n\nclass Baseline(object):\n     Calculate baseline predictions. \n\n    def __init__(self, train_data):\n         Simple heuristic-based transductive learning to fill in missing\n        values in data matrix. \n        self.predict(train_data.copy())\n\n    def predict(self, train_data):\n        raise NotImplementedError(\n            'baseline prediction not implemented for base class')\n\n    def rmse(self, test_data):\n         Calculate root mean squared error for predictions on test data. \n        return rmse(test_data, self.predicted)\n\n    def __str__(self):\n        return split_title(self.__class__.__name__)\n\n\n\n# Implement the 3 baselines.\n\nclass UniformRandomBaseline(Baseline):\n     Fill missing values with uniform random values. \n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        pmin, pmax = masked_train.min(), masked_train.max()\n        N = nan_mask.sum()\n        train_data[nan_mask] = np.random.uniform(pmin, pmax, N)\n        self.predicted = train_data\n\n\nclass GlobalMeanBaseline(Baseline):\n     Fill in missing values using the global mean. \n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        train_data[nan_mask] = train_data[~nan_mask].mean()\n        self.predicted = train_data\n\n\nclass MeanOfMeansBaseline(Baseline):\n     Fill in missing values using mean of user/item/global means. \n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        global_mean = masked_train.mean()\n        user_means = masked_train.mean(axis=1)\n        item_means = masked_train.mean(axis=0)\n        self.predicted = train_data.copy()\n        n, m = train_data.shape\n        for i in xrange(n):\n            for j in xrange(m):\n                if np.ma.isMA(item_means[j]):\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i]))\n                else:\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i], item_means[j]))\n\n\nbaseline_methods = OrderedDict()\nbaseline_methods['ur'] = UniformRandomBaseline\nbaseline_methods['gm'] = GlobalMeanBaseline\nbaseline_methods['mom'] = MeanOfMeansBaseline", 
            "title": "Baselines"
        }, 
        {
            "location": "/pmf-pymc/#probabilistic-matrix-factorization", 
            "text": "Probabilistic Matrix Factorization (PMF)  [3] is a probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective. The ratings  $R$  are modeled as draws from a Gaussian distribution.  The mean for  $R_{ij}$  is  $U_i V_j^T$ . The precision  $\\alpha$  is a fixed parameter that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance. Complexity is controlled by placing zero-mean spherical Gaussian priors on  $U$  and  $V$ . In other words, each row of  $U$  is drawn from a multivariate Gaussian with mean  $\\mu = 0$  and precision which is some multiple of the identity matrix  $I$ . Those multiples are  $\\alpha_U$  for  $U$  and  $\\alpha_V$  for  $V$ . So our model is defined by:  $\\newcommand\\given[1][]{\\:#1\\vert\\:}$  \\begin{equation}\nP(R \\given U, V, \\alpha^2) = \n    \\prod_{i=1}^N \\prod_{j=1}^M\n        \\left[ \\mathcal{N}(R_{ij} \\given U_i V_j^T, \\alpha^{-1}) \\right]^{I_{ij}}\n\\end{equation}  \\begin{equation}\nP(U \\given \\alpha_U^2) =\n    \\prod_{i=1}^N \\mathcal{N}(U_i \\given 0, \\alpha_U^{-1} \\boldsymbol{I})\n\\end{equation}  \\begin{equation}\nP(V \\given \\alpha_U^2) =\n    \\prod_{j=1}^M \\mathcal{N}(V_j \\given 0, \\alpha_V^{-1} \\boldsymbol{I})\n\\end{equation}  Given small precision parameters, the priors on  $U$  and  $V$  ensure our latent variables do not grow too far from 0. This prevents overly strong user preferences and item factor compositions from being learned. This is commonly known as complexity control, where the complexity of the model here is measured by the magnitude of the latent variables. Controlling complexity like this helps prevent overfitting, which allows the model to generalize better for unseen data. We must also choose an appropriate  $\\alpha$  value for the normal distribution for  $R$ . So the challenge becomes choosing appropriate values for  $\\alpha_U$ ,  $\\alpha_V$ , and  $\\alpha$ . This challenge can be tackled with the soft weight-sharing methods discussed by  Nowland and Hinton, 1992  [4]. However, for the purposes of this analysis, we will stick to using point estimates obtained from our data.  import time\nimport logging\nimport pymc3 as pm\nimport theano\nimport scipy as sp\n\n\n# Enable on-the-fly graph computations, but ignore \n# absence of intermediate test values.\ntheano.config.compute_test_value = 'ignore'\n\n# Set up logging.\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n\nclass PMF(object):\n     Probabilistic Matrix Factorization model using pymc3. \n\n    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(-10, 10)):\n         Build the Probabilistic Matrix Factorization model using pymc3.\n\n        :param np.ndarray train: The training data to use for learning the model.\n        :param int dim: Dimensionality of the model; number of latent factors.\n        :param int alpha: Fixed precision for the likelihood function.\n        :param float std: Amount of noise to use for model initialization.\n        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n            These bounds will simply be used to cap the estimates produced for R.\n\n         \n        self.dim = dim\n        self.alpha = alpha\n        self.std = np.sqrt(1.0 / alpha)\n        self.bounds = bounds\n        self.data = train.copy()\n        n, m = self.data.shape\n\n        # Perform mean value imputation\n        nan_mask = np.isnan(self.data)\n        self.data[nan_mask] = self.data[~nan_mask].mean()\n\n        # Low precision reflects uncertainty; prevents overfitting.\n        # Set to the mean variance across users and items.\n        self.alpha_u = 1 / self.data.var(axis=1).mean()\n        self.alpha_v = 1 / self.data.var(axis=0).mean()\n\n        # Specify the model.\n        logging.info('building the PMF model')\n        with pm.Model() as pmf:\n            U = pm.MvNormal(\n                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n            V = pm.MvNormal(\n                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n            R = pm.Normal(\n                'R', mu=theano.tensor.dot(U, V.T), tau=self.alpha * np.ones((n, m)),\n                observed=self.data)\n\n        logging.info('done building the PMF model') \n        self.model = pmf\n\n    def __str__(self):\n        return self.name  We'll also need functions for calculating the MAP and performing sampling on our PMF model. When the observation noise variance  $\\alpha$  and the prior variances  $\\alpha_U$  and  $\\alpha_V$  are all kept fixed, maximizing the log posterior is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms.  $$\nE = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - U_i V_j^T)^2 +\n    \\frac{\\lambda_U}{2} \\sum_{i=1}^N \\|U\\|_{Fro}^2 +\n    \\frac{\\lambda_V}{2} \\sum_{j=1}^M \\|V\\|_{Fro}^2,\n$$  where  $\\lambda_U = \\alpha_U / \\alpha$ ,  $\\lambda_V = \\alpha_V / \\alpha$ , and  $\\|\\cdot\\|_{Fro}^2$  denotes the Frobenius norm [3]. Minimizing this objective function gives a local minimum, which is essentially a maximum a posteriori (MAP) estimate. While it is possible to use a fast Stochastic Gradient Descent procedure to find this MAP, we'll be finding it using the utilities built into  pymc3 . In particular, we'll use  find_MAP  with Powell optimization ( scipy.optimize.fmin_powell ). Having found this MAP estimate, we can use it as our starting point for MCMC sampling.  Since it is a reasonably complex model, we expect the MAP estimation to take some time. So let's save it after we've found it. Note that we define a function for finding the MAP below, assuming it will receive a namespace with some variables in it. Then we attach that function to the PMF class, where it will have such a namespace after initialization. The PMF class is defined in pieces this way so I can say a few things between each piece to make it clearer.  try:\n    import ujson as json\nexcept ImportError:\n    import json\n\n\n# First define functions to save our MAP estimate after it is found.\n# We adapt these from `pymc3`'s `backends` module, where the original\n# code is used to save the traces from MCMC samples.\ndef save_np_vars(vars, savedir):\n     Save a dictionary of numpy variables to `savedir`. We assume\n    the directory does not exist; an OSError will be raised if it does.\n     \n    logging.info('writing numpy vars to directory: %s' % savedir)\n    os.mkdir(savedir)\n    shapes = {}\n    for varname in vars:\n        data = vars[varname]\n        var_file = os.path.join(savedir, varname + '.txt')\n        np.savetxt(var_file, data.reshape(-1, data.size))\n        shapes[varname] = data.shape\n\n        ## Store shape information for reloading.\n        shape_file = os.path.join(savedir, 'shapes.json')\n        with open(shape_file, 'w') as sfh:\n            json.dump(shapes, sfh)\n\n\ndef load_np_vars(savedir):\n     Load numpy variables saved with `save_np_vars`. \n    shape_file = os.path.join(savedir, 'shapes.json')\n    with open(shape_file, 'r') as sfh:\n        shapes = json.load(sfh)\n\n    vars = {}\n    for varname, shape in shapes.items():\n        var_file = os.path.join(savedir, varname + '.txt')\n        vars[varname] = np.loadtxt(var_file).reshape(shape)\n\n    return vars\n\n\n# Now define the MAP estimation infrastructure.\ndef _map_dir(self):\n    basename = 'pmf-map-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _find_map(self):\n     Find mode of posterior using Powell optimization. \n    tstart = time.time()\n    with self.model:\n        logging.info('finding PMF MAP using Powell optimization...')\n        self._map = pm.find_MAP(fmin=sp.optimize.fmin_powell, disp=True)\n\n    elapsed = int(time.time() - tstart)\n    logging.info('found PMF MAP in %d seconds' % elapsed)\n\n    # This is going to take a good deal of time to find, so let's save it.\n    save_np_vars(self._map, self.map_dir)\n\ndef _load_map(self):\n    self._map = load_np_vars(self.map_dir)\n\ndef _map(self):\n    try:\n        return self._map\n    except:\n        if os.path.isdir(self.map_dir):\n            self.load_map()\n        else:\n            self.find_map()\n        return self._map\n\n\n# Update our class with the new MAP infrastructure.\nPMF.find_map = _find_map\nPMF.load_map = _load_map\nPMF.map_dir = property(_map_dir)\nPMF.map = property(_map)  So now our PMF class has a  map   property  which will either be found using Powell optimization or loaded from a previous optimization. Once we have the MAP, we can use it as a starting point for our MCMC sampler. We'll need a sampling function in order to draw MCMC samples to approximate the posterior distribution of the PMF model.  # Draw MCMC samples.\ndef _trace_dir(self):\n    basename = 'pmf-mcmc-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _draw_samples(self, nsamples=1000, njobs=2):\n    # First make sure the trace_dir does not already exist.\n    if os.path.isdir(self.trace_dir):\n        raise OSError(\n            'trace directory %s already exists. Please move or delete.' % self.trace_dir)\n    start = self.map  # use our MAP as the starting point\n    with self.model:\n        logging.info('drawing %d samples using %d jobs' % (nsamples, njobs))\n        step = pm.NUTS(scaling=start)\n        backend = pm.backends.Text(self.trace_dir)\n        logging.info('backing up trace to directory: %s' % self.trace_dir)\n        self.trace = pm.sample(nsamples, step, start=start, njobs=njobs, trace=backend)\n\ndef _load_trace(self):\n    with self.model:\n        self.trace = pm.backends.text.load(self.trace_dir)\n\n\n# Update our class with the sampling infrastructure.\nPMF.trace_dir = property(_trace_dir)\nPMF.draw_samples = _draw_samples\nPMF.load_trace = _load_trace  We could define some kind of default trace property like we did for the MAP, but that would mean using possibly nonsensical values for  nsamples  and  njobs . Better to leave it as a non-optional call to  draw_samples . Finally, we'll need a function to make predictions using our inferred values for  $U$  and  $V$ . For user  $i$  and joke  $j$ , a prediction is generated by drawing from  $\\mathcal{N}(U_i V_j^T, \\alpha)$ . To generate predictions from the sampler, we generate an  $R$  matrix for each  $U$  and  $V$  sampled, then we combine these by averaging over the  $K$  samples.  \\begin{equation}\nP(R_{ij}^* \\given R, \\alpha, \\alpha_U, \\alpha_V) \\approx\n    \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(U_i V_j^T, \\alpha)\n\\end{equation}  We'll want to inspect the individual  $R$  matrices before averaging them for diagnostic purposes. So we'll write code for the averaging piece during evaluation. The function below simply draws an  $R$  matrix given a  $U$  and  $V$  and the fixed  $\\alpha$  stored in the PMF object.  def _predict(self, U, V):\n     Estimate R from the given values of U and V. \n    R = np.dot(U, V.T)\n    n, m = R.shape\n    sample_R = np.array([\n        [np.random.normal(R[i,j], self.std) for j in xrange(m)]\n        for i in xrange(n)\n    ])\n\n    # bound ratings\n    low, high = self.bounds\n    sample_R[sample_R   low] = low\n    sample_R[sample_R   high] = high\n    return sample_R\n\n\nPMF.predict = _predict  One final thing to note: the dot products in this model are often constrained using a logistic function  $g(x) = 1/(1 + exp(-x))$ , that bounds the predictions to the range [0, 1]. To facilitate this bounding, the ratings are also mapped to the range [0, 1] using  $t(x) = (x + min) / range$ . The authors of PMF also introduced a constrained version which performs better on users with less ratings [3]. Both models are generally improvements upon the basic model presented here. However, in the interest of time and space, these will not be implemented here.", 
            "title": "Probabilistic Matrix Factorization"
        }, 
        {
            "location": "/pmf-pymc/#evaluation", 
            "text": "", 
            "title": "Evaluation"
        }, 
        {
            "location": "/pmf-pymc/#metrics", 
            "text": "In order to understand how effective our models are, we'll need to be able to evaluate them. We'll be evaluating in terms of root mean squared error (RMSE), which looks like this:  \\begin{equation}\nRMSE = \\sqrt{ \\frac{ \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - R_{ij}^*)^2 }\n                   { \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} } }\n\\end{equation}  In this case, the RMSE can be thought of as the standard deviation of our predictions from the actual user preferences.  # Define our evaluation function.\ndef rmse(test_data, predicted):\n     Calculate root mean squared error.\n    Ignoring missing values in the test data.\n     \n    I = ~np.isnan(test_data)   # indicator for missing values\n    N = I.sum()                # number of non-missing values\n    sqerror = abs(test_data - predicted) ** 2  # squared error array\n    mse = sqerror[I].sum() / N                 # mean squared error\n    return np.sqrt(mse)                        # RMSE", 
            "title": "Metrics"
        }, 
        {
            "location": "/pmf-pymc/#training-data-vs-test-data", 
            "text": "The next thing we need to do is split our data into a training set and a test set. Matrix factorization techniques use  transductive learning  rather than inductive learning. So we produce a test set by taking a random sample of the cells in the full  $N \\times M$  data matrix. The values selected as test samples are replaced with  nan  values in a copy of the original data matrix to produce the training set. Since we'll be producing random splits, let's also write out the train/test sets generated. This will allow us to replicate our results. We'd like to be able to idenfity which split is which, so we'll take a hash of the indices selected for testing and use that to save the data.  import hashlib\n\n\n# Define a function for splitting train/test data.\ndef split_train_test(data, percent_test=10):\n     Split the data into train/test sets.\n    :param int percent_test: Percentage of data to use for testing. Default 10.\n     \n    n, m = data.shape             # # users, # jokes\n    N = n * m                     # # cells in matrix\n    test_size = N / percent_test  # use 10% of data as test set\n    train_size = N - test_size    # and remainder for training\n\n    # Prepare train/test ndarrays.\n    train = data.copy().values\n    test = np.ones(data.shape) * np.nan\n\n    # Draw random sample of training data to use for testing.\n    tosample = np.where(~np.isnan(train))       # ignore nan values in data\n    idx_pairs = zip(tosample[0], tosample[1])   # tuples of row/col index pairs\n    indices = np.arange(len(idx_pairs))         # indices of index pairs\n    sample = np.random.choice(indices, replace=False, size=test_size)\n\n    # Transfer random sample from train set to test set.\n    for idx in sample:\n        idx_pair = idx_pairs[idx]\n        test[idx_pair] = train[idx_pair]  # transfer to test set\n        train[idx_pair] = np.nan          # remove from train set\n\n    # Verify everything worked properly\n    assert(np.isnan(train).sum() == test_size)\n    assert(np.isnan(test).sum() == train_size)\n\n    # Finally, hash the indices and save the train/test sets.\n    index_string = ''.join(map(str, np.sort(sample)))\n    name = hashlib.sha1(index_string).hexdigest()\n    savedir = os.path.join('data', name)\n    save_np_vars({'train': train, 'test': test}, savedir)\n\n    # Return train set, test set, and unique hash of indices.\n    return train, test, name\n\n\ndef load_train_test(name):\n     Load the train/test sets. \n    savedir = os.path.join('data', name)\n    vars = load_np_vars(savedir)\n    return vars['train'], vars['test']\n\n# train, test, name = split_train_test(data)  In order to facilitate reproducibility, I've produced a train/test split using the code above which we'll now use for all the evaluations below.  train, test = load_train_test('6bb8d06c69c0666e6da14c094d4320d115f1ffc8')", 
            "title": "Training Data vs. Test Data"
        }, 
        {
            "location": "/pmf-pymc/#results", 
            "text": "# Let's see the results:\nbaselines = {}\nfor name in baseline_methods:\n    Method = baseline_methods[name]\n    method = Method(train)\n    baselines[name] = method.rmse(test)\n    print '%s RMSE:\\t%.5f' % (method, baselines[name])  Uniform Random Baseline RMSE:   7.77062\nGlobal Mean Baseline RMSE:  5.25004\nMean Of Means Baseline RMSE:    4.79832  As expected: the uniform random baseline is the worst by far, the global mean baseline is next best, and the mean of means method is our best baseline. Now let's see how PMF stacks up.  # We use a fixed precision for the likelihood.\n# This reflects uncertainty in the dot product.\n# We choose 2 in the footsteps Salakhutdinov\n# Mnihof.\nALPHA = 2\n\n# The dimensionality D; the number of latent factors.\n# We can adjust this higher to try to capture more subtle\n# characteristics of each joke. However, the higher it is,\n# the more expensive our inference procedures will be.\n# Specifically, we have D(N + M) latent variables. For our\n# Jester dataset, this means we have D(1100), so for 5\n# dimensions, we are sampling 5500 latent variables.\nDIM = 5\n\n\npmf = PMF(train, DIM, ALPHA, std=0.05)  INFO:root:building the PMF model\nINFO:root:done building the PMF model", 
            "title": "Results"
        }, 
        {
            "location": "/pmf-pymc/#predictions-using-map", 
            "text": "# Find MAP for PMF.\npmf.find_map()\n# pmf.load_map()  INFO:root:finding MAP using Powell optimization...\nINFO:root:found MAP in 2575 seconds\n\n\nOptimization terminated successfully.\n         Current function value: 1553644.881552\n         Iterations: 33\n         Function evaluations: 1644948  Excellent. The first thing we want to do is make sure the MAP estimate we obtained is reasonable. We can do this by computing RMSE on the predicted ratings obtained from the MAP values of  $U$  and  $V$ . First we define a function for generating the predicted ratings  $R$  from  $U$  and  $V$ . We ensure the actual rating bounds are enforced by setting all values below -10 to -10 and all values above 10 to 10. Finally, we compute RMSE for both the training set and the test set. We expect the test RMSE to be higher. The difference between the two gives some idea of how much we have overfit. Some difference is always expected, but a very low RMSE on the training set with a high RMSE on the test set is a definite sign of overfitting.  def eval_map(pmf_model, train, test):\n    U = pmf_model.map['U']\n    V = pmf_model.map['V']\n\n    # Make predictions and calculate RMSE on train   test sets.\n    predictions = pmf_model.predict(U, V)\n    train_rmse = rmse(train, predictions)\n    test_rmse = rmse(test, predictions)\n    overfit = test_rmse - train_rmse\n\n    # Print report.\n    print 'PMF MAP training RMSE: %.5f' % train_rmse\n    print 'PMF MAP testing RMSE:  %.5f' % test_rmse\n    print 'Train/test difference: %.5f' % overfit\n\n    return test_rmse\n\n\n# Add eval function to PMF class.\nPMF.eval_map = eval_map  # Evaluate PMF MAP estimates.\npmf_map_rmse = pmf.eval_map(train, test)\npmf_improvement = baselines['mom'] - pmf_map_rmse\nprint 'PMF MAP Improvement:   %.5f' % pmf_improvement  PMF MAP training RMSE: 4.00824\nPMF MAP testing RMSE:  4.02974\nTrain/test difference: 0.02150\nPMF MAP Improvement:   0.76858  So we see a pretty nice improvement here when compared to our best baseline, which was the mean of means method. We also have a fairly small difference in the RMSE values between the train and the test sets. This indicates that the point estimates for  $\\alpha_U$  and  $\\alpha_V$  that we calculated from our data are doing a good job of controlling model complexity. Now let's see if we can improve our estimates by approximating our posterior distribution with MCMC sampling. We'll draw 1000 samples and back them up using the  pymc3.backend.Text  backend.", 
            "title": "Predictions Using MAP"
        }, 
        {
            "location": "/pmf-pymc/#predictions-using-mcmc", 
            "text": "# Draw MCMC samples.\npmf.draw_samples(5000, njobs=3)\n\n# uncomment to load previous trace rather than drawing new samples.\n# pmf.load_trace()  INFO:root:drawing 5000 samples using 3 jobs\n/home/mack/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *\nINFO:root:backing up trace to directory: data/pmf-mcmc-d5\n\n\n [-----------------100%-----------------] 5001 of 5000 complete in 7506.2 sec  Diagnostics and Posterior Predictive Check  The next step is to check how many samples we should discard as burn-in. Normally, we'd do this using a traceplot to get some idea of where the sampled variables start to converge. In this case, we have high-dimensional samples, so we need to find a way to approximate them. One way was proposed by  Salakhutdinov and Mnih, p.886 . We can calculate the Frobenius norms of  $U$  and  $V$  at each step and monitor those for convergence. This essentially gives us some idea when the average magnitude of the latent variables is stabilizing. The equations for the Frobenius norms of  $U$  and  $V$  are shown below. We will use  numpy 's  linalg  package to calculate these.  $$\n\\|U\\|_{Fro}^2 = \\sqrt{\\sum_{i=1}^N \\sum_{d=1}^D |U_{id}|^2}, \\hspace{40pt}\n\\|V\\|_{Fro}^2 = \\sqrt{\\sum_{j=1}^M \\sum_{d=1}^D |V_{jd}|^2}\n$$  def _norms(pmf_model, monitor=('U', 'V'), ord='fro'):\n     Return norms of latent variables at each step in the\n    sample trace. These can be used to monitor convergence\n    of the sampler.\n     \n    monitor = ('U', 'V')\n    norms = {var: [] for var in monitor}\n    for sample in pmf_model.trace:\n        for var in monitor:\n            norms[var].append(np.linalg.norm(sample[var], ord))\n    return norms\n\n\ndef _traceplot(pmf_model):\n     Plot Frobenius norms of U and V as a function of sample #. \n    trace_norms = pmf_model.norms()\n    u_series = pd.Series(trace_norms['U'])\n    v_series = pd.Series(trace_norms['V'])\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    u_series.plot(kind='line', ax=ax1, grid=False,\n                  title= $\\|U\\|_{Fro}^2$ at Each Sample )\n    v_series.plot(kind='line', ax=ax2, grid=False,\n                  title= $\\|V\\|_{Fro}^2$ at Each Sample )\n    ax1.set_xlabel( Sample Number )\n    ax2.set_xlabel( Sample Number )\n\n\nPMF.norms = _norms\nPMF.traceplot = _traceplot  pmf.traceplot()   It appears we get convergence of  $U$  and  $V$  after about 200 samples. When testing for convergence, we also want to see convergence of the particular statistics we are looking for, since different characteristics of the posterior may converge at different rates. Let's also do a traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample.  def _running_rmse(pmf_model, test_data, train_data, burn_in=0, plot=True):\n     Calculate RMSE for each step of the trace to monitor convergence.\n     \n    burn_in = burn_in if len(pmf_model.trace)  = burn_in else 0\n    results = {'per-step-train': [], 'running-train': [],\n               'per-step-test': [], 'running-test': []}\n    R = np.zeros(test_data.shape)\n    for cnt, sample in enumerate(pmf_model.trace[burn_in:]):\n        sample_R = pmf_model.predict(sample['U'], sample['V'])\n        R += sample_R\n        running_R = R / (cnt + 1)\n        results['per-step-train'].append(rmse(train_data, sample_R))\n        results['running-train'].append(rmse(train_data, running_R))\n        results['per-step-test'].append(rmse(test_data, sample_R))\n        results['running-test'].append(rmse(test_data, running_R))\n\n    results = pd.DataFrame(results)\n\n    if plot:\n        results.plot(\n            kind='line', grid=False, figsize=(15, 7),\n            title='Per-step and Running RMSE From Posterior Predictive')\n\n    # Return the final predictions, and the RMSE calculations\n    return running_R, results\n\n\nPMF.running_rmse = _running_rmse  predicted, results = pmf.running_rmse(test, train, burn_in=200)   # And our final RMSE?\nfinal_test_rmse = results['running-test'].values[-1]\nfinal_train_rmse = results['running-train'].values[-1]\nprint 'Posterior predictive train RMSE: %.5f' % final_train_rmse\nprint 'Posterior predictive test RMSE:  %.5f' % final_test_rmse\nprint 'Train/test difference:           %.5f' % (final_test_rmse - final_train_rmse)\nprint 'Improvement from MAP:            %.5f' % (pmf_map_rmse - final_test_rmse)\nprint 'Improvement from Mean of Means:  %.5f' % (baselines['mom'] - final_test_rmse)  Posterior predictive train RMSE: 3.92230\nPosterior predictive test RMSE:  4.18027\nTrain/test difference:           0.25797\nImprovement from MAP:            -0.15052\nImprovement from Mean of Means:  0.61806  We have some interesting results here. As expected, our MCMC sampler provides lower error on the training set. However, it seems it does so at the cost of overfitting the data. This results in a decrease in test RMSE as compared to the MAP, even though it is still much better than our best baseline. So why might this be the case? Recall that we used point estimates for our precision paremeters  $\\alpha_U$  and  $\\alpha_V$  and we chose a fixed precision  $\\alpha$ . It is quite likely that by doing this, we constrained our posterior in a way that biased it towards the training data. In reality, the variance in the user ratings and the joke ratings is unlikely to be equal to the means of sample variances we used. Also, the most reasonable observation precision  $\\alpha$  is likely different as well.", 
            "title": "Predictions using MCMC"
        }, 
        {
            "location": "/pmf-pymc/#summary-of-results", 
            "text": "Let's summarize our results.  size = 100  # RMSE doesn't really change after 100th sample anyway.\nall_results = pd.DataFrame({\n    'uniform random': np.repeat(baselines['ur'], size),\n    'global means': np.repeat(baselines['gm'], size),\n    'mean of means': np.repeat(baselines['mom'], size),\n    'PMF MAP': np.repeat(pmf_map_rmse, size),\n    'PMF MCMC': results['running-test'][:size],\n})\nfig, ax = plt.subplots(figsize=(10, 5))\nall_results.plot(kind='line', grid=False, ax=ax,\n                 title='RMSE for all methods')\nax.set_xlabel( Number of Samples )\nax.set_ylabel( RMSE )  matplotlib.text.Text at 0x7f5327451a10", 
            "title": "Summary of Results"
        }, 
        {
            "location": "/pmf-pymc/#summary", 
            "text": "We set out to predict user preferences for unseen jokes. First we discussed the intuitive notion behind the user-user and item-item neighborhood approaches to collaborative filtering. Then we formalized our intuitions. With a firm understanding of our problem context, we moved on to exploring our subset of the Jester data. After discovering some general patterns, we defined three baseline methods: uniform random, global mean, and mean of means. With the goal of besting our baseline methods, we implemented the basic version of Probabilistic Matrix Factorization (PMF) using  pymc3 .  Our results demonstrate that the mean of means method is our best baseline on our prediction task. As expected, we are able to obtain a significant decrease in RMSE using the PMF MAP estimate obtained via Powell optimization. We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space using the Frobenius norms of the sampled variables. The traceplots using this method seem to indicate that our sampler converged to the posterior. Results using this posterior showed that attempting to improve the MAP estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters  $\\alpha$ ,  $\\alpha_U$ , and  $\\alpha_V$ .  As a followup to this analysis, it would be interesting to also implement the logistic and constrained versions of PMF. We expect both models to outperform the basic PMF model. We could also implement the  fully Bayesian version of PMF  (BPMF), which places hyperpriors on the model parameters to automatically learn ideal mean and precision parameters for  $U$  and  $V$ . This would likely resolve the issue we faced in this analysis. We would expect BPMF to improve upon the MAP estimation produced here by learning more suitable hyperparameters and parameters. For a basic (but working!) implementation of BPMF in  pymc3 , see  this gist .  If you made it this far, then congratulations! You now have some idea of how to build a basic recommender system. These same ideas and methods can be used on many different recommendation tasks. Items can be movies, products, advertisements, courses, or even other people. Any time you can build yourself a user-item matrix with user preferences in the cells, you can use these types of collaborative filtering algorithms to predict the missing values. If you want to learn more about recommender systems, the first reference is a good place to start.", 
            "title": "Summary"
        }, 
        {
            "location": "/pmf-pymc/#references", 
            "text": "Y. Koren, R. Bell, and C. Volinsky, \u201cMatrix Factorization Techniques for Recommender Systems,\u201d Computer, vol. 42, no. 8, pp. 30\u201337, Aug. 2009.  K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, \u201cEigentaste: A constant time collaborative filtering algorithm,\u201d Information Retrieval, vol. 4, no. 2, pp. 133\u2013151, 2001.  A. Mnih and R. Salakhutdinov, \u201cProbabilistic matrix factorization,\u201d in Advances in neural information processing systems, 2007, pp. 1257\u20131264.  S. J. Nowlan and G. E. Hinton, \u201cSimplifying Neural Networks by Soft Weight-sharing,\u201d Neural Comput., vol. 4, no. 4, pp. 473\u2013493, Jul. 1992.  R. Salakhutdinov and A. Mnih, \u201cBayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo,\u201d in Proceedings of the 25th International Conference on Machine Learning, New York, NY, USA, 2008, pp. 880\u2013887.", 
            "title": "References"
        }, 
        {
            "location": "/rugby_analytics/", 
            "text": "A Hierarchical model for Rugby prediction\n\n\nBayesian Rugby\nI came across the following blog post on http://danielweitzenfeld.github.io/passtheroc/blog/2014/10/28/bayes-premier-league/\n* Based on the work of \nBaio and Blangiardo\n\nIn this example, we're going to reproduce the first model described in the paper using PyMC3.\n\n\nSince I am a rugby fan I decide to apply the results of the paper Bayesian Football to the Six Nations.\nRugby is a physical sport popular worldwide.\n\n Six Nations consists of Italy, Ireland, Scotland, England, France and Wales\n\n Game consists of scoring tries (similar to touch downs) or kicking the goal.\n\n Average player is something like 100kg and 1.82m tall.\n\n Paul O'Connell the Irish captain is Height: 6' 6\" (1.98 m) Weight: 243 lbs (110 kg)\n\n\nWe will use a data set only consisting of the Six Nations 2014 data, and use this to build a generative and explainable model about the Six Nations 2015.\n\n\nMotivation\n\n\nYour estimate of the strength of a team depends on your estimates of the other strengths\n\n\nIreland are a stronger team than Italy for example - but by how much?\n\n\nSource for Results 2014 are Wikipedia.\nI handcrafted these results\nSmall data\n\n We want to infer a latent parameter - that is the 'strength' of a team based only on their \nscoring intensity\n, and all we have are their scores and results, we can't accurately measure the 'strength' of a team. \n\n Probabilistic Programming is a brilliant paradigm for modeling these \nlatent\n parameters\n\n\n!date\n\nimport numpy as np\nimport pandas as pd\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n%matplotlib inline\nimport pymc3 as pm, theano.tensor as tt\n\n\n\n\nERROR! Session/line number was not unique in database. History logging moved to new session 99\nSun 23 Aug 2015 18:36:33 CEST\n\n\n\nThis is a Rugby prediction exercise. So we'll input some data\n\n\ndata_csv = StringIO(\nhome_team,away_team,home_score,away_score\nWales,Italy,23,15\nFrance,England,26,24\nIreland,Scotland,28,6\nIreland,Wales,26,3\nScotland,England,0,20\nFrance,Italy,30,10\nWales,France,27,6\nItaly,Scotland,20,21\nEngland,Ireland,13,10\nIreland,Italy,46,7\nScotland,France,17,19\nEngland,Wales,29,18\nItaly,England,11,52\nWales,Scotland,51,3\nFrance,Ireland,20,22\n)\n\n\n\n\n\n\n\n\n\nWhat do we want to infer?\n\n\n\n\nWe want to infer the latent paremeters (every team's strength) that are generating the data we observe (the scorelines).\n\n\n\n\nMoreover, we know that the scorelines are a noisy measurement of team strength, so ideally, we want a model that makes it easy to quantify our uncertainty about the underlying strengths.\n\n\n\n\n\n\nOften we don't know what the Bayesian Model is explicitly, so we have to 'estimate' the Bayesian Model'\n\n\n\n\n\n\nIf we can't solve something, approximate it.\n\n\n\n\n\n\nMarkov-Chain Monte Carlo (MCMC) instead draws samples from the posterior.\n\n\n\n\nFortunately, this algorithm can be applied to almost any model.\n\n\n\n\nWhat do we want?\n\n\nWe want to quantify our uncertainty\nWe want to also use this to generate a model\nWe want the answers as distributions not point estimates\n\n\nWhat assumptions do we know for our 'generative story'?\n\n\n\n\nWe know that the Six Nations in Rugby only has 6 teams - they each play each other once\n\n\nWe have data from last year!\n\n\nWe also know that in sports scoring is modelled as a Poisson distribution\n\n\nWe consider home advantage to be a strong effect in sports\n\n\n\n\nThe model.\n\n\nThe league is made up by a total of T= 6 teams, playing each other once \nin a season. We indicate the number of points scored by the home and the away team in the g-th game of the season (15 games) as $y_{g1}$ and $y_{g2}$ respectively. \n\n\n\nThe vector of observed counts $\\mathbb{y} = (y_{g1}, y_{g2})$ is modelled as independent Poisson:\n$y_{gi}| \\theta_{gj} \\tilde\\;\\;  Poisson(\\theta_{gj})$\nwhere the theta parameters represent the scoring intensity in the g-th game for the team playing at home (j=1) and away (j=2), respectively.\n\n\n\nWe model these parameters according to a formulation that has been used widely in the statistical literature, assuming a log-linear random effect model:\n$$log \\theta_{g1} = home + att_{h(g)} + def_{a(g)} $$\n$$log \\theta_{g2} = att_{a(g)} + def_{h(g)}$$\n\n* The parameter home represents the advantage for the team hosting the game and we assume that this effect is constant for all the teams and throughout the season\n* The scoring intensity is determined jointly by the attack and defense ability of the two teams involved, represented by the parameters att and def, respectively\n\n* Conversely, for each t = 1, ..., T, the team-specific effects are modelled as exchangeable from a common distribution:\n\n* $att_{t}$ ~ $Normal(\u03bc_att,\u03c4_att)$ and $def_{t}$ ~Normal$(\u03bc_{def},\u03c4_{def})$\n\nNote that they're breaking out team strength into attacking and defending strength. A negative defense parameter will sap the mojo from the opposing team's attacking parameter.\nI made two tweaks to the model. It didn't make sense to me to have a \u03bcatt when we're enforcing the sum-to-zero constraint by subtracting the mean anyway. So I eliminated \u03bcatt and \u03bcdef\nAlso because of the sum-to-zero constraint, it seemed to me that we needed an intercept term in the log-linear model, capturing the average goals scored per game by the away team. This we model with the following hyperprior.\nintercept~Normal(0,0.001)\n\nThe hyper-priors on the attack and defense parameters are also flat\n* \u03bcatt~Normal(0,0.001)\n* \u03bcdef~Normal(0,0.001)\n* \u03c4att~\u0393(0.1,0.1)\n* \u03c4def~\u0393(0.1,0.1)\n\n\n\n\n\n\u0002wzxhzdk:3\u0003\n\n\n* We did some munging above and adjustments of the data to make it **tidier** for our model. \n* The log function to away scores and home scores is a standard trick in the sports analytics literature\n# Building of the model \n* We now build the model in PyMC3, specifying the global parameters, and the team-specific parameters and the likelihood function \n\n\n\n\n\u0002wzxhzdk:4\u0003\n\n\n* We specified the model and the likelihood function\n\n* All this runs on a Theano graph under the hood\n* Now we need to fit our model using the Maximum A Posteriori algorithm to decide where to start out No U Turn Sampler\n\n\n\n\u0002wzxhzdk:5\u0003\n\n\n     [-----------------100%-----------------] 2000 of 2000 complete in 4.3 sec\n\n\n![png](rugby_analytics_files/rugby_analytics_15_1.png)\n\n\n# Results\nFrom the above we can start to understand the different distributions of attacking strength and defensive strength.\nThese are probabilistic estimates and help us better understand the uncertainty in sports analytics\n\n\n\n\u0002wzxhzdk:6\u0003", 
            "title": "Rugby Analytics example"
        }, 
        {
            "location": "/rugby_analytics/#a-hierarchical-model-for-rugby-prediction", 
            "text": "Bayesian Rugby\nI came across the following blog post on http://danielweitzenfeld.github.io/passtheroc/blog/2014/10/28/bayes-premier-league/\n* Based on the work of  Baio and Blangiardo \nIn this example, we're going to reproduce the first model described in the paper using PyMC3.  Since I am a rugby fan I decide to apply the results of the paper Bayesian Football to the Six Nations.\nRugby is a physical sport popular worldwide.  Six Nations consists of Italy, Ireland, Scotland, England, France and Wales  Game consists of scoring tries (similar to touch downs) or kicking the goal.  Average player is something like 100kg and 1.82m tall.  Paul O'Connell the Irish captain is Height: 6' 6\" (1.98 m) Weight: 243 lbs (110 kg)  We will use a data set only consisting of the Six Nations 2014 data, and use this to build a generative and explainable model about the Six Nations 2015.", 
            "title": "A Hierarchical model for Rugby prediction"
        }, 
        {
            "location": "/rugby_analytics/#motivation", 
            "text": "Your estimate of the strength of a team depends on your estimates of the other strengths  Ireland are a stronger team than Italy for example - but by how much?  Source for Results 2014 are Wikipedia.\nI handcrafted these results\nSmall data  We want to infer a latent parameter - that is the 'strength' of a team based only on their  scoring intensity , and all we have are their scores and results, we can't accurately measure the 'strength' of a team.   Probabilistic Programming is a brilliant paradigm for modeling these  latent  parameters  !date\n\nimport numpy as np\nimport pandas as pd\ntry:\n    from StringIO import StringIO\nexcept ImportError:\n    from io import StringIO\n%matplotlib inline\nimport pymc3 as pm, theano.tensor as tt  ERROR! Session/line number was not unique in database. History logging moved to new session 99\nSun 23 Aug 2015 18:36:33 CEST  This is a Rugby prediction exercise. So we'll input some data  data_csv = StringIO( home_team,away_team,home_score,away_score\nWales,Italy,23,15\nFrance,England,26,24\nIreland,Scotland,28,6\nIreland,Wales,26,3\nScotland,England,0,20\nFrance,Italy,30,10\nWales,France,27,6\nItaly,Scotland,20,21\nEngland,Ireland,13,10\nIreland,Italy,46,7\nScotland,France,17,19\nEngland,Wales,29,18\nItaly,England,11,52\nWales,Scotland,51,3\nFrance,Ireland,20,22 )", 
            "title": "Motivation"
        }, 
        {
            "location": "/rugby_analytics/#what-do-we-want-to-infer", 
            "text": "We want to infer the latent paremeters (every team's strength) that are generating the data we observe (the scorelines).   Moreover, we know that the scorelines are a noisy measurement of team strength, so ideally, we want a model that makes it easy to quantify our uncertainty about the underlying strengths.    Often we don't know what the Bayesian Model is explicitly, so we have to 'estimate' the Bayesian Model'    If we can't solve something, approximate it.    Markov-Chain Monte Carlo (MCMC) instead draws samples from the posterior.   Fortunately, this algorithm can be applied to almost any model.", 
            "title": "What do we want to infer?"
        }, 
        {
            "location": "/rugby_analytics/#what-do-we-want", 
            "text": "We want to quantify our uncertainty\nWe want to also use this to generate a model\nWe want the answers as distributions not point estimates", 
            "title": "What do we want?"
        }, 
        {
            "location": "/rugby_analytics/#what-assumptions-do-we-know-for-our-generative-story", 
            "text": "We know that the Six Nations in Rugby only has 6 teams - they each play each other once  We have data from last year!  We also know that in sports scoring is modelled as a Poisson distribution  We consider home advantage to be a strong effect in sports", 
            "title": "What assumptions do we know for our 'generative story'?"
        }, 
        {
            "location": "/rugby_analytics/#the-model", 
            "text": "The league is made up by a total of T= 6 teams, playing each other once \nin a season. We indicate the number of points scored by the home and the away team in the g-th game of the season (15 games) as $y_{g1}$ and $y_{g2}$ respectively.   The vector of observed counts $\\mathbb{y} = (y_{g1}, y_{g2})$ is modelled as independent Poisson:\n$y_{gi}| \\theta_{gj} \\tilde\\;\\;  Poisson(\\theta_{gj})$\nwhere the theta parameters represent the scoring intensity in the g-th game for the team playing at home (j=1) and away (j=2), respectively.  We model these parameters according to a formulation that has been used widely in the statistical literature, assuming a log-linear random effect model:\n$$log \\theta_{g1} = home + att_{h(g)} + def_{a(g)} $$\n$$log \\theta_{g2} = att_{a(g)} + def_{h(g)}$$\n\n* The parameter home represents the advantage for the team hosting the game and we assume that this effect is constant for all the teams and throughout the season\n* The scoring intensity is determined jointly by the attack and defense ability of the two teams involved, represented by the parameters att and def, respectively\n\n* Conversely, for each t = 1, ..., T, the team-specific effects are modelled as exchangeable from a common distribution:\n\n* $att_{t}$ ~ $Normal(\u03bc_att,\u03c4_att)$ and $def_{t}$ ~Normal$(\u03bc_{def},\u03c4_{def})$\n\nNote that they're breaking out team strength into attacking and defending strength. A negative defense parameter will sap the mojo from the opposing team's attacking parameter.\nI made two tweaks to the model. It didn't make sense to me to have a \u03bcatt when we're enforcing the sum-to-zero constraint by subtracting the mean anyway. So I eliminated \u03bcatt and \u03bcdef\nAlso because of the sum-to-zero constraint, it seemed to me that we needed an intercept term in the log-linear model, capturing the average goals scored per game by the away team. This we model with the following hyperprior.\nintercept~Normal(0,0.001)\n\nThe hyper-priors on the attack and defense parameters are also flat\n* \u03bcatt~Normal(0,0.001)\n* \u03bcdef~Normal(0,0.001)\n* \u03c4att~\u0393(0.1,0.1)\n* \u03c4def~\u0393(0.1,0.1)\n\n\n\n\n\n\u0002wzxhzdk:3\u0003\n\n\n* We did some munging above and adjustments of the data to make it **tidier** for our model. \n* The log function to away scores and home scores is a standard trick in the sports analytics literature\n# Building of the model \n* We now build the model in PyMC3, specifying the global parameters, and the team-specific parameters and the likelihood function \n\n\n\n\n\u0002wzxhzdk:4\u0003\n\n\n* We specified the model and the likelihood function\n\n* All this runs on a Theano graph under the hood\n* Now we need to fit our model using the Maximum A Posteriori algorithm to decide where to start out No U Turn Sampler\n\n\n\n\u0002wzxhzdk:5\u0003\n\n\n     [-----------------100%-----------------] 2000 of 2000 complete in 4.3 sec\n\n\n![png](rugby_analytics_files/rugby_analytics_15_1.png)\n\n\n# Results\nFrom the above we can start to understand the different distributions of attacking strength and defensive strength.\nThese are probabilistic estimates and help us better understand the uncertainty in sports analytics\n\n\n\n\u0002wzxhzdk:6\u0003", 
            "title": "The model."
        }, 
        {
            "location": "/posterior_predictive/", 
            "text": "Posterior Predictive Checks in PyMC3\n\n\nPPCs are a great way to validate a model. The idea is to generate data sets from the model using parameter settings from draws from the posterior.\n\n\nPyMC3\n has random number support thanks to \nMark Wibrow\n as implemented in \nPR784\n.\n\n\nHere we will implement a general routine to draw samples from the observed nodes of a model.\n\n\n%matplotlib inline\nimport numpy as np\nimport pymc3 as pm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\n\n\n\nERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\nERROR:theano.sandbox.cuda:nvcc compiler not found on $PATH. Check your nvcc installation and try again.\n\n\n\nLets generate a very simple model:\n\n\ndata = np.random.randn(100)\n\nwith pm.Model() as model: \n    mu = pm.Normal('mu', mu=0, sd=1, testval=0)\n    sd = pm.HalfNormal('sd', sd=1)\n    n = pm.Normal('n', mu=mu, sd=sd, observed=data)\n\n    step = pm.NUTS()\n    trace = pm.sample(5000, step)\n\n\n\n\n [-----------------100%-----------------] 5000 of 5000 complete in 2.4 sec\n\n/home/wiecki/miniconda3/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)\n\n\n\npm.traceplot(trace);\n\n\n\n\n\n\nThis function will make it into \nPyMC3\n eventually, for now this should work on all models:\n\n\ndef run_ppc(trace, samples=100, model=None):\n    \nGenerate Posterior Predictive samples from a model given a trace.\n    \n\n    if model is None:\n         model = pm.modelcontext(model)\n\n    ppc = defaultdict(list)\n    for idx in np.random.randint(0, len(trace), samples):\n        param = trace[idx]\n        for obs in model.observed_RVs:\n            ppc[obs.name].append(obs.distribution.random(point=param))\n\n    return ppc\n\n\n\n\nppc = run_ppc(trace, samples=500, model=model)\n\n\n\n\nppc\n contains 500 generated data sets (containing 50 samples each), each using a different parameter setting from the posterior:\n\n\nnp.asarray(ppc['o']).shape\n\n\n\n\n(500, 50)\n\n\n\nOne common way to visualize is to look if the model can reproduce the patterns observed in the real data. For example, how close are the inferred means to the actual sample mean:\n\n\nax = plt.subplot()\nsns.distplot([n.mean() for n in ppc['n']], kde=False, ax=ax)\nax.axvline(data.mean())\nax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');\n\n\n\n\n\n\nPrediction\n\n\nThe same pattern can be used for prediction. Here we're building a logistic regression model. Note that since we're dealing the full posterior, we're also getting uncertainty in our predictions for free.\n\n\n# Use a theano shared variable to be able to exchange the data the model runs on\nfrom theano import shared\n\n\n\n\ndef invlogit(x):\n    return np.exp(x) / (1 + np.exp(x))\n\nn = 4000\nn_oos = 50\ncoeff = 1.\n\npredictors = np.random.normal(size=n)\n# Turn predictor into a shared var so that we can change it later\npredictors_shared = shared(predictors)\n\noutcomes = np.random.binomial(1, invlogit(coeff * predictors))\n\n\n\n\noutcomes\n\n\n\n\narray([1, 1, 0, ..., 1, 0, 0])\n\n\n\npredictors_oos = np.random.normal(size=50)\noutcomes_oos = np.random.binomial(1, invlogit(coeff * predictors_oos))\n\n\n\n\ndef tinvlogit(x):\n    import theano.tensor as t\n    return t.exp(x) / (1 + t.exp(x))\n\nwith pm.Model() as model:\n    coeff = pm.Normal('coeff', mu=0, sd=1)\n    p = tinvlogit(coeff * predictors_shared)\n\n    o = pm.Bernoulli('o', p, observed=outcomes)\n\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(500, step)\n\n\n\n\n [-----------------100%-----------------] 500 of 500 complete in 2.6 sec\n\n\n\n# Changing values here will also change values in the model\npredictors_shared.set_value(predictors_oos)\n\n\n\n\n# Simply running PPC will use the updated values and do prediction\nppc = run_ppc(trace, model=model, samples=500)\n\n\n\n\nMean predicted values plus error bars to give sense of uncertainty in prediction\n\n\nplt.errorbar(x=predictors_oos, y=np.asarray(ppc['o']).mean(axis=0), yerr=np.asarray(ppc['o']).std(axis=0), linestyle='', marker='o')\nplt.plot(predictors_oos, outcomes_oos, 'o')\nplt.ylim(-.05, 1.05)\nplt.xlabel('predictor')\nplt.ylabel('outcome')\n\n\n\n\nmatplotlib.text.Text at 0x7f1e23214f98", 
            "title": "Posterior Predictive checks and prediction"
        }, 
        {
            "location": "/posterior_predictive/#posterior-predictive-checks-in-pymc3", 
            "text": "PPCs are a great way to validate a model. The idea is to generate data sets from the model using parameter settings from draws from the posterior.  PyMC3  has random number support thanks to  Mark Wibrow  as implemented in  PR784 .  Here we will implement a general routine to draw samples from the observed nodes of a model.  %matplotlib inline\nimport numpy as np\nimport pymc3 as pm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict  ERROR (theano.sandbox.cuda): nvcc compiler not found on $PATH. Check your nvcc installation and try again.\nERROR:theano.sandbox.cuda:nvcc compiler not found on $PATH. Check your nvcc installation and try again.  Lets generate a very simple model:  data = np.random.randn(100)\n\nwith pm.Model() as model: \n    mu = pm.Normal('mu', mu=0, sd=1, testval=0)\n    sd = pm.HalfNormal('sd', sd=1)\n    n = pm.Normal('n', mu=mu, sd=sd, observed=data)\n\n    step = pm.NUTS()\n    trace = pm.sample(5000, step)   [-----------------100%-----------------] 5000 of 5000 complete in 2.4 sec\n\n/home/wiecki/miniconda3/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)  pm.traceplot(trace);   This function will make it into  PyMC3  eventually, for now this should work on all models:  def run_ppc(trace, samples=100, model=None):\n     Generate Posterior Predictive samples from a model given a trace.\n     \n    if model is None:\n         model = pm.modelcontext(model)\n\n    ppc = defaultdict(list)\n    for idx in np.random.randint(0, len(trace), samples):\n        param = trace[idx]\n        for obs in model.observed_RVs:\n            ppc[obs.name].append(obs.distribution.random(point=param))\n\n    return ppc  ppc = run_ppc(trace, samples=500, model=model)  ppc  contains 500 generated data sets (containing 50 samples each), each using a different parameter setting from the posterior:  np.asarray(ppc['o']).shape  (500, 50)  One common way to visualize is to look if the model can reproduce the patterns observed in the real data. For example, how close are the inferred means to the actual sample mean:  ax = plt.subplot()\nsns.distplot([n.mean() for n in ppc['n']], kde=False, ax=ax)\nax.axvline(data.mean())\nax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');", 
            "title": "Posterior Predictive Checks in PyMC3"
        }, 
        {
            "location": "/posterior_predictive/#prediction", 
            "text": "The same pattern can be used for prediction. Here we're building a logistic regression model. Note that since we're dealing the full posterior, we're also getting uncertainty in our predictions for free.  # Use a theano shared variable to be able to exchange the data the model runs on\nfrom theano import shared  def invlogit(x):\n    return np.exp(x) / (1 + np.exp(x))\n\nn = 4000\nn_oos = 50\ncoeff = 1.\n\npredictors = np.random.normal(size=n)\n# Turn predictor into a shared var so that we can change it later\npredictors_shared = shared(predictors)\n\noutcomes = np.random.binomial(1, invlogit(coeff * predictors))  outcomes  array([1, 1, 0, ..., 1, 0, 0])  predictors_oos = np.random.normal(size=50)\noutcomes_oos = np.random.binomial(1, invlogit(coeff * predictors_oos))  def tinvlogit(x):\n    import theano.tensor as t\n    return t.exp(x) / (1 + t.exp(x))\n\nwith pm.Model() as model:\n    coeff = pm.Normal('coeff', mu=0, sd=1)\n    p = tinvlogit(coeff * predictors_shared)\n\n    o = pm.Bernoulli('o', p, observed=outcomes)\n\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(500, step)   [-----------------100%-----------------] 500 of 500 complete in 2.6 sec  # Changing values here will also change values in the model\npredictors_shared.set_value(predictors_oos)  # Simply running PPC will use the updated values and do prediction\nppc = run_ppc(trace, model=model, samples=500)", 
            "title": "Prediction"
        }, 
        {
            "location": "/posterior_predictive/#mean-predicted-values-plus-error-bars-to-give-sense-of-uncertainty-in-prediction", 
            "text": "plt.errorbar(x=predictors_oos, y=np.asarray(ppc['o']).mean(axis=0), yerr=np.asarray(ppc['o']).std(axis=0), linestyle='', marker='o')\nplt.plot(predictors_oos, outcomes_oos, 'o')\nplt.ylim(-.05, 1.05)\nplt.xlabel('predictor')\nplt.ylabel('outcome')  matplotlib.text.Text at 0x7f1e23214f98", 
            "title": "Mean predicted values plus error bars to give sense of uncertainty in prediction"
        }
    ]
}