{
    "docs": [
        {
            "location": "/",
            "text": "PyMC3\n\n\n\n\n\n\nPyMC3 is a python module for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems.\n\n\nCheck out the \nTutorial\n!\n\n\nPyMC3 is Beta software. Users should consider using \nPyMC 2 repository\n.\n\n\nFeatures\n\n\n\n\nIntuitive model specification syntax, for example, \nx ~ N(0,1)\n translates to \nx = Normal(0,1)\n\n\nPowerful sampling algorithms, such as the \nNo U-Turn Sampler\n, allow complex models with thousands of parameters with little specialized knowledge of fitting algorithms.\n\n\nEasy optimization for finding the \nmaximum a posteriori\n(MAP) point\n\n\nTheano\n features\n\n\nNumpy broadcasting and advanced indexing\n\n\nLinear algebra operators\n\n\nComputation optimization and dynamic C compilation\n\n\nSimple extensibility\n\n\nTransparent support for missing value imputation\n\n\n\n\nGetting started\n\n\n\n\nPyMC3 Tutorial\n\n\nBayesian Modelling in Python -- tutorials on Bayesian statistics and PyMC3 as Jupyter Notebooks by Mark Dregan\n\n\nCoal Mining Disasters model in \nPyMC 2\n and \nPyMC 3\n\n\nGlobal Health Metrics \n Evaluation model\n case study for GHME 2013\n\n\nStochastic Volatility model\n\n\nSeveral blog posts on linear regression\n\n\nTalk at PyData NYC 2013 on PyMC3\n\n\nPyMC3 port of the models presented in the book \"Doing Bayesian Data Analysis\" by John Kruschke\n\n\nThe PyMC3 examples folder\n\n\n\n\nInstallation\n\n\nThe latest version of PyMC3 can be installed from the master branch using pip:\n\n\npip install --process-dependency-links git+https://github.com/pymc-devs/pymc3\n\n\n\n\nThe \n--process-dependency-links\n flag ensures that the developmental branch of Theano, which PyMC3 requires, is installed. If a recent developmental version of Theano has been installed with another method, this flag can be dropped.\n\n\nAnother option is to clone the repository and install PyMC3 using \npython setup.py install\n or \npython setup.py develop\n.\n\n\nNote:\n Running \npip install pymc\n will install PyMC 2.3, not PyMC3, from PyPI.\n\n\nDependencies\n\n\nPyMC3 is tested on Python 2.7 and 3.3 and depends on Theano, NumPy,\nSciPy, Pandas, and Matplotlib (see setup.py for version information).\n\n\nOptional\n\n\nIn addtion to the above dependencies, the GLM submodule relies on\nPatsy.\n\n\nscikits.sparse\n enables sparse scaling matrices which are useful for large problems. Installation on Ubuntu is easy:\n\n\nsudo apt-get install libsuitesparse-dev\npip install git+https://github.com/njsmith/scikits-sparse.git\n\n\n\n\nOn Mac OS X you can install libsuitesparse 4.2.1 via homebrew (see http://brew.sh/ to install homebrew), manually add a link so the include files are where scikits-sparse expects them, and then install scikits-sparse:\n\n\nbrew install suite-sparse\nln -s /usr/local/Cellar/suite-sparse/4.2.1/include/ /usr/local/include/suitesparse\npip install git+https://github.com/njsmith/scikits-sparse.git\n\n\n\n\nLicense\n\n\nApache License, Version 2.0\n\n\nContributors\n\n\nSee the \nGitHub contributor page",
            "title": "Overview"
        },
        {
            "location": "/#pymc3",
            "text": "PyMC3 is a python module for Bayesian statistical modeling and model fitting which focuses on advanced Markov chain Monte Carlo fitting algorithms. Its flexibility and extensibility make it applicable to a large suite of problems.  Check out the  Tutorial !  PyMC3 is Beta software. Users should consider using  PyMC 2 repository .",
            "title": "PyMC3"
        },
        {
            "location": "/#features",
            "text": "Intuitive model specification syntax, for example,  x ~ N(0,1)  translates to  x = Normal(0,1)  Powerful sampling algorithms, such as the  No U-Turn Sampler , allow complex models with thousands of parameters with little specialized knowledge of fitting algorithms.  Easy optimization for finding the  maximum a posteriori (MAP) point  Theano  features  Numpy broadcasting and advanced indexing  Linear algebra operators  Computation optimization and dynamic C compilation  Simple extensibility  Transparent support for missing value imputation",
            "title": "Features"
        },
        {
            "location": "/#getting-started",
            "text": "PyMC3 Tutorial  Bayesian Modelling in Python -- tutorials on Bayesian statistics and PyMC3 as Jupyter Notebooks by Mark Dregan  Coal Mining Disasters model in  PyMC 2  and  PyMC 3  Global Health Metrics   Evaluation model  case study for GHME 2013  Stochastic Volatility model  Several blog posts on linear regression  Talk at PyData NYC 2013 on PyMC3  PyMC3 port of the models presented in the book \"Doing Bayesian Data Analysis\" by John Kruschke  The PyMC3 examples folder",
            "title": "Getting started"
        },
        {
            "location": "/#installation",
            "text": "The latest version of PyMC3 can be installed from the master branch using pip:  pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3  The  --process-dependency-links  flag ensures that the developmental branch of Theano, which PyMC3 requires, is installed. If a recent developmental version of Theano has been installed with another method, this flag can be dropped.  Another option is to clone the repository and install PyMC3 using  python setup.py install  or  python setup.py develop .  Note:  Running  pip install pymc  will install PyMC 2.3, not PyMC3, from PyPI.",
            "title": "Installation"
        },
        {
            "location": "/#dependencies",
            "text": "PyMC3 is tested on Python 2.7 and 3.3 and depends on Theano, NumPy,\nSciPy, Pandas, and Matplotlib (see setup.py for version information).  Optional  In addtion to the above dependencies, the GLM submodule relies on\nPatsy.  scikits.sparse  enables sparse scaling matrices which are useful for large problems. Installation on Ubuntu is easy:  sudo apt-get install libsuitesparse-dev\npip install git+https://github.com/njsmith/scikits-sparse.git  On Mac OS X you can install libsuitesparse 4.2.1 via homebrew (see http://brew.sh/ to install homebrew), manually add a link so the include files are where scikits-sparse expects them, and then install scikits-sparse:  brew install suite-sparse\nln -s /usr/local/Cellar/suite-sparse/4.2.1/include/ /usr/local/include/suitesparse\npip install git+https://github.com/njsmith/scikits-sparse.git",
            "title": "Dependencies"
        },
        {
            "location": "/#license",
            "text": "Apache License, Version 2.0",
            "title": "License"
        },
        {
            "location": "/#contributors",
            "text": "See the  GitHub contributor page",
            "title": "Contributors"
        },
        {
            "location": "/getting_started/",
            "text": "Probabilistic Programming in Python using PyMC\n\n\nAuthors: John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck\n\n\nAbstract\n\n\nProbabilistic Programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamliltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source Probabilistic Programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other Probabilistic Programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.\n\n\nIntroduction\n\n\nProbabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intuitive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). This class of samplers works well on high dimensional and complex posterior distributions and allows many complex models to be fit without specialized knowledge about fitting algorithms. HMC and NUTS take advantage of gradient information from the likelihood to achieve much faster convergence than traditional sampling methods, especially for larger models. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo, which means you usually don't need to have specialized knowledge about how the algorithms work. PyMC3, Stan (Stan Development Team, 2014), and the LaplacesDemon package for R are currently the only PP packages to offer HMC.\n\n\nProbabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.\n\n\nWhile most of PyMC3's user-facing features are written in pure Python, it leverages Theano (Bergstra et al., 2010) to transparently transcode models to C and compile them to machine code, thereby boosting performance. Theano is a library that allows expressions to be defined using generalized vector data structures called \ntensors\n, which are tightly integrated with the popular NumPy \nndarray\n data structure, and similarly allow for broadcasting and advanced indexing, just as NumPy arrays do. Theano also automatically optimizes the likelihood's computational graph for speed and provides simple GPU integration.\n\n\nHere, we present a primer on the use of PyMC3 for solving general Bayesian statistical inference and prediction problems. We will first see the basics of how to use PyMC3, motivated by a simple example: installation, data creation, model definition, model fitting and posterior analysis. Then we will cover two case studies and use them to show how to define and fit more sophisticated models. Finally we will show how to extend PyMC3 and discuss other useful features: the Generalized Linear Models subpackage, custom distributions, custom transformations and alternative storage backends.\n\n\nInstallation\n\n\nRunning PyMC3 requires a working Python interpreter, either version 2.7 (or more recent) or 3.4 (or more recent); we recommend that new users install version 3.4. A complete Python installation for Mac OSX, Linux and Windows can most easily be obtained by downloading and installing the free \nAnaconda Python Distribution\n by ContinuumIO. \n\n\nPyMC3\n can be installed using \npip\n (https://pip.pypa.io/en/latest/installing.html):\n\n\npip install git+https://github.com/pymc-devs/pymc3\n\n\n\n\nPyMC3 depends on several third-party Python packages which will be automatically installed when installing via pip. The four required dependencies are: \nTheano\n, \nNumPy\n, \nSciPy\n, and \nMatplotlib\n. \n\n\nTo take full advantage of PyMC3, the optional dependencies \nPandas\n and \nPatsy\n should also be installed. These are \nnot\n automatically installed, but can be installed by:\n\n\npip install patsy pandas\n\n\n\n\nThe source code for PyMC3 is hosted on GitHub at https://github.com/pymc-devs/pymc3 and is distributed under the liberal \nApache License 2.0\n. On the GitHub site, users may also report bugs and other issues, as well as contribute code to the project, which we actively encourage.\n\n\nA Motivating Example: Linear Regression\n\n\nTo introduce model definition, fitting and posterior analysis, we first consider a simple Bayesian linear regression model with normal priors for the parameters. We are interested in predicting outcomes \n$Y$\n as normally-distributed observations with an expected value \n$\\mu$\n that is a linear function of two predictor variables, \n$X_1$\n and \n$X_2$\n.\n\n\n$$\\begin{aligned} \nY  \n\\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n\\mu \n= \\alpha + \\beta_1 X_1 + \\beta_2 X_2\n\\end{aligned}$$\n\n\nwhere \n$\\alpha$\n is the intercept, and \n$\\beta_i$\n is the coefficient for covariate \n$X_i$\n, while \n$\\sigma$\n represents the observation error. Since we are constructing a Bayesian model, the unknown variables in the model must be assigned a prior distribution. We choose zero-mean normal priors with variance of 100 for both regression coefficients, which corresponds to \nweak\n information regarding the true parameter values. We choose a half-normal distribution (normal distribution bounded at zero) as the prior for \n$\\sigma$\n.\n\n\n$$\\begin{aligned} \n\\alpha \n\\sim \\mathcal{N}(0, 100) \\\\\n\\beta_i \n\\sim \\mathcal{N}(0, 100) \\\\\n\\sigma \n\\sim \\lvert\\mathcal{N}(0, 1){\\rvert}\n\\end{aligned}$$\n\n\nGenerating data\n\n\nWe can simulate some artificial data from this model using only NumPy's \nrandom\n module, and then use PyMC3 to try to recover the corresponding parameters. We are intentionally generating the data to closely correspond the PyMC3 model structure.\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Initialize random number generator\nnp.random.seed(123)\n\n# True parameter values\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\n\n# Size of dataset\nsize = 100\n\n# Predictor variable\nX1 = np.random.randn(size)\nX2 = np.random.randn(size) * 0.2\n\n# Simulate outcome variable\nY = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma\n\n\n\n\nHere is what the simulated data look like. We use the \npylab\n module from the plotting library matplotlib. \n\n\n%matplotlib inline \n\nfig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,4))\naxes[0].scatter(X1, Y)\naxes[1].scatter(X2, Y)\naxes[0].set_ylabel('Y'); axes[0].set_xlabel('X1'); axes[1].set_xlabel('X2');\n\n\n\n\n\n\nModel Specification\n\n\nSpecifying this model in PyMC3 is straightforward because the syntax is as close to the statistical notation. For the most part, each line of Python code corresponds to a line in the model notation above. \n\n\nFirst, we import the components we will need from PyMC.\n\n\nfrom pymc3 import Model, Normal, HalfNormal\n\n\n\n\nNow we build our model, which we will present in full first, then explain each part line-by-line.\n\n\nbasic_model = Model()\n\nwith basic_model:\n\n    # Priors for unknown model parameters\n    alpha = Normal('alpha', mu=0, sd=10)\n    beta = Normal('beta', mu=0, sd=10, shape=2)\n    sigma = HalfNormal('sigma', sd=1)\n\n    # Expected value of outcome\n    mu = alpha + beta[0]*X1 + beta[1]*X2\n\n    # Likelihood (sampling distribution) of observations\n    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)\n\n\n\n\nThe first line,\n\n\nbasic_model = Model()\n\n\n\n\ncreates a new \nModel\n object which is a container for the model random variables.\n\n\nFollowing instantiation of the model, the subsequent specification of the model components is performed inside a  \nwith\n statement:\n\n\nwith basic_model:\n\n\n\n\nThis creates a \ncontext manager\n, with our \nbasic_model\n as the context, that includes all statements until the indented block ends. This means all PyMC3 objects introduced in the indented code block below the \nwith\n statement are added to the model behind the scenes. Absent this context manager idiom, we would be forced to manually associate each of the variables with \nbasic_model\n right after we create them. If you try to create a new random variable without a \nwith model:\n statement, it will raise an error since there is no obvious model for the variable to be added to.\n\n\nThe first three statements in the context manager:\n\n\nalpha = Normal('alpha', mu=0, sd=10)\nbeta = Normal('beta', mu=0, sd=10, shape=2)\nsigma = HalfNormal('sigma', sd=1)\n\n\n\n\ncreate a \nstochastic\n random variables with a Normal prior distributions for the regression coefficients with a mean of 0 and standard deviation of 10 for the regression coefficients, and a half-normal distribution for the standard deviation of the observations, \n$\\sigma$\n. These are stochastic because their values are partly determined by its parents in the dependency graph of random variables, which for priors are simple constants, and partly random (or stochastic). \n\n\nWe call the \nNormal\n constructor to create a random variable to use as a normal prior. The first argument is always the \nname\n of the random variable, which should almost always match the name of the Python variable being assigned to, since it sometimes used to retrieve the variable from the model for summarizing output. The remaining required arguments for a stochastic object are the parameters, in this case \nmu\n, the mean, and \nsd\n, the standard deviation, which we assign hyperparameter values for the model. In general, a distribution's parameters are values that determine the location, shape or scale of the random variable, depending on the parameterization of the distribution. Most commonly used distributions, such as \nBeta\n, \nExponential\n, \nCategorical\n, \nGamma\n, \nBinomial\n and many others, are available in PyMC3.\n\n\nThe \nbeta\n variable has an additional \nshape\n argument to denote it as a vector-valued parameter of size 2. The \nshape\n argument is available for all distributions and specifies the length or shape of the random variable, but is optional for scalar variables, since it defaults to a value of one. It can be an integer, to specify an array, or a tuple, to specify a multidimensional array (\ne.g.\n \nshape=(5,7)\n makes random variable that takes on 5 by 7 matrix values). \n\n\nDetailed notes about distributions, sampling methods and other PyMC3 functions are available via the \nhelp\n function.\n\n\nhelp(Normal) #try help(Model), help(Uniform) or help(basic_model)\n\n\n\n\nHelp on class Normal in module pymc3.distributions.continuous:\n\nclass Normal(pymc3.distributions.distribution.Continuous)\n |  Normal log-likelihood.\n |  \n |  .. math::\night\\}\n |  \n |  Parameters\n |  ----------\n |  mu : float\n |      Mean of the distribution.\n |  tau : float\n |      Precision of the distribution, which corresponds to\n |      :math:`1/\\sigma^2` (tau \n 0).\n |  sd : float\n |      Standard deviation of the distribution. Alternative parameterization.\n |  \n |  .. note::\n |  - :math:`E(X) = \\mu`\n |  - :math:`Var(X) = 1/        au`\n |  \n |  Method resolution order:\n |      Normal\n |      pymc3.distributions.distribution.Continuous\n |      pymc3.distributions.distribution.Distribution\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, mu=0.0, tau=None, sd=None, *args, **kwargs)\n |  \n |  logp(self, value)\n |  \n |  random(self, point=None, size=None, repeat=None)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __getnewargs__(self)\n |  \n |  default(self)\n |  \n |  get_test_val(self, val, defaults)\n |  \n |  getattr_value(self, val)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  dist(*args, **kwargs) from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __new__(cls, name, *args, **kwargs)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n\n\nHaving defined the priors, the next statement creates the expected value \nmu\n of the outcomes, specifying the linear relationship:\n\n\nmu = alpha + beta[0]*X1 + beta[1]*X2\n\n\n\n\nThis creates a \ndeterministic\n random variable, which implies that its value is \ncompletely\n determined by its parents' values. That is, there is no uncertainty beyond that which is inherent in the parents' values. Here, \nmu\n is just the sum of the intercept \nalpha\n and the two products of the coefficients in \nbeta\n and the predictor variables, whatever their values may be. \n\n\nPyMC3 random variables and data can be arbitrarily added, subtracted, divided, multiplied together and indexed-into to create new random variables. This allows for great model expressivity. Many common mathematical functions like \nsum\n, \nsin\n, \nexp\n and linear algebra functions like \ndot\n (for inner product) and \ninv\n (for inverse) are also provided. \n\n\nThe final line of the model, defines \nY_obs\n, the sampling distribution of the outcomes in the dataset.\n\n\nY_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)\n\n\n\n\nThis is a special case of a stochastic variable that we call an \nobserved stochastic\n, and represents the data likelihood of the model. It is identical to a standard stochastic, except that its \nobserved\n argument, which passes the data to the variable, indicates that the values for this variable were observed, and should not be changed by any fitting algorithm applied to the model. The data can be passed in the form of either a \nnumpy.ndarray\n or \npandas.DataFrame\n object.\n\n\nNotice that, unlike for the priors of the model, the parameters for the normal distribution of \nY_obs\n are not fixed values, but rather are the deterministic object \nmu\n and the stochastic \nsigma\n. This creates parent-child relationships between the likelihood and these two variables.\n\n\nModel fitting\n\n\nHaving completely specified our model, the next step is to obtain posterior estimates for the unknown variables in the model. Ideally, we could calculate the posterior estimates analytically, but for most non-trivial models, this is not feasible. We will consider two approaches, whose appropriateness depends on the structure of the model and the goals of the analysis: finding the \nmaximum a posteriori\n (MAP) point using optimization methods, and computing summaries based on samples drawn from the posterior distribution using Markov Chain Monte Carlo (MCMC) sampling methods.\n\n\nMaximum a posteriori methods\n\n\nThe \nmaximum a posteriori (MAP)\n estimate for a model, is the mode of the posterior distribution and is generally found using numerical optimization methods. This is often fast and easy to do, but only gives a point estimate for the parameters and can be biased if the mode isn't representative of the distribution. PyMC3 provides this functionality with the \nfind_MAP\n function.\n\n\nBelow we find the MAP for our original model. The MAP is returned as a parameter \npoint\n, which is always represented by a Python dictionary of variable names to NumPy arrays of parameter values. \n\n\nfrom pymc3 import find_MAP\n\nmap_estimate = find_MAP(model=basic_model)\n\nprint(map_estimate)\n\n\n\n\n{'alpha': array(0.9065985757590524), 'sigma_log': array(-0.03278147323479127), 'beta': array([ 0.94848596,  2.60705517])}\n\n\n\nBy default, \nfind_MAP\n uses the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) optimization algorithm to find the maximum of the log-posterior but also allows selection of other optimization algorithms from the \nscipy.optimize\n module. For example, below we use Powell's method to find the MAP.\n\n\nfrom scipy import optimize\n\nmap_estimate = find_MAP(model=basic_model, fmin=optimize.fmin_powell)\n\nprint(map_estimate)\n\n\n\n\n{'alpha': array(0.9090521896381477), 'sigma_log': array(-0.030009775415224028), 'beta': array([ 0.95140146,  2.61437458])}\n\n\n\nIt is important to note that the MAP estimate is not always reasonable, especially if the mode is at an extreme. This can be a subtle issue; with high dimensional posteriors, one can have areas of extremely high density but low total probability because the volume is very small. This will often occur in hierarchical models with the variance parameter for the random effect. If the individual group means are all the same, the posterior will have near infinite density if the scale parameter for the group means is almost zero, even though the probability of such a small scale parameter will be small since the group means must be extremely close together. \n\n\nMost techniques for finding the MAP estimate also only find a \nlocal\n optimum (which is often good enough), but can fail badly for multimodal posteriors if the different modes are meaningfully different.\n\n\nSampling methods\n\n\nThough finding the MAP is a fast and easy way of obtaining estimates of the unknown model parameters, it is limited because there is no associated estimate of uncertainty produced with the MAP estimates. Instead, a simulation-based approach such as Markov chain Monte Carlo (MCMC) can be used to obtain a Markov chain of values that, given the satisfaction of certain conditions, are indistinguishable from samples from the posterior distribution. \n\n\nTo conduct MCMC sampling to generate posterior samples in PyMC3, we specify a \nstep method\n object that corresponds to a particular MCMC algorithm, such as Metropolis, Slice sampling, or the No-U-Turn Sampler (NUTS). PyMC3's \nstep_methods\n submodule contains the following samplers: \nNUTS\n, \nMetropolis\n, \nSlice\n, \nHamiltonianMC\n, and \nBinaryMetropolis\n. These step methods can be assigned manually, or assigned automatically by PyMC3. Auto-assignment is based on the attributes of each variable in the model. In general:\n\n\n\n\nBinary variables will be assigned to \nBinaryMetropolis\n\n\nDiscrete variables will be assigned to \nMetropolis\n\n\nContinuous variables will be assigned to \nNUTS\n\n\n\n\nAuto-assignment can be overriden for any subset of variables by specifying them manually prior to sampling.\n\n\nGradient-based sampling methods\n\n\nPyMC3 has the standard sampling algorithms like adaptive Metropolis-Hastings and adaptive slice sampling, but PyMC3's most capable step method is the No-U-Turn Sampler. NUTS is especially useful on models that have many continuous parameters, a situation where other MCMC algorithms work very slowly. It takes advantage of information about where regions of higher probability are, based on the gradient of the log posterior-density. This helps it achieve dramatically faster convergence on large problems than traditional sampling methods achieve. PyMC3 relies on Theano to analytically compute model gradients via automatic differentiation of the posterior density. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo. For random variables that are undifferentiable (namely, discrete variables) NUTS cannot be used, but it may still be used on the differentiable variables in a model that contains undifferentiable variables. \n\n\nNUTS requires a scaling matrix parameter, which is analogous to the variance parameter for the jump proposal distribution in Metropolis-Hastings, although NUTS uses it somewhat differently. The matrix gives the rough shape of the distribution so that NUTS does not make jumps that are too large in some directions and too small in other directions. It is important to set this scaling parameter to a reasonable value to facilitate efficient sampling. This is especially true for models that have many unobserved stochastic random variables or models with highly non-normal posterior distributions. Poor scaling parameters will slow down NUTS significantly, sometimes almost stopping it completely. A reasonable starting point for sampling can also be important for efficient sampling, but not as often.\n\n\nFortunately NUTS can often make good guesses for the scaling parameters. If you pass a point in parameter space (as a dictionary of variable names to parameter values, the same format as returned by \nfind_MAP\n) to NUTS, it will look at the local curvature of the log posterior-density (the diagonal of the Hessian matrix) at that point to make a guess for a good scaling vector, which often results in a good value. The MAP estimate is often a good point to use to initiate sampling. It is also possible to supply your own vector or scaling matrix to NUTS, though this is a more advanced use. If you wish to modify a Hessian at a specific point to use as your scaling matrix or vector, you can use \nfind_hessian\n or \nfind_hessian_diag\n.\n\n\nFor our basic linear regression example in \nbasic_model\n, we will use NUTS to sample 2000 draws from the posterior using the MAP as the starting point and scaling point. This must also be performed inside the context of the model.\n\n\nfrom pymc3 import NUTS, sample\n\nwith basic_model:\n\n    # obtain starting values via MAP\n    start = find_MAP(fmin=optimize.fmin_powell)\n\n    # draw 2000 posterior samples\n    trace = sample(2000, start=start) \n\n\n\n\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to alpha\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to beta\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to sigma_log\n [-----------------100%-----------------] 2000 of 2000 complete in 1.4 sec\n\n\n\nThe \nsample\n function runs the step method(s) assigned (or passed) to it for the given number of iterations and returns a \nTrace\n object containing the samples collected, in the order they were collected. The \ntrace\n object can be queried in a similar way to a \ndict\n containing a map from variable names to \nnumpy.array\ns. The first dimension of the array is the sampling index and the later dimensions match the shape of the variable. We can see the last 5 values for the \nalpha\n variable as follows:\n\n\ntrace['alpha'][-5:]\n\n\n\n\narray([ 0.80387719,  0.88728378,  1.06832995,  0.71061793,  0.84123803])\n\n\n\nIf we wanted to use the slice sampling algorithm to \nsigma\n instead of NUTS (which was assigned automatically), we could have specified this as the \nstep\n argument for \nsample\n.\n\n\nfrom pymc3 import Slice\n\nwith basic_model:\n\n    # obtain starting values via MAP\n    start = find_MAP(fmin=optimize.fmin_powell)\n\n    # instantiate sampler\n    step = Slice(vars=[sigma]) \n\n    # draw 5000 posterior samples\n    trace = sample(5000, step=step, start=start)   \n\n\n\n\n\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to alpha\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to beta\n [-----------------100%-----------------] 5000 of 5000 complete in 13.3 sec\n\n\n\nPosterior analysis\n\n\nPyMC3\n provides plotting and summarization functions for inspecting the sampling output. A simple posterior plot can be created using \ntraceplot\n.\n\n\nfrom pymc3 import traceplot\n\ntraceplot(trace);\n\n\n\n\n\n\nThe left column consists of a smoothed histogram (using kernel density estimation) of the marginal posteriors of each stochastic random variable while the right column contains the samples of the Markov chain plotted in sequential order. The \nbeta\n variable, being vector-valued, produces two histograms and two sample traces, corresponding to both predictor coefficients.\n\n\nIn addition, the \nsummary\n function provides a text-based output of common posterior statistics:\n\n\nfrom pymc3 import summary\n\nsummary(trace)\n\n\n\n\nalpha:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.907            0.099            0.001            [0.718, 1.099]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.720          0.839          0.904          0.975          1.103\n\n\nbeta:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.949            0.091            0.001            [0.777, 1.127]\n  2.598            0.510            0.006            [1.647, 3.635]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.774          0.887          0.948          1.010          1.126\n  1.585          2.257          2.602          2.946          3.584\n\n\nsigma_log:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  -0.012           0.070            0.001            [-0.149, 0.124]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -0.147         -0.059         -0.013         0.034          0.127\n\n\nsigma:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.991            0.069            0.001            [0.852, 1.121]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.863          0.943          0.987          1.034          1.136\n\n\n\nCase study 1: Stochastic volatility\n\n\nWe present a case study of stochastic volatility, time varying stock market volatility, to illustrate PyMC3's use in addressing a more realistic problem. The distribution of market returns is highly non-normal, which makes sampling the volatilities significantly more difficult. This example has 400+ parameters so using common sampling algorithms like Metropolis-Hastings would get bogged down, generating highly autocorrelated samples. Instead, we use NUTS, which is dramatically more efficient.\n\n\nThe Model\n\n\nAsset prices have time-varying volatility (variance of day over day \nreturns\n). In some periods, returns are highly variable, while in others they are very stable. Stochastic volatility models address this with a latent volatility variable, which changes over time. The following model is similar to the one described in the NUTS paper (Hoffman 2014, p. 21).\n\n\n$$\\begin{aligned} \n  \\sigma \n\\sim exp(50) \\\\\n  \\nu \n\\sim exp(.1) \\\\\n  s_i \n\\sim \\mathcal{N}(s_{i-1}, \\sigma^{-2}) \\\\\n  log(y_i) \n\\sim t(\\nu, 0, exp(-2 s_i))\n\\end{aligned}$$\n\n\nHere, \n$y$\n is the daily return series which is modeled with a Student-t distribution with an unknown degrees of freedom parameter, and a scale parameter determined by a latent process \n$s$\n. The individual \n$s_i$\n are the individual daily log volatilities in the latent log volatility process. \n\n\nThe Data\n\n\nOur data consist of daily returns of the S\nP 500 during the 2008 financial crisis.\n\n\nimport pandas as pd\nreturns = pd.read_csv('data/SP500.csv', index_col=0, parse_dates=True)\nprint(len(returns))\n\n\n\n\n400\n\n\n\nreturns.plot(figsize=(10, 6))\nplt.ylabel('daily returns in %');\n\n\n\n\n\n\nModel Specification\n\n\nAs with the linear regression example, specifying the model in PyMC3 mirrors its statistical specification. This model employs several new distributions: the \nExponential\n distribution for the \n$ \\nu $\n and \n$\\sigma$\n priors, the student-t (\nT\n) distribution for distribution of returns, and the \nGaussianRandomWalk\n for the prior for the latent volatilities.   \n\n\nIn PyMC3, variables with purely positive priors like \nExponential\n are transformed with a log transform. This makes sampling more robust. Behind the scenes, a variable in the unconstrained space (named \"variableName_log\") is added to the model for sampling. In this model this happens behind the scenes for both the degrees of freedom, \nnu\n, and the scale parameter for the volatility process, \nsigma\n, since they both have exponential priors. Variables with priors that constrain them on two sides, like \nBeta\n or \nUniform\n, are also transformed to be unconstrained but with a log odds transform. \n\n\nAlthough, unlike model specification in PyMC2, we do not typically provide starting points for variables at the model specification stage, we can also provide an initial value for any distribution (called a \"test value\") using the \ntestval\n argument. This overrides the default test value for the distribution (usually the mean, median or mode of the distribution), and is most often useful if some values are illegal and we want to ensure we select a legal one. The test values for the distributions are also used as a starting point for sampling and optimization by default, though this is easily overriden. \n\n\nThe vector of latent volatilities \ns\n is given a prior distribution by \nGaussianRandomWalk\n. As its name suggests GaussianRandomWalk is a vector valued distribution where the values of the vector form a random normal walk of length n, as specified by the \nshape\n argument. The scale of the innovations of the random walk, \nsigma\n, is specified in terms of the precision of the normally distributed innovations and can be a scalar or vector. \n\n\nfrom pymc3 import Exponential, T, exp, Deterministic\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nwith Model() as sp500_model:\n\n    nu = Exponential('nu', 1./10, testval=5.)\n\n    sigma = Exponential('sigma', 1./.02, testval=.1)\n\n    s = GaussianRandomWalk('s', sigma**-2, shape=len(returns))\n\n    volatility_process = Deterministic('volatility_process', exp(-2*s))\n\n    r = T('r', nu, lam=1/volatility_process, observed=returns['S\nP500'])\n\n\n\n\nNotice that we transform the log volatility process \ns\n into the volatility process by \nexp(-2*s)\n. Here, \nexp\n is a Theano function, rather than the corresponding function in NumPy; Theano provides a large subset of the mathematical functions that NumPy does.\n\n\nAlso note that we have declared the \nModel\n name \nsp500_model\n in the first occurrence of the context manager, rather than splitting it into two lines, as we did for the first example.\n\n\nFitting\n\n\nBefore we draw samples from the posterior, it is prudent to find a decent starting value by finding a point of relatively high probability. For this model, the full \nmaximum a posteriori\n (MAP) point over all variables is degenerate and has infinite density. But, if we fix \nlog_sigma\n and \nnu\n it is no longer degenerate, so we find the MAP with respect only to the volatility process \ns\n keeping \nlog_sigma\n and \nnu\n constant at their default values (remember that we set \ntestval=.1\n for \nsigma\n). We use the Limited-memory BFGS (L-BFGS) optimizer, which is provided by the \nscipy.optimize\n package, as it is more efficient for high dimensional functions and we have 400 stochastic random variables (mostly from \ns\n).\n\n\nTo do the sampling, we do a short initial run to put us in a volume of high probability, then start again at the new starting point. \ntrace[-1]\n gives us the last point in the sampling trace. NUTS will recalculate the scaling parameters based on the new point, and in this case it leads to faster sampling due to better scaling.\n\n\nimport scipy\nwith sp500_model:\n    start = find_MAP(vars=[s], fmin=scipy.optimize.fmin_l_bfgs_b)\n\n    step = NUTS(scaling=start)\n    trace = sample(100, step, progressbar=False)\n\n    # Start next run at the last sampled position.\n    step = NUTS(scaling=trace[-1], gamma=.25)\n    trace = sample(2000, step, start=trace[-1], progressbar=False, njobs=4)\n\n\n\n\nWe can check our samples by looking at the traceplot for \nnu\n and \nsigma\n.\n\n\ntraceplot(trace, [nu, sigma]);\n\n\n\n\n\n\nFinally we plot the distribution of volatility paths by plotting many of our sampled volatility paths on the same graph. Each is rendered partially transparent (via the \nalpha\n argument in Matplotlib's \nplot\n function) so the regions where many paths overlap are shaded more darkly.\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\nreturns.plot(ax=ax)\nax.plot(returns.index, 1/np.exp(trace['s',::30].T), 'r', alpha=.03);\nax.set(title='volatility_process', xlabel='time', ylabel='volatility');\nax.legend(['S\nP500', 'stochastic volatility process'])\n\n\n\n\nmatplotlib.legend.Legend at 0x7f6055033b00\n\n\n\n\n\n\nAs you can see, the model correctly infers the increase in volatility during the 2008 financial crash. Moreover, note that this model is quite complex because of its high dimensionality and dependency-structure in the random walk distribution. NUTS as implemented in PyMC3, however, correctly infers the posterior distribution with ease.\n\n\nCase study 2: Coal mining disasters\n\n\nConsider the following time series of recorded coal mining disasters in the UK from 1851 to 1962 (Jarrett, 1979). The number of disasters is thought to have been affected by changes in safety regulations during this period. Unfortunately, we also have pair of years with missing data, identified as missing by a NumPy MaskedArray using -999 as the marker value. \n\n\nNext we will build a model for this series and attempt to estimate when the change occurred. At the same time, we will see how to handle missing data, use multiple samplers and sample from discrete random variables. \n\n\ndisaster_data = np.ma.masked_values([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n                            3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n                            2, 2, 3, 4, 2, 1, 3, -999, 2, 1, 1, 1, 1, 3, 0, 0,\n                            1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n                            0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n                            3, 3, 1, -999, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n                            0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], value=-999)\nyear = np.arange(1851, 1962)\n\nplot(year, disaster_data, 'o', markersize=8);\nylabel(\nDisaster count\n)\nxlabel(\nYear\n)\n\n\n\n\nmatplotlib.text.Text at 0x7f605096dcc0\n\n\n\n\n\n\nOccurrences of disasters in the time series is thought to follow a Poisson process with a large rate parameter in the early part of the time series, and from one with a smaller rate in the later part. We are interested in locating the change point in the series, which perhaps is related to changes in mining safety regulations.\n\n\nIn our model, \n\n\n$$ \n\\begin{aligned}  \n  D_t \n\\sim \\text{Pois}(r_t), r_t= \\begin{cases} \n   l, \n \\text{if } t \\lt s \\\\\n   r, \n \\text{if } t \\ge s \n   \\end{cases} \\\\\n  s \n\\sim \\text{Unif}(t_l, t_h)\\\\         \n  e \n\\sim \\text{exp}(1)\\\\\n  l \n\\sim \\text{exp}(1)    \n\\end{aligned}\n$$\n\n the parameters are defined as follows: \n   * \n$D_t$\n: The number of disasters in year \n$t$\n\n   * \n$r_t$\n: The rate parameter of the Poisson distribution of disasters in year \n$t$\n.\n   * \n$s$\n: The year in which the rate parameter changes (the switchpoint).\n   * \n$e$\n: The rate parameter before the switchpoint \n$s$\n.\n   * \n$l$\n: The rate parameter after the switchpoint \n$s$\n.\n   * \n$t_l$\n, \n$t_h$\n: The lower and upper boundaries of year \n$t$\n.\n\n\nThis model is built much like our previous models. The major differences are the introduction of discrete variables with the Poisson and discrete-uniform priors and the novel form of the deterministic random variable \nrate\n.\n\n\nfrom pymc3 import DiscreteUniform, Poisson, switch\n\nwith Model() as disaster_model:\n\n    switchpoint = DiscreteUniform('switchpoint', lower=year.min(), upper=year.max(), testval=1900)\n\n    # Priors for pre- and post-switch rates number of disasters\n    early_rate = Exponential('early_rate', 1)\n    late_rate = Exponential('late_rate', 1)\n\n    # Allocate appropriate Poisson rates to years before and after current\n    rate = switch(switchpoint \n= year, early_rate, late_rate)\n\n    disasters = Poisson('disasters', rate, observed=disaster_data)\n\n\n\n\nThe logic for the rate random variable,\n\n\nrate = switch(switchpoint \n= year, early_rate, late_rate)\n\n\n\n\nis implemented using \nswitch\n, a Theano function that works like an if statement. It uses the first argument to switch between the next two arguments.\n\n\nMissing values are handled transparently by passing a \nMaskedArray\n or a \npandas.DataFrame\n with NaN values to the \nobserved\n argument when creating an observed stochastic random variable. Behind the scenes, another random variable, \ndisasters.missing_values\n is created to model the missing values. All we need to do to handle the missing values is ensure we sample this random variable as well.\n\n\nUnfortunately because they are discrete variables and thus have no meaningful gradient, we cannot use NUTS for sampling \nswitchpoint\n or the missing disaster observations. Instead, we will sample using a \nMetroplis\n step method, which implements adaptive Metropolis-Hastings, because it is designed to handle discrete values.\n\n\nWe sample with both samplers at once by passing them to the \nsample\n function in a list. Each new sample is generated by first applying \nstep1\n then \nstep2\n.\n\n\nfrom pymc3 import Metropolis \n\nwith disaster_model:\n    step1 = NUTS([early_rate, late_rate])\n\n    # Use Metropolis for switchpoint, and missing values since it accommodates discrete variables\n    step2 = Metropolis([switchpoint, disasters.missing_values[0]] )\n\n    trace = sample(10000, step=[step1, step2])\n\n\n\n\n [-----------------100%-----------------] 10000 of 10000 complete in 8.8 sec\n\n\n\nIn the trace plot below we can see that there's about a 10 year span that's plausible for a significant change in safety, but a 5 year span that contains most of the probability mass. The distribution is jagged because of the jumpy relationship between the year switchpoint and the likelihood  and not due to sampling error.\n\n\ntraceplot(trace);\n\n\n\n\n\n\nArbitrary deterministics\n\n\nDue to its reliance on Theano, PyMC3 provides many mathematical functions and operators for transforming random variables into new random variables. However, the library of functions in Theano is not exhaustive, therefore Theano and PyMC3 provide functionality for creating arbitrary Theano functions in pure Python, and including these functions in PyMC models. This is supported with the \nas_op\n function decorator.\n\n\nTheano needs to know the types of the inputs and outputs of a function, which are specified for \nas_op\n by \nitypes\n for inputs and \notypes\n for outputs. The Theano documentation includes \nan overview of the available types\n.\n\n\nimport theano.tensor as T \nfrom theano.compile.ops import as_op\n\n@as_op(itypes=[T.lscalar], otypes=[T.lscalar])\ndef crazy_modulo3(value):\n    if value \n 0: \n        return value % 3\n    else :\n        return (-value + 1) % 3\n\nwith Model() as model_deterministic:\n    a = Poisson('a', 1)\n    b = crazy_modulo3(a)\n\n\n\n\nAn important drawback of this approach is that it is not possible for \ntheano\n to inspect these functions in order to compute the gradient required for the Hamiltonian-based samplers. Therefore, it is not possible to use the HMC or NUTS samplers for a model that uses such an operator. However, it is possible to add a gradient if we inherit from \ntheano.Op\n instead of using \nas_op\n. The PyMC example set includes \na more elaborate example of the usage of \nas_op\n.\n\n\nArbitrary distributions\n\n\nSimilarly, the library of statistical distributions in PyMC3 is not exhaustive, but PyMC allows for the creation of user-defined functions for an arbitrary probability distribution. For simple statistical distributions, the \nDensityDist\n function takes as an argument any function that calculates a log-probability \n$log(p(x))$\n. This function may employ other random variables in its calculation. Here is an example inspired by a blog post by Jake Vanderplas on which priors to use for a linear regression (Vanderplas, 2014). \n\n\nimport theano.tensor as T\nfrom pymc3 import DensityDist, Uniform\n\nwith Model() as model:\n    alpha = Uniform('intercept', -100, 100)\n\n    # Create custom densities\n    beta = DensityDist('beta', lambda value: -1.5 * T.log(1 + value**2), testval=0)\n    eps = DensityDist('eps', lambda value: -T.log(T.abs_(value)), testval=1)\n\n    # Create likelihood\n    like = Normal('y_est', mu=alpha + beta * X, sd=eps, observed=Y)\n\n\n\n\nFor more complex distributions, one can create a subclass of \nContinuous\n or \nDiscrete\n and provide the custom \nlogp\n function, as required. This is how the built-in distributions in PyMC are specified. As an example, fields like psychology and astrophysics have complex likelihood functions for a particular process that may require numerical approximation. In these cases, it is impossible to write the function in terms of predefined theano operators and we must use a custom theano operator using \nas_op\n or inheriting from \ntheano.Op\n. \n\n\nImplementing the \nbeta\n variable above as a \nContinuous\n subclass is shown below, along with a sub-function using the \nas_op\n decorator, though this is not strictly necessary.\n\n\nfrom pymc3.distributions import Continuous\n\nclass Beta(Continuous):\n    def __init__(self, mu, *args, **kwargs):\n        super(Beta, self).__init__(*args, **kwargs)\n        self.mu = mu\n        self.mode = mu\n\n    def logp(self, value):\n        mu = self.mu\n        return beta_logp(value - mu)\n\n@as_op(itypes=[T.dscalar], otypes=[T.dscalar])\ndef beta_logp(value):\n    return -1.5 * np.log(1 + (value)**2)\n\n\nwith Model() as model:\n    beta = Beta('slope', mu=0, testval=0)\n\n\n\n\nGeneralized Linear Models\n\n\nGeneralized Linear Models (GLMs) are a class of flexible models that are widely used to estimate regression relationships between a single outcome variable and one or multiple predictors. Because these models are so common, \nPyMC3\n offers a \nglm\n submodule that allows flexible creation of various GLMs with an intuitive \nR\n-like syntax that is implemented via the \npatsy\n module.\n\n\nThe \nglm\n submodule requires data to be included as a \npandas\n \nDataFrame\n. Hence, for our linear regression example:\n\n\n# Convert X and Y to a pandas DataFrame\nimport pandas \n\ndf = pandas.DataFrame({'x1': X1, 'x2': X2, 'y': Y})\n\n\n\n\nThe model can then be very concisely specified in one line of code.\n\n\nfrom pymc3.glm import glm\n\nwith Model() as model_glm:\n    glm('y ~ x1 + x2', df)\n    trace = sample(5000)\n\n\n\n\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to Intercept\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to x1\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to x2\nAssigned \nclass 'pymc3.step_methods.nuts.NUTS'\n to sd_log\n [-----------------100%-----------------] 5000 of 5000 complete in 4.3 sec\n\n\n\nThe error distribution, if not specified via the \nfamily\n argument, is assumed to be normal. In the case of logistic regression, this can be modified by passing in a \nBinomial\n family object.\n\n\nfrom pymc3.glm.families import Binomial\n\ndf_logistic = pandas.DataFrame({'x1': X1, 'y': Y \n np.median(Y)})\n\nwith Model() as model_glm_logistic:\n    glm('y ~ x1', df_logistic, family=Binomial())\n\n\n\n\nBackends\n\n\nPyMC3\n has support for different ways to store samples during and after sampling, called backends, including in-memory (default), text file, and SQLite. These can be found in \npymc.backends\n:\n\n\nBy default, an in-memory \nndarray\n is used but if the samples would get too large to be held in memory we could use the \nsqlite\n backend:\n\n\nfrom pymc3.backends import SQLite\n\nwith Model() as model_glm_logistic:\n    glm('y ~ x1', df_logistic, family=Binomial())\n\n    backend = SQLite('trace.sqlite')\n    start = find_MAP()\n    step = NUTS(scaling=start)\n    trace = sample(5000, step=step, start=start, trace=backend)\n\n\n\n\n [-----------------100%-----------------] 5000 of 5000 complete in 4.5 sec\n\n\n\nsummary(trace, vars=['x1'])\n\n\n\n\nx1:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  -1.128           211.811          1.126            [-0.917, 0.916]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -0.908         -0.216         0.002          0.223          0.926\n\n\n\nThe stored trace can then later be loaded using the \nload\n command:\n\n\nfrom pymc3.backends.sqlite import load\n\nwith basic_model:\n    trace_loaded = load('trace.sqlite')\n\n\n\n\nMore information about \nbackends\n can be found in the docstring of \npymc.backends\n.\n\n\nDiscussion\n\n\nProbabilistic programming is an emerging paradigm in statistical learning, of which Bayesian modeling is an important sub-discipline. The signature characteristics of probabilistic programming--specifying variables as probability distributions and conditioning variables on other variables and on observations--makes it a powerful tool for building models in a variety of settings, and over a range of model complexity. Accompanying the rise of probabilistic programming has been a burst of innovation in fitting methods for Bayesian models that represent notable improvement over existing MCMC methods. Yet, despite this expansion, there are few software packages available that have kept pace with the methodological innovation, and still fewer that allow non-expert users to implement models.\n\n\nPyMC3 provides a probabilistic programming platform for quantitative researchers to implement statistical models flexibly and succinctly. A large library of statistical distributions and several pre-defined fitting algorithms allows users to focus on the scientific problem at hand, rather than the implementation details of Bayesian modeling. The choice of Python as a development language, rather than a domain-specific language, means that PyMC3 users are able to work interactively to build models, introspect model objects, and debug or profile their work, using a dynamic, high-level programming language that is easy to learn. The modular, object-oriented design of PyMC3 means that adding new fitting algorithms or other features is straightforward. In addition, PyMC3 comes with several features not found in most other packages, most notably Hamiltonian-based samplers as well as automatical transforms of constrained random variables which is only offered by STAN. Unlike STAN, however, PyMC3 supports discrete variables as well as non-gradient based sampling algorithms like Metropolis-Hastings and Slice sampling.\n\n\nDevelopment of PyMC3 is an ongoing effort and several features are planned for future versions. Most notably, variational inference techniques are often more efficient than MCMC sampling, at the cost of generalizability. More recently, however, black-box variational inference algorithms have been developed, such as automatic differentiation variational inference (ADVI; Kucukelbir et al., in prep). This algorithm is slated for addition to PyMC3. As an open-source scientific computing toolkit, we encourage researchers developing new fitting algorithms for Bayesian models to provide reference implementations in PyMC3. Since samplers can be written in pure Python code, they can be implemented generally to make them work on arbitrary PyMC3 models, giving authors a larger audience to put their methods into use.\n\n\nReferences\n\n\nPatil, A., D. Huard and C.J. Fonnesbeck. (2010) PyMC: Bayesian Stochastic Modelling in Python. Journal of Statistical Software, 35(4), pp. 1-81\n\n\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. (2012) \u201cTheano: new features and speed improvements\u201d. NIPS 2012 deep learning workshop.\n\n\nBergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010) \u201cTheano: A CPU and GPU Math Expression Compiler\u201d. Proceedings of the Python for Scientific Computing Conference (SciPy) 2010. June 30 - July 3, Austin, TX\n\n\nLunn, D.J., Thomas, A., Best, N., and Spiegelhalter, D. (2000) WinBUGS -- a Bayesian modelling framework: concepts, structure, and extensibility. Statistics and Computing, 10:325--337.\n\n\nNeal, R.M. Slice sampling. Annals of Statistics. (2003). doi:10.2307/3448413.\n\n\nvan Rossum, G. The Python Library Reference Release 2.6.5., (2010). URL http://docs.python.org/library/.\n\n\nDuane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987) \u201cHybrid Monte Carlo\u201d, Physics Letters, vol. 195, pp. 216-222.\n\n\nStan Development Team. (2014). Stan: A C++ Library for Probability and Sampling, Version 2.5.0.   http://mc-stan.org. \n\n\nGamerman, D. Markov Chain Monte Carlo: statistical simulation for Bayesian inference. Chapman and Hall, 1997.\n\n\nHoffman, M. D., \n Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research, 30.\n\n\nKucukelbir A, Ranganath R, Gelman A, and Blei DM. Automatic Variational Inference in Stan http://arxiv.org/abs/1506.03431, in prep.\n\n\nVanderplas, Jake. \"Frequentism and Bayesianism IV: How to be a Bayesian in Python.\" Pythonic Perambulations. N.p., 14 Jun 2014. Web. 27 May. 2015. \nhttps://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/\n.\n\n\nR.G. Jarrett. A note on the intervals between coal mining disasters. Biometrika, 66:191\u2013193, 1979.",
            "title": "Getting started"
        },
        {
            "location": "/getting_started/#probabilistic-programming-in-python-using-pymc",
            "text": "Authors: John Salvatier, Thomas V. Wiecki, Christopher Fonnesbeck",
            "title": "Probabilistic Programming in Python using PyMC"
        },
        {
            "location": "/getting_started/#abstract",
            "text": "Probabilistic Programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamliltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source Probabilistic Programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other Probabilistic Programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.",
            "title": "Abstract"
        },
        {
            "location": "/getting_started/#introduction",
            "text": "Probabilistic programming (PP) allows flexible specification of Bayesian statistical models in code. PyMC3 is a new, open-source PP framework with an intuitive and readable, yet powerful, syntax that is close to the natural syntax statisticians use to describe models. It features next-generation Markov chain Monte Carlo (MCMC) sampling algorithms such as the No-U-Turn Sampler (NUTS; Hoffman, 2014), a self-tuning variant of Hamiltonian Monte Carlo (HMC; Duane, 1987). This class of samplers works well on high dimensional and complex posterior distributions and allows many complex models to be fit without specialized knowledge about fitting algorithms. HMC and NUTS take advantage of gradient information from the likelihood to achieve much faster convergence than traditional sampling methods, especially for larger models. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo, which means you usually don't need to have specialized knowledge about how the algorithms work. PyMC3, Stan (Stan Development Team, 2014), and the LaplacesDemon package for R are currently the only PP packages to offer HMC.  Probabilistic programming in Python confers a number of advantages including multi-platform compatibility, an expressive yet clean and readable syntax, easy integration with other scientific libraries, and extensibility via C, C++, Fortran or Cython. These features make it relatively straightforward to write and use custom statistical distributions, samplers and transformation functions, as required by Bayesian analysis.  While most of PyMC3's user-facing features are written in pure Python, it leverages Theano (Bergstra et al., 2010) to transparently transcode models to C and compile them to machine code, thereby boosting performance. Theano is a library that allows expressions to be defined using generalized vector data structures called  tensors , which are tightly integrated with the popular NumPy  ndarray  data structure, and similarly allow for broadcasting and advanced indexing, just as NumPy arrays do. Theano also automatically optimizes the likelihood's computational graph for speed and provides simple GPU integration.  Here, we present a primer on the use of PyMC3 for solving general Bayesian statistical inference and prediction problems. We will first see the basics of how to use PyMC3, motivated by a simple example: installation, data creation, model definition, model fitting and posterior analysis. Then we will cover two case studies and use them to show how to define and fit more sophisticated models. Finally we will show how to extend PyMC3 and discuss other useful features: the Generalized Linear Models subpackage, custom distributions, custom transformations and alternative storage backends.",
            "title": "Introduction"
        },
        {
            "location": "/getting_started/#installation",
            "text": "Running PyMC3 requires a working Python interpreter, either version 2.7 (or more recent) or 3.4 (or more recent); we recommend that new users install version 3.4. A complete Python installation for Mac OSX, Linux and Windows can most easily be obtained by downloading and installing the free  Anaconda Python Distribution  by ContinuumIO.   PyMC3  can be installed using  pip  (https://pip.pypa.io/en/latest/installing.html):  pip install git+https://github.com/pymc-devs/pymc3  PyMC3 depends on several third-party Python packages which will be automatically installed when installing via pip. The four required dependencies are:  Theano ,  NumPy ,  SciPy , and  Matplotlib .   To take full advantage of PyMC3, the optional dependencies  Pandas  and  Patsy  should also be installed. These are  not  automatically installed, but can be installed by:  pip install patsy pandas  The source code for PyMC3 is hosted on GitHub at https://github.com/pymc-devs/pymc3 and is distributed under the liberal  Apache License 2.0 . On the GitHub site, users may also report bugs and other issues, as well as contribute code to the project, which we actively encourage.",
            "title": "Installation"
        },
        {
            "location": "/getting_started/#a-motivating-example-linear-regression",
            "text": "To introduce model definition, fitting and posterior analysis, we first consider a simple Bayesian linear regression model with normal priors for the parameters. We are interested in predicting outcomes  $Y$  as normally-distributed observations with an expected value  $\\mu$  that is a linear function of two predictor variables,  $X_1$  and  $X_2$ .  $$\\begin{aligned} \nY   \\sim \\mathcal{N}(\\mu, \\sigma^2) \\\\\n\\mu  = \\alpha + \\beta_1 X_1 + \\beta_2 X_2\n\\end{aligned}$$  where  $\\alpha$  is the intercept, and  $\\beta_i$  is the coefficient for covariate  $X_i$ , while  $\\sigma$  represents the observation error. Since we are constructing a Bayesian model, the unknown variables in the model must be assigned a prior distribution. We choose zero-mean normal priors with variance of 100 for both regression coefficients, which corresponds to  weak  information regarding the true parameter values. We choose a half-normal distribution (normal distribution bounded at zero) as the prior for  $\\sigma$ .  $$\\begin{aligned} \n\\alpha  \\sim \\mathcal{N}(0, 100) \\\\\n\\beta_i  \\sim \\mathcal{N}(0, 100) \\\\\n\\sigma  \\sim \\lvert\\mathcal{N}(0, 1){\\rvert}\n\\end{aligned}$$  Generating data  We can simulate some artificial data from this model using only NumPy's  random  module, and then use PyMC3 to try to recover the corresponding parameters. We are intentionally generating the data to closely correspond the PyMC3 model structure.  import numpy as np\nimport matplotlib.pyplot as plt\n\n# Initialize random number generator\nnp.random.seed(123)\n\n# True parameter values\nalpha, sigma = 1, 1\nbeta = [1, 2.5]\n\n# Size of dataset\nsize = 100\n\n# Predictor variable\nX1 = np.random.randn(size)\nX2 = np.random.randn(size) * 0.2\n\n# Simulate outcome variable\nY = alpha + beta[0]*X1 + beta[1]*X2 + np.random.randn(size)*sigma  Here is what the simulated data look like. We use the  pylab  module from the plotting library matplotlib.   %matplotlib inline \n\nfig, axes = plt.subplots(1, 2, sharex=True, figsize=(10,4))\naxes[0].scatter(X1, Y)\naxes[1].scatter(X2, Y)\naxes[0].set_ylabel('Y'); axes[0].set_xlabel('X1'); axes[1].set_xlabel('X2');   Model Specification  Specifying this model in PyMC3 is straightforward because the syntax is as close to the statistical notation. For the most part, each line of Python code corresponds to a line in the model notation above.   First, we import the components we will need from PyMC.  from pymc3 import Model, Normal, HalfNormal  Now we build our model, which we will present in full first, then explain each part line-by-line.  basic_model = Model()\n\nwith basic_model:\n\n    # Priors for unknown model parameters\n    alpha = Normal('alpha', mu=0, sd=10)\n    beta = Normal('beta', mu=0, sd=10, shape=2)\n    sigma = HalfNormal('sigma', sd=1)\n\n    # Expected value of outcome\n    mu = alpha + beta[0]*X1 + beta[1]*X2\n\n    # Likelihood (sampling distribution) of observations\n    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)  The first line,  basic_model = Model()  creates a new  Model  object which is a container for the model random variables.  Following instantiation of the model, the subsequent specification of the model components is performed inside a   with  statement:  with basic_model:  This creates a  context manager , with our  basic_model  as the context, that includes all statements until the indented block ends. This means all PyMC3 objects introduced in the indented code block below the  with  statement are added to the model behind the scenes. Absent this context manager idiom, we would be forced to manually associate each of the variables with  basic_model  right after we create them. If you try to create a new random variable without a  with model:  statement, it will raise an error since there is no obvious model for the variable to be added to.  The first three statements in the context manager:  alpha = Normal('alpha', mu=0, sd=10)\nbeta = Normal('beta', mu=0, sd=10, shape=2)\nsigma = HalfNormal('sigma', sd=1)  create a  stochastic  random variables with a Normal prior distributions for the regression coefficients with a mean of 0 and standard deviation of 10 for the regression coefficients, and a half-normal distribution for the standard deviation of the observations,  $\\sigma$ . These are stochastic because their values are partly determined by its parents in the dependency graph of random variables, which for priors are simple constants, and partly random (or stochastic).   We call the  Normal  constructor to create a random variable to use as a normal prior. The first argument is always the  name  of the random variable, which should almost always match the name of the Python variable being assigned to, since it sometimes used to retrieve the variable from the model for summarizing output. The remaining required arguments for a stochastic object are the parameters, in this case  mu , the mean, and  sd , the standard deviation, which we assign hyperparameter values for the model. In general, a distribution's parameters are values that determine the location, shape or scale of the random variable, depending on the parameterization of the distribution. Most commonly used distributions, such as  Beta ,  Exponential ,  Categorical ,  Gamma ,  Binomial  and many others, are available in PyMC3.  The  beta  variable has an additional  shape  argument to denote it as a vector-valued parameter of size 2. The  shape  argument is available for all distributions and specifies the length or shape of the random variable, but is optional for scalar variables, since it defaults to a value of one. It can be an integer, to specify an array, or a tuple, to specify a multidimensional array ( e.g.   shape=(5,7)  makes random variable that takes on 5 by 7 matrix values).   Detailed notes about distributions, sampling methods and other PyMC3 functions are available via the  help  function.  help(Normal) #try help(Model), help(Uniform) or help(basic_model)  Help on class Normal in module pymc3.distributions.continuous:\n\nclass Normal(pymc3.distributions.distribution.Continuous)\n |  Normal log-likelihood.\n |  \n |  .. math::\night\\}\n |  \n |  Parameters\n |  ----------\n |  mu : float\n |      Mean of the distribution.\n |  tau : float\n |      Precision of the distribution, which corresponds to\n |      :math:`1/\\sigma^2` (tau   0).\n |  sd : float\n |      Standard deviation of the distribution. Alternative parameterization.\n |  \n |  .. note::\n |  - :math:`E(X) = \\mu`\n |  - :math:`Var(X) = 1/        au`\n |  \n |  Method resolution order:\n |      Normal\n |      pymc3.distributions.distribution.Continuous\n |      pymc3.distributions.distribution.Distribution\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __init__(self, mu=0.0, tau=None, sd=None, *args, **kwargs)\n |  \n |  logp(self, value)\n |  \n |  random(self, point=None, size=None, repeat=None)\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __getnewargs__(self)\n |  \n |  default(self)\n |  \n |  get_test_val(self, val, defaults)\n |  \n |  getattr_value(self, val)\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  dist(*args, **kwargs) from builtins.type\n |  \n |  ----------------------------------------------------------------------\n |  Static methods inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __new__(cls, name, *args, **kwargs)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from pymc3.distributions.distribution.Distribution:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)  Having defined the priors, the next statement creates the expected value  mu  of the outcomes, specifying the linear relationship:  mu = alpha + beta[0]*X1 + beta[1]*X2  This creates a  deterministic  random variable, which implies that its value is  completely  determined by its parents' values. That is, there is no uncertainty beyond that which is inherent in the parents' values. Here,  mu  is just the sum of the intercept  alpha  and the two products of the coefficients in  beta  and the predictor variables, whatever their values may be.   PyMC3 random variables and data can be arbitrarily added, subtracted, divided, multiplied together and indexed-into to create new random variables. This allows for great model expressivity. Many common mathematical functions like  sum ,  sin ,  exp  and linear algebra functions like  dot  (for inner product) and  inv  (for inverse) are also provided.   The final line of the model, defines  Y_obs , the sampling distribution of the outcomes in the dataset.  Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=Y)  This is a special case of a stochastic variable that we call an  observed stochastic , and represents the data likelihood of the model. It is identical to a standard stochastic, except that its  observed  argument, which passes the data to the variable, indicates that the values for this variable were observed, and should not be changed by any fitting algorithm applied to the model. The data can be passed in the form of either a  numpy.ndarray  or  pandas.DataFrame  object.  Notice that, unlike for the priors of the model, the parameters for the normal distribution of  Y_obs  are not fixed values, but rather are the deterministic object  mu  and the stochastic  sigma . This creates parent-child relationships between the likelihood and these two variables.  Model fitting  Having completely specified our model, the next step is to obtain posterior estimates for the unknown variables in the model. Ideally, we could calculate the posterior estimates analytically, but for most non-trivial models, this is not feasible. We will consider two approaches, whose appropriateness depends on the structure of the model and the goals of the analysis: finding the  maximum a posteriori  (MAP) point using optimization methods, and computing summaries based on samples drawn from the posterior distribution using Markov Chain Monte Carlo (MCMC) sampling methods.  Maximum a posteriori methods  The  maximum a posteriori (MAP)  estimate for a model, is the mode of the posterior distribution and is generally found using numerical optimization methods. This is often fast and easy to do, but only gives a point estimate for the parameters and can be biased if the mode isn't representative of the distribution. PyMC3 provides this functionality with the  find_MAP  function.  Below we find the MAP for our original model. The MAP is returned as a parameter  point , which is always represented by a Python dictionary of variable names to NumPy arrays of parameter values.   from pymc3 import find_MAP\n\nmap_estimate = find_MAP(model=basic_model)\n\nprint(map_estimate)  {'alpha': array(0.9065985757590524), 'sigma_log': array(-0.03278147323479127), 'beta': array([ 0.94848596,  2.60705517])}  By default,  find_MAP  uses the Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (BFGS) optimization algorithm to find the maximum of the log-posterior but also allows selection of other optimization algorithms from the  scipy.optimize  module. For example, below we use Powell's method to find the MAP.  from scipy import optimize\n\nmap_estimate = find_MAP(model=basic_model, fmin=optimize.fmin_powell)\n\nprint(map_estimate)  {'alpha': array(0.9090521896381477), 'sigma_log': array(-0.030009775415224028), 'beta': array([ 0.95140146,  2.61437458])}  It is important to note that the MAP estimate is not always reasonable, especially if the mode is at an extreme. This can be a subtle issue; with high dimensional posteriors, one can have areas of extremely high density but low total probability because the volume is very small. This will often occur in hierarchical models with the variance parameter for the random effect. If the individual group means are all the same, the posterior will have near infinite density if the scale parameter for the group means is almost zero, even though the probability of such a small scale parameter will be small since the group means must be extremely close together.   Most techniques for finding the MAP estimate also only find a  local  optimum (which is often good enough), but can fail badly for multimodal posteriors if the different modes are meaningfully different.  Sampling methods  Though finding the MAP is a fast and easy way of obtaining estimates of the unknown model parameters, it is limited because there is no associated estimate of uncertainty produced with the MAP estimates. Instead, a simulation-based approach such as Markov chain Monte Carlo (MCMC) can be used to obtain a Markov chain of values that, given the satisfaction of certain conditions, are indistinguishable from samples from the posterior distribution.   To conduct MCMC sampling to generate posterior samples in PyMC3, we specify a  step method  object that corresponds to a particular MCMC algorithm, such as Metropolis, Slice sampling, or the No-U-Turn Sampler (NUTS). PyMC3's  step_methods  submodule contains the following samplers:  NUTS ,  Metropolis ,  Slice ,  HamiltonianMC , and  BinaryMetropolis . These step methods can be assigned manually, or assigned automatically by PyMC3. Auto-assignment is based on the attributes of each variable in the model. In general:   Binary variables will be assigned to  BinaryMetropolis  Discrete variables will be assigned to  Metropolis  Continuous variables will be assigned to  NUTS   Auto-assignment can be overriden for any subset of variables by specifying them manually prior to sampling.  Gradient-based sampling methods  PyMC3 has the standard sampling algorithms like adaptive Metropolis-Hastings and adaptive slice sampling, but PyMC3's most capable step method is the No-U-Turn Sampler. NUTS is especially useful on models that have many continuous parameters, a situation where other MCMC algorithms work very slowly. It takes advantage of information about where regions of higher probability are, based on the gradient of the log posterior-density. This helps it achieve dramatically faster convergence on large problems than traditional sampling methods achieve. PyMC3 relies on Theano to analytically compute model gradients via automatic differentiation of the posterior density. NUTS also has several self-tuning strategies for adaptively setting the tunable parameters of Hamiltonian Monte Carlo. For random variables that are undifferentiable (namely, discrete variables) NUTS cannot be used, but it may still be used on the differentiable variables in a model that contains undifferentiable variables.   NUTS requires a scaling matrix parameter, which is analogous to the variance parameter for the jump proposal distribution in Metropolis-Hastings, although NUTS uses it somewhat differently. The matrix gives the rough shape of the distribution so that NUTS does not make jumps that are too large in some directions and too small in other directions. It is important to set this scaling parameter to a reasonable value to facilitate efficient sampling. This is especially true for models that have many unobserved stochastic random variables or models with highly non-normal posterior distributions. Poor scaling parameters will slow down NUTS significantly, sometimes almost stopping it completely. A reasonable starting point for sampling can also be important for efficient sampling, but not as often.  Fortunately NUTS can often make good guesses for the scaling parameters. If you pass a point in parameter space (as a dictionary of variable names to parameter values, the same format as returned by  find_MAP ) to NUTS, it will look at the local curvature of the log posterior-density (the diagonal of the Hessian matrix) at that point to make a guess for a good scaling vector, which often results in a good value. The MAP estimate is often a good point to use to initiate sampling. It is also possible to supply your own vector or scaling matrix to NUTS, though this is a more advanced use. If you wish to modify a Hessian at a specific point to use as your scaling matrix or vector, you can use  find_hessian  or  find_hessian_diag .  For our basic linear regression example in  basic_model , we will use NUTS to sample 2000 draws from the posterior using the MAP as the starting point and scaling point. This must also be performed inside the context of the model.  from pymc3 import NUTS, sample\n\nwith basic_model:\n\n    # obtain starting values via MAP\n    start = find_MAP(fmin=optimize.fmin_powell)\n\n    # draw 2000 posterior samples\n    trace = sample(2000, start=start)   Assigned  class 'pymc3.step_methods.nuts.NUTS'  to alpha\nAssigned  class 'pymc3.step_methods.nuts.NUTS'  to beta\nAssigned  class 'pymc3.step_methods.nuts.NUTS'  to sigma_log\n [-----------------100%-----------------] 2000 of 2000 complete in 1.4 sec  The  sample  function runs the step method(s) assigned (or passed) to it for the given number of iterations and returns a  Trace  object containing the samples collected, in the order they were collected. The  trace  object can be queried in a similar way to a  dict  containing a map from variable names to  numpy.array s. The first dimension of the array is the sampling index and the later dimensions match the shape of the variable. We can see the last 5 values for the  alpha  variable as follows:  trace['alpha'][-5:]  array([ 0.80387719,  0.88728378,  1.06832995,  0.71061793,  0.84123803])  If we wanted to use the slice sampling algorithm to  sigma  instead of NUTS (which was assigned automatically), we could have specified this as the  step  argument for  sample .  from pymc3 import Slice\n\nwith basic_model:\n\n    # obtain starting values via MAP\n    start = find_MAP(fmin=optimize.fmin_powell)\n\n    # instantiate sampler\n    step = Slice(vars=[sigma]) \n\n    # draw 5000 posterior samples\n    trace = sample(5000, step=step, start=start)     Assigned  class 'pymc3.step_methods.nuts.NUTS'  to alpha\nAssigned  class 'pymc3.step_methods.nuts.NUTS'  to beta\n [-----------------100%-----------------] 5000 of 5000 complete in 13.3 sec  Posterior analysis  PyMC3  provides plotting and summarization functions for inspecting the sampling output. A simple posterior plot can be created using  traceplot .  from pymc3 import traceplot\n\ntraceplot(trace);   The left column consists of a smoothed histogram (using kernel density estimation) of the marginal posteriors of each stochastic random variable while the right column contains the samples of the Markov chain plotted in sequential order. The  beta  variable, being vector-valued, produces two histograms and two sample traces, corresponding to both predictor coefficients.  In addition, the  summary  function provides a text-based output of common posterior statistics:  from pymc3 import summary\n\nsummary(trace)  alpha:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.907            0.099            0.001            [0.718, 1.099]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.720          0.839          0.904          0.975          1.103\n\n\nbeta:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.949            0.091            0.001            [0.777, 1.127]\n  2.598            0.510            0.006            [1.647, 3.635]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.774          0.887          0.948          1.010          1.126\n  1.585          2.257          2.602          2.946          3.584\n\n\nsigma_log:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  -0.012           0.070            0.001            [-0.149, 0.124]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -0.147         -0.059         -0.013         0.034          0.127\n\n\nsigma:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.991            0.069            0.001            [0.852, 1.121]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.863          0.943          0.987          1.034          1.136",
            "title": "A Motivating Example: Linear Regression"
        },
        {
            "location": "/getting_started/#case-study-1-stochastic-volatility",
            "text": "We present a case study of stochastic volatility, time varying stock market volatility, to illustrate PyMC3's use in addressing a more realistic problem. The distribution of market returns is highly non-normal, which makes sampling the volatilities significantly more difficult. This example has 400+ parameters so using common sampling algorithms like Metropolis-Hastings would get bogged down, generating highly autocorrelated samples. Instead, we use NUTS, which is dramatically more efficient.  The Model  Asset prices have time-varying volatility (variance of day over day  returns ). In some periods, returns are highly variable, while in others they are very stable. Stochastic volatility models address this with a latent volatility variable, which changes over time. The following model is similar to the one described in the NUTS paper (Hoffman 2014, p. 21).  $$\\begin{aligned} \n  \\sigma  \\sim exp(50) \\\\\n  \\nu  \\sim exp(.1) \\\\\n  s_i  \\sim \\mathcal{N}(s_{i-1}, \\sigma^{-2}) \\\\\n  log(y_i)  \\sim t(\\nu, 0, exp(-2 s_i))\n\\end{aligned}$$  Here,  $y$  is the daily return series which is modeled with a Student-t distribution with an unknown degrees of freedom parameter, and a scale parameter determined by a latent process  $s$ . The individual  $s_i$  are the individual daily log volatilities in the latent log volatility process.   The Data  Our data consist of daily returns of the S P 500 during the 2008 financial crisis.  import pandas as pd\nreturns = pd.read_csv('data/SP500.csv', index_col=0, parse_dates=True)\nprint(len(returns))  400  returns.plot(figsize=(10, 6))\nplt.ylabel('daily returns in %');   Model Specification  As with the linear regression example, specifying the model in PyMC3 mirrors its statistical specification. This model employs several new distributions: the  Exponential  distribution for the  $ \\nu $  and  $\\sigma$  priors, the student-t ( T ) distribution for distribution of returns, and the  GaussianRandomWalk  for the prior for the latent volatilities.     In PyMC3, variables with purely positive priors like  Exponential  are transformed with a log transform. This makes sampling more robust. Behind the scenes, a variable in the unconstrained space (named \"variableName_log\") is added to the model for sampling. In this model this happens behind the scenes for both the degrees of freedom,  nu , and the scale parameter for the volatility process,  sigma , since they both have exponential priors. Variables with priors that constrain them on two sides, like  Beta  or  Uniform , are also transformed to be unconstrained but with a log odds transform.   Although, unlike model specification in PyMC2, we do not typically provide starting points for variables at the model specification stage, we can also provide an initial value for any distribution (called a \"test value\") using the  testval  argument. This overrides the default test value for the distribution (usually the mean, median or mode of the distribution), and is most often useful if some values are illegal and we want to ensure we select a legal one. The test values for the distributions are also used as a starting point for sampling and optimization by default, though this is easily overriden.   The vector of latent volatilities  s  is given a prior distribution by  GaussianRandomWalk . As its name suggests GaussianRandomWalk is a vector valued distribution where the values of the vector form a random normal walk of length n, as specified by the  shape  argument. The scale of the innovations of the random walk,  sigma , is specified in terms of the precision of the normally distributed innovations and can be a scalar or vector.   from pymc3 import Exponential, T, exp, Deterministic\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nwith Model() as sp500_model:\n\n    nu = Exponential('nu', 1./10, testval=5.)\n\n    sigma = Exponential('sigma', 1./.02, testval=.1)\n\n    s = GaussianRandomWalk('s', sigma**-2, shape=len(returns))\n\n    volatility_process = Deterministic('volatility_process', exp(-2*s))\n\n    r = T('r', nu, lam=1/volatility_process, observed=returns['S P500'])  Notice that we transform the log volatility process  s  into the volatility process by  exp(-2*s) . Here,  exp  is a Theano function, rather than the corresponding function in NumPy; Theano provides a large subset of the mathematical functions that NumPy does.  Also note that we have declared the  Model  name  sp500_model  in the first occurrence of the context manager, rather than splitting it into two lines, as we did for the first example.  Fitting  Before we draw samples from the posterior, it is prudent to find a decent starting value by finding a point of relatively high probability. For this model, the full  maximum a posteriori  (MAP) point over all variables is degenerate and has infinite density. But, if we fix  log_sigma  and  nu  it is no longer degenerate, so we find the MAP with respect only to the volatility process  s  keeping  log_sigma  and  nu  constant at their default values (remember that we set  testval=.1  for  sigma ). We use the Limited-memory BFGS (L-BFGS) optimizer, which is provided by the  scipy.optimize  package, as it is more efficient for high dimensional functions and we have 400 stochastic random variables (mostly from  s ).  To do the sampling, we do a short initial run to put us in a volume of high probability, then start again at the new starting point.  trace[-1]  gives us the last point in the sampling trace. NUTS will recalculate the scaling parameters based on the new point, and in this case it leads to faster sampling due to better scaling.  import scipy\nwith sp500_model:\n    start = find_MAP(vars=[s], fmin=scipy.optimize.fmin_l_bfgs_b)\n\n    step = NUTS(scaling=start)\n    trace = sample(100, step, progressbar=False)\n\n    # Start next run at the last sampled position.\n    step = NUTS(scaling=trace[-1], gamma=.25)\n    trace = sample(2000, step, start=trace[-1], progressbar=False, njobs=4)  We can check our samples by looking at the traceplot for  nu  and  sigma .  traceplot(trace, [nu, sigma]);   Finally we plot the distribution of volatility paths by plotting many of our sampled volatility paths on the same graph. Each is rendered partially transparent (via the  alpha  argument in Matplotlib's  plot  function) so the regions where many paths overlap are shaded more darkly.  fig, ax = plt.subplots(figsize=(15, 8))\nreturns.plot(ax=ax)\nax.plot(returns.index, 1/np.exp(trace['s',::30].T), 'r', alpha=.03);\nax.set(title='volatility_process', xlabel='time', ylabel='volatility');\nax.legend(['S P500', 'stochastic volatility process'])  matplotlib.legend.Legend at 0x7f6055033b00    As you can see, the model correctly infers the increase in volatility during the 2008 financial crash. Moreover, note that this model is quite complex because of its high dimensionality and dependency-structure in the random walk distribution. NUTS as implemented in PyMC3, however, correctly infers the posterior distribution with ease.",
            "title": "Case study 1: Stochastic volatility"
        },
        {
            "location": "/getting_started/#case-study-2-coal-mining-disasters",
            "text": "Consider the following time series of recorded coal mining disasters in the UK from 1851 to 1962 (Jarrett, 1979). The number of disasters is thought to have been affected by changes in safety regulations during this period. Unfortunately, we also have pair of years with missing data, identified as missing by a NumPy MaskedArray using -999 as the marker value.   Next we will build a model for this series and attempt to estimate when the change occurred. At the same time, we will see how to handle missing data, use multiple samplers and sample from discrete random variables.   disaster_data = np.ma.masked_values([4, 5, 4, 0, 1, 4, 3, 4, 0, 6, 3, 3, 4, 0, 2, 6,\n                            3, 3, 5, 4, 5, 3, 1, 4, 4, 1, 5, 5, 3, 4, 2, 5,\n                            2, 2, 3, 4, 2, 1, 3, -999, 2, 1, 1, 1, 1, 3, 0, 0,\n                            1, 0, 1, 1, 0, 0, 3, 1, 0, 3, 2, 2, 0, 1, 1, 1,\n                            0, 1, 0, 1, 0, 0, 0, 2, 1, 0, 0, 0, 1, 1, 0, 2,\n                            3, 3, 1, -999, 2, 1, 1, 1, 1, 2, 4, 2, 0, 0, 1, 4,\n                            0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1], value=-999)\nyear = np.arange(1851, 1962)\n\nplot(year, disaster_data, 'o', markersize=8);\nylabel( Disaster count )\nxlabel( Year )  matplotlib.text.Text at 0x7f605096dcc0    Occurrences of disasters in the time series is thought to follow a Poisson process with a large rate parameter in the early part of the time series, and from one with a smaller rate in the later part. We are interested in locating the change point in the series, which perhaps is related to changes in mining safety regulations.  In our model,   $$ \n\\begin{aligned}  \n  D_t  \\sim \\text{Pois}(r_t), r_t= \\begin{cases} \n   l,   \\text{if } t \\lt s \\\\\n   r,   \\text{if } t \\ge s \n   \\end{cases} \\\\\n  s  \\sim \\text{Unif}(t_l, t_h)\\\\         \n  e  \\sim \\text{exp}(1)\\\\\n  l  \\sim \\text{exp}(1)    \n\\end{aligned}\n$$ \n the parameters are defined as follows: \n   *  $D_t$ : The number of disasters in year  $t$ \n   *  $r_t$ : The rate parameter of the Poisson distribution of disasters in year  $t$ .\n   *  $s$ : The year in which the rate parameter changes (the switchpoint).\n   *  $e$ : The rate parameter before the switchpoint  $s$ .\n   *  $l$ : The rate parameter after the switchpoint  $s$ .\n   *  $t_l$ ,  $t_h$ : The lower and upper boundaries of year  $t$ .  This model is built much like our previous models. The major differences are the introduction of discrete variables with the Poisson and discrete-uniform priors and the novel form of the deterministic random variable  rate .  from pymc3 import DiscreteUniform, Poisson, switch\n\nwith Model() as disaster_model:\n\n    switchpoint = DiscreteUniform('switchpoint', lower=year.min(), upper=year.max(), testval=1900)\n\n    # Priors for pre- and post-switch rates number of disasters\n    early_rate = Exponential('early_rate', 1)\n    late_rate = Exponential('late_rate', 1)\n\n    # Allocate appropriate Poisson rates to years before and after current\n    rate = switch(switchpoint  = year, early_rate, late_rate)\n\n    disasters = Poisson('disasters', rate, observed=disaster_data)  The logic for the rate random variable,  rate = switch(switchpoint  = year, early_rate, late_rate)  is implemented using  switch , a Theano function that works like an if statement. It uses the first argument to switch between the next two arguments.  Missing values are handled transparently by passing a  MaskedArray  or a  pandas.DataFrame  with NaN values to the  observed  argument when creating an observed stochastic random variable. Behind the scenes, another random variable,  disasters.missing_values  is created to model the missing values. All we need to do to handle the missing values is ensure we sample this random variable as well.  Unfortunately because they are discrete variables and thus have no meaningful gradient, we cannot use NUTS for sampling  switchpoint  or the missing disaster observations. Instead, we will sample using a  Metroplis  step method, which implements adaptive Metropolis-Hastings, because it is designed to handle discrete values.  We sample with both samplers at once by passing them to the  sample  function in a list. Each new sample is generated by first applying  step1  then  step2 .  from pymc3 import Metropolis \n\nwith disaster_model:\n    step1 = NUTS([early_rate, late_rate])\n\n    # Use Metropolis for switchpoint, and missing values since it accommodates discrete variables\n    step2 = Metropolis([switchpoint, disasters.missing_values[0]] )\n\n    trace = sample(10000, step=[step1, step2])   [-----------------100%-----------------] 10000 of 10000 complete in 8.8 sec  In the trace plot below we can see that there's about a 10 year span that's plausible for a significant change in safety, but a 5 year span that contains most of the probability mass. The distribution is jagged because of the jumpy relationship between the year switchpoint and the likelihood  and not due to sampling error.  traceplot(trace);",
            "title": "Case study 2: Coal mining disasters"
        },
        {
            "location": "/getting_started/#arbitrary-deterministics",
            "text": "Due to its reliance on Theano, PyMC3 provides many mathematical functions and operators for transforming random variables into new random variables. However, the library of functions in Theano is not exhaustive, therefore Theano and PyMC3 provide functionality for creating arbitrary Theano functions in pure Python, and including these functions in PyMC models. This is supported with the  as_op  function decorator.  Theano needs to know the types of the inputs and outputs of a function, which are specified for  as_op  by  itypes  for inputs and  otypes  for outputs. The Theano documentation includes  an overview of the available types .  import theano.tensor as T \nfrom theano.compile.ops import as_op\n\n@as_op(itypes=[T.lscalar], otypes=[T.lscalar])\ndef crazy_modulo3(value):\n    if value   0: \n        return value % 3\n    else :\n        return (-value + 1) % 3\n\nwith Model() as model_deterministic:\n    a = Poisson('a', 1)\n    b = crazy_modulo3(a)  An important drawback of this approach is that it is not possible for  theano  to inspect these functions in order to compute the gradient required for the Hamiltonian-based samplers. Therefore, it is not possible to use the HMC or NUTS samplers for a model that uses such an operator. However, it is possible to add a gradient if we inherit from  theano.Op  instead of using  as_op . The PyMC example set includes  a more elaborate example of the usage of  as_op .",
            "title": "Arbitrary deterministics"
        },
        {
            "location": "/getting_started/#arbitrary-distributions",
            "text": "Similarly, the library of statistical distributions in PyMC3 is not exhaustive, but PyMC allows for the creation of user-defined functions for an arbitrary probability distribution. For simple statistical distributions, the  DensityDist  function takes as an argument any function that calculates a log-probability  $log(p(x))$ . This function may employ other random variables in its calculation. Here is an example inspired by a blog post by Jake Vanderplas on which priors to use for a linear regression (Vanderplas, 2014).   import theano.tensor as T\nfrom pymc3 import DensityDist, Uniform\n\nwith Model() as model:\n    alpha = Uniform('intercept', -100, 100)\n\n    # Create custom densities\n    beta = DensityDist('beta', lambda value: -1.5 * T.log(1 + value**2), testval=0)\n    eps = DensityDist('eps', lambda value: -T.log(T.abs_(value)), testval=1)\n\n    # Create likelihood\n    like = Normal('y_est', mu=alpha + beta * X, sd=eps, observed=Y)  For more complex distributions, one can create a subclass of  Continuous  or  Discrete  and provide the custom  logp  function, as required. This is how the built-in distributions in PyMC are specified. As an example, fields like psychology and astrophysics have complex likelihood functions for a particular process that may require numerical approximation. In these cases, it is impossible to write the function in terms of predefined theano operators and we must use a custom theano operator using  as_op  or inheriting from  theano.Op .   Implementing the  beta  variable above as a  Continuous  subclass is shown below, along with a sub-function using the  as_op  decorator, though this is not strictly necessary.  from pymc3.distributions import Continuous\n\nclass Beta(Continuous):\n    def __init__(self, mu, *args, **kwargs):\n        super(Beta, self).__init__(*args, **kwargs)\n        self.mu = mu\n        self.mode = mu\n\n    def logp(self, value):\n        mu = self.mu\n        return beta_logp(value - mu)\n\n@as_op(itypes=[T.dscalar], otypes=[T.dscalar])\ndef beta_logp(value):\n    return -1.5 * np.log(1 + (value)**2)\n\n\nwith Model() as model:\n    beta = Beta('slope', mu=0, testval=0)",
            "title": "Arbitrary distributions"
        },
        {
            "location": "/getting_started/#generalized-linear-models",
            "text": "Generalized Linear Models (GLMs) are a class of flexible models that are widely used to estimate regression relationships between a single outcome variable and one or multiple predictors. Because these models are so common,  PyMC3  offers a  glm  submodule that allows flexible creation of various GLMs with an intuitive  R -like syntax that is implemented via the  patsy  module.  The  glm  submodule requires data to be included as a  pandas   DataFrame . Hence, for our linear regression example:  # Convert X and Y to a pandas DataFrame\nimport pandas \n\ndf = pandas.DataFrame({'x1': X1, 'x2': X2, 'y': Y})  The model can then be very concisely specified in one line of code.  from pymc3.glm import glm\n\nwith Model() as model_glm:\n    glm('y ~ x1 + x2', df)\n    trace = sample(5000)  Assigned  class 'pymc3.step_methods.nuts.NUTS'  to Intercept\nAssigned  class 'pymc3.step_methods.nuts.NUTS'  to x1\nAssigned  class 'pymc3.step_methods.nuts.NUTS'  to x2\nAssigned  class 'pymc3.step_methods.nuts.NUTS'  to sd_log\n [-----------------100%-----------------] 5000 of 5000 complete in 4.3 sec  The error distribution, if not specified via the  family  argument, is assumed to be normal. In the case of logistic regression, this can be modified by passing in a  Binomial  family object.  from pymc3.glm.families import Binomial\n\ndf_logistic = pandas.DataFrame({'x1': X1, 'y': Y   np.median(Y)})\n\nwith Model() as model_glm_logistic:\n    glm('y ~ x1', df_logistic, family=Binomial())",
            "title": "Generalized Linear Models"
        },
        {
            "location": "/getting_started/#backends",
            "text": "PyMC3  has support for different ways to store samples during and after sampling, called backends, including in-memory (default), text file, and SQLite. These can be found in  pymc.backends :  By default, an in-memory  ndarray  is used but if the samples would get too large to be held in memory we could use the  sqlite  backend:  from pymc3.backends import SQLite\n\nwith Model() as model_glm_logistic:\n    glm('y ~ x1', df_logistic, family=Binomial())\n\n    backend = SQLite('trace.sqlite')\n    start = find_MAP()\n    step = NUTS(scaling=start)\n    trace = sample(5000, step=step, start=start, trace=backend)   [-----------------100%-----------------] 5000 of 5000 complete in 4.5 sec  summary(trace, vars=['x1'])  x1:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  -1.128           211.811          1.126            [-0.917, 0.916]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  -0.908         -0.216         0.002          0.223          0.926  The stored trace can then later be loaded using the  load  command:  from pymc3.backends.sqlite import load\n\nwith basic_model:\n    trace_loaded = load('trace.sqlite')  More information about  backends  can be found in the docstring of  pymc.backends .",
            "title": "Backends"
        },
        {
            "location": "/getting_started/#discussion",
            "text": "Probabilistic programming is an emerging paradigm in statistical learning, of which Bayesian modeling is an important sub-discipline. The signature characteristics of probabilistic programming--specifying variables as probability distributions and conditioning variables on other variables and on observations--makes it a powerful tool for building models in a variety of settings, and over a range of model complexity. Accompanying the rise of probabilistic programming has been a burst of innovation in fitting methods for Bayesian models that represent notable improvement over existing MCMC methods. Yet, despite this expansion, there are few software packages available that have kept pace with the methodological innovation, and still fewer that allow non-expert users to implement models.  PyMC3 provides a probabilistic programming platform for quantitative researchers to implement statistical models flexibly and succinctly. A large library of statistical distributions and several pre-defined fitting algorithms allows users to focus on the scientific problem at hand, rather than the implementation details of Bayesian modeling. The choice of Python as a development language, rather than a domain-specific language, means that PyMC3 users are able to work interactively to build models, introspect model objects, and debug or profile their work, using a dynamic, high-level programming language that is easy to learn. The modular, object-oriented design of PyMC3 means that adding new fitting algorithms or other features is straightforward. In addition, PyMC3 comes with several features not found in most other packages, most notably Hamiltonian-based samplers as well as automatical transforms of constrained random variables which is only offered by STAN. Unlike STAN, however, PyMC3 supports discrete variables as well as non-gradient based sampling algorithms like Metropolis-Hastings and Slice sampling.  Development of PyMC3 is an ongoing effort and several features are planned for future versions. Most notably, variational inference techniques are often more efficient than MCMC sampling, at the cost of generalizability. More recently, however, black-box variational inference algorithms have been developed, such as automatic differentiation variational inference (ADVI; Kucukelbir et al., in prep). This algorithm is slated for addition to PyMC3. As an open-source scientific computing toolkit, we encourage researchers developing new fitting algorithms for Bayesian models to provide reference implementations in PyMC3. Since samplers can be written in pure Python code, they can be implemented generally to make them work on arbitrary PyMC3 models, giving authors a larger audience to put their methods into use.",
            "title": "Discussion"
        },
        {
            "location": "/getting_started/#references",
            "text": "Patil, A., D. Huard and C.J. Fonnesbeck. (2010) PyMC: Bayesian Stochastic Modelling in Python. Journal of Statistical Software, 35(4), pp. 1-81  Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley, D., and Bengio, Y. (2012) \u201cTheano: new features and speed improvements\u201d. NIPS 2012 deep learning workshop.  Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010) \u201cTheano: A CPU and GPU Math Expression Compiler\u201d. Proceedings of the Python for Scientific Computing Conference (SciPy) 2010. June 30 - July 3, Austin, TX  Lunn, D.J., Thomas, A., Best, N., and Spiegelhalter, D. (2000) WinBUGS -- a Bayesian modelling framework: concepts, structure, and extensibility. Statistics and Computing, 10:325--337.  Neal, R.M. Slice sampling. Annals of Statistics. (2003). doi:10.2307/3448413.  van Rossum, G. The Python Library Reference Release 2.6.5., (2010). URL http://docs.python.org/library/.  Duane, S., Kennedy, A. D., Pendleton, B. J., and Roweth, D. (1987) \u201cHybrid Monte Carlo\u201d, Physics Letters, vol. 195, pp. 216-222.  Stan Development Team. (2014). Stan: A C++ Library for Probability and Sampling, Version 2.5.0.   http://mc-stan.org.   Gamerman, D. Markov Chain Monte Carlo: statistical simulation for Bayesian inference. Chapman and Hall, 1997.  Hoffman, M. D.,   Gelman, A. (2014). The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. The Journal of Machine Learning Research, 30.  Kucukelbir A, Ranganath R, Gelman A, and Blei DM. Automatic Variational Inference in Stan http://arxiv.org/abs/1506.03431, in prep.  Vanderplas, Jake. \"Frequentism and Bayesianism IV: How to be a Bayesian in Python.\" Pythonic Perambulations. N.p., 14 Jun 2014. Web. 27 May. 2015.  https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/ .  R.G. Jarrett. A note on the intervals between coal mining disasters. Biometrika, 66:191\u2013193, 1979.",
            "title": "References"
        },
        {
            "location": "/BEST/",
            "text": "Bayesian Estimation Supersedes the T-Test\n\nThis model replicates the example used in:\nKruschke, John. (2012) Bayesian estimation supersedes the t test. Journal of Experimental Psychology: General.\n\nThe original pymc2 implementation was written by Andrew Straw and can be found here: https://github.com/strawlab/best\n\nPorted to PyMC3 by Thomas Wiecki (c) 2015.\n\n\n\nimport numpy as np\nimport pymc3 as pm\n\ndrug = (101,100,102,104,102,97,105,105,98,101,100,123,105,103,100,95,102,106,\n        109,102,82,102,100,102,102,101,102,102,103,103,97,97,103,101,97,104,\n        96,103,124,101,101,100,101,101,104,100,101)\nplacebo = (99,101,100,101,102,100,97,101,104,101,102,102,100,105,88,101,100,\n           104,100,100,100,101,102,103,97,101,101,100,101,99,101,100,100,\n           101,100,99,101,100,102,99,100,99)\n\ny1 = np.array(drug)\ny2 = np.array(placebo)\ny = np.concatenate((y1, y2))\n\nmu_m = np.mean( y )\nmu_p = 0.000001 * 1/np.std(y)**2\n\nsigma_low = np.std(y)/1000\nsigma_high = np.std(y)*1000\n\nwith pm.Model() as model:\n    group1_mean = pm.Normal('group1_mean', mu=mu_m, tau=mu_p, testval=y1.mean())\n    group2_mean = pm.Normal('group2_mean', mu=mu_m, tau=mu_p, testval=y2.mean())\n    group1_std = pm.Uniform('group1_std', lower=sigma_low, upper=sigma_high, testval=y1.std())\n    group2_std = pm.Uniform('group2_std', lower=sigma_low, upper=sigma_high, testval=y2.std())\n    nu = pm.Exponential('nu_minus_one', 1/29.) + 1\n\n    lam1 = group1_std**-2\n    lam2 = group2_std**-2\n\n    group1 = pm.T('drug', nu=nu, mu=group1_mean, lam=lam1, observed=y1)\n    group2 = pm.T('placebo', nu=nu, mu=group2_mean, lam=lam2, observed=y2)\n\n    diff_of_means = pm.Deterministic('difference of means', group1_mean - group2_mean)\n    diff_of_stds = pm.Deterministic('difference of stds', group1_std - group2_std)\n    effect_size = pm.Deterministic('effect size', diff_of_means / pm.sqrt((group1_std**2 + group2_std**2) / 2))\n\n    step = pm.NUTS()\n    trace = pm.sample(5000, step)\n\n\n\n\n [-----------------100%-----------------] 5000 of 5000 complete in 29.0 sec\n\n\n\n%matplotlib inline\n\n\n\n\npm.traceplot(trace[1000:]);\n\n\n\n\n\n\npm.plots.summary(trace[1000:])\n\n\n\n\ngroup1_mean:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  101.546          0.385            0.007            [100.745, 102.275]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  100.771        101.300        101.549        101.789        102.323\n\n\ngroup2_mean:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  100.526          0.212            0.005            [100.095, 100.912]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  100.101        100.383        100.532        100.667        100.922\n\n\ngroup1_std:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  2.060            0.422            0.010            [1.266, 2.882]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  1.349          1.756          2.027          2.315          2.998\n\n\ngroup2_std:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.053            0.213            0.004            [0.689, 1.481]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.705          0.905          1.029          1.184          1.524\n\n\nnu_minus_one:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.848            0.486            0.014            [0.033, 1.789]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.156          0.498          0.772          1.098          2.026\n\n\ndifference of means:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.020            0.441            0.008            [0.153, 1.838]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.168          0.731          1.010          1.320          1.861\n\n\ndifference of stds:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  1.007            0.435            0.008            [0.197, 1.899]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.230          0.699          0.989          1.287          1.946\n\n\neffect size:\n\n  Mean             SD               MC Error         95% HPD interval\n  -------------------------------------------------------------------\n\n  0.643            0.302            0.006            [0.080, 1.256]\n\n  Posterior quantiles:\n  2.5            25             50             75             97.5\n  |--------------|==============|==============|--------------|\n\n  0.092          0.438          0.628          0.838          1.280",
            "title": "BEST"
        },
        {
            "location": "/stochastic_volatility/",
            "text": "Stochastic Volatility model\n\n\nimport numpy as np\nimport pymc3 as pm\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nfrom scipy.sparse import csc_matrix\nfrom scipy import optimize\n\n%pylab inline\n\n\n\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n:0: FutureWarning: IPython widgets are experimental and may change in the future.\n\n\n\nAsset prices have time-varying volatility (variance of day over day \nreturns\n). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, Hoffman (2011) p21.\n\n\n$$ \\sigma \\sim Exponential(50) $$\n\n\n$$ \\nu \\sim Exponential(.1) $$\n\n\n$$ s_i \\sim Normal(s_{i-1}, \\sigma^{-2}) $$\n\n\n$$ log(\\frac{y_i}{y_{i-1}}) \\sim t(\\nu, 0, exp(-2 s_i)) $$\n\n\nHere, \n$y$\n is the daily return series and \n$s$\n is the latent log volatility process.\n\n\nBuild Model\n\n\nFirst we load some daily returns of the S\nP 500.\n\n\nn = 400\nreturns = np.genfromtxt(\ndata/SP500.csv\n)[-n:]\nreturns[:5]\n\n\n\n\narray([-0.00637 , -0.004045, -0.02547 ,  0.005102, -0.047733])\n\n\n\nplt.plot(returns)\n\n\n\n\n[\nmatplotlib.lines.Line2D at 0xaeaec1cc\n]\n\n\n\n\n\nSpecifying the model in pymc3 mirrors its statistical specification. \n\n\nmodel = pm.Model()\nwith model:\n    sigma = pm.Exponential('sigma', 1./.02, testval=.1)\n\n    nu = pm.Exponential('nu', 1./10)\n    s = GaussianRandomWalk('s', sigma**-2, shape=n)\n\n    r = pm.T('r', nu, lam=pm.exp(-2*s), observed=returns)\n\n\n\n\nFit Model\n\n\nFor this model, the full maximum a posteriori (MAP) point is degenerate and has infinite density. However, if we fix \nlog_sigma\n and \nnu\n it is no longer degenerate, so we find the MAP with respect to the volatility process, 's', keeping \nlog_sigma\n and \nnu\n constant at their default values. \n\n\nWe use L-BFGS because it is more efficient for high dimensional functions (\ns\n has n elements).\n\n\nwith model:\n    start = pm.find_MAP(vars=[s], fmin=optimize.fmin_l_bfgs_b)\n\n\n\n\nWe do a short initial run to get near the right area, then start again using a new Hessian at the new starting point to get faster sampling due to better scaling. We do a short run since this is an interactive example.\n\n\nwith model:\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start, gamma=.25)\n    start2 = pm.sample(100, step, start=start)[-1]\n\n    # Start next run at the last sampled position.\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start2, gamma=.55)\n    trace = pm.sample(2000, step, start=start2)\n\n\n\n\n [-----------------100%-----------------] 2001 of 2000 complete in 368.9 sec\n\n/usr/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)\n\n\n\nfigsize(12,6)\npm.traceplot(trace, model.vars[:-1]);\n\n\n\n\n\n\nfigsize(12,6)\ntitle(str(s))\nplot(trace[s][::10].T,'b', alpha=.03);\nxlabel('time')\nylabel('log volatility')\n\n\n\n\nmatplotlib.text.Text at 0xac04168c\n\n\n\n\n\n\nLooking at the returns over time and overlaying the estimated standard deviation we can see how the model tracks the volatility over time.\n\n\nplot(returns)\nplot(np.exp(trace[s][::10].T), 'r', alpha=.03);\nsd = np.exp(trace[s].T)\nplot(-np.exp(trace[s][::10].T), 'r', alpha=.03);\nxlabel('time')\nylabel('returns')\n\n\n\n\nmatplotlib.text.Text at 0xaab3854c\n\n\n\n\n\n\nReferences\n\n\n\n\nHoffman \n Gelman. (2011). \nThe No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\n.",
            "title": "Stochastic Volatility"
        },
        {
            "location": "/stochastic_volatility/#stochastic-volatility-model",
            "text": "import numpy as np\nimport pymc3 as pm\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\n\nfrom scipy.sparse import csc_matrix\nfrom scipy import optimize\n\n%pylab inline  Populating the interactive namespace from numpy and matplotlib\n\n\n:0: FutureWarning: IPython widgets are experimental and may change in the future.  Asset prices have time-varying volatility (variance of day over day  returns ). In some periods, returns are highly variable, while in others very stable. Stochastic volatility models model this with a latent volatility variable, modeled as a stochastic process. The following model is similar to the one described in the No-U-Turn Sampler paper, Hoffman (2011) p21.  $$ \\sigma \\sim Exponential(50) $$  $$ \\nu \\sim Exponential(.1) $$  $$ s_i \\sim Normal(s_{i-1}, \\sigma^{-2}) $$  $$ log(\\frac{y_i}{y_{i-1}}) \\sim t(\\nu, 0, exp(-2 s_i)) $$  Here,  $y$  is the daily return series and  $s$  is the latent log volatility process.",
            "title": "Stochastic Volatility model"
        },
        {
            "location": "/stochastic_volatility/#build-model",
            "text": "First we load some daily returns of the S P 500.  n = 400\nreturns = np.genfromtxt( data/SP500.csv )[-n:]\nreturns[:5]  array([-0.00637 , -0.004045, -0.02547 ,  0.005102, -0.047733])  plt.plot(returns)  [ matplotlib.lines.Line2D at 0xaeaec1cc ]   Specifying the model in pymc3 mirrors its statistical specification.   model = pm.Model()\nwith model:\n    sigma = pm.Exponential('sigma', 1./.02, testval=.1)\n\n    nu = pm.Exponential('nu', 1./10)\n    s = GaussianRandomWalk('s', sigma**-2, shape=n)\n\n    r = pm.T('r', nu, lam=pm.exp(-2*s), observed=returns)",
            "title": "Build Model"
        },
        {
            "location": "/stochastic_volatility/#fit-model",
            "text": "For this model, the full maximum a posteriori (MAP) point is degenerate and has infinite density. However, if we fix  log_sigma  and  nu  it is no longer degenerate, so we find the MAP with respect to the volatility process, 's', keeping  log_sigma  and  nu  constant at their default values.   We use L-BFGS because it is more efficient for high dimensional functions ( s  has n elements).  with model:\n    start = pm.find_MAP(vars=[s], fmin=optimize.fmin_l_bfgs_b)  We do a short initial run to get near the right area, then start again using a new Hessian at the new starting point to get faster sampling due to better scaling. We do a short run since this is an interactive example.  with model:\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start, gamma=.25)\n    start2 = pm.sample(100, step, start=start)[-1]\n\n    # Start next run at the last sampled position.\n    step = pm.NUTS(vars=[s, nu,sigma],scaling=start2, gamma=.55)\n    trace = pm.sample(2000, step, start=start2)   [-----------------100%-----------------] 2001 of 2000 complete in 368.9 sec\n\n/usr/lib/python3.4/importlib/_bootstrap.py:321: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  return f(*args, **kwds)  figsize(12,6)\npm.traceplot(trace, model.vars[:-1]);   figsize(12,6)\ntitle(str(s))\nplot(trace[s][::10].T,'b', alpha=.03);\nxlabel('time')\nylabel('log volatility')  matplotlib.text.Text at 0xac04168c    Looking at the returns over time and overlaying the estimated standard deviation we can see how the model tracks the volatility over time.  plot(returns)\nplot(np.exp(trace[s][::10].T), 'r', alpha=.03);\nsd = np.exp(trace[s].T)\nplot(-np.exp(trace[s][::10].T), 'r', alpha=.03);\nxlabel('time')\nylabel('returns')  matplotlib.text.Text at 0xaab3854c",
            "title": "Fit Model"
        },
        {
            "location": "/stochastic_volatility/#references",
            "text": "Hoffman   Gelman. (2011).  The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo .",
            "title": "References"
        },
        {
            "location": "/hierarchical/",
            "text": "%pylab --no-import-all inline\n\nfrom pymc3 import *\nimport theano.tensor as T\nfrom numpy import random, sum as nsum, ones, concatenate, newaxis, dot, arange\nimport numpy as np\n\nrandom.seed(1)\n\nn_groups = 1\nno_pergroup = 30\nn_observed = no_pergroup * n_groups\nn_group_predictors = 1\nn_predictors = 3\n\ngroup = concatenate([[i] * no_pergroup for i in range(n_groups)])\ngroup_predictors = random.normal(size=(n_groups, n_group_predictors))  # random.normal(size = (n_groups, n_group_predictors))\npredictors = random.normal(size=(n_observed, n_predictors))\n\ngroup_effects_a = random.normal(size=(n_group_predictors, n_predictors))\neffects_a = random.normal(\n    size=(n_groups, n_predictors)) + dot(group_predictors, group_effects_a)\n\ny = nsum(\n    effects_a[group, :] * predictors, 1) + random.normal(size=(n_observed))\n\n\nmodel = Model()\nwith model:\n\n    # m_g ~ N(0, .1)\n    group_effects = Normal(\n        \ngroup_effects\n, 0, .1, shape=(1, n_group_predictors, n_predictors))\n\n    # sg ~ Uniform(.05, 10)\n    sg = Uniform(\nsg\n, .0, 10, testval=2.)\n\n\n    # m ~ N(mg * pg, sg)\n    effects = Normal(\neffects\n,\n                     sum(group_predictors[:, :, newaxis] *\n                     group_effects, 1), sg ** -2,\n                     shape=(n_groups, n_predictors))\n\n    s = Uniform(\ns\n, .01, 10, shape=n_groups)\n\n    g = T.constant(group)\n\n    # y ~ Normal(m[g] * p, s)\n    yd = Normal('y', sum(effects[g] * predictors, 1), s[g] ** -2, observed=y)\n\n    #start = find_MAP()\n    #h = find_hessian(start)\n\n\n    #step = Metropolis()\n    #step = Slice()\n    step = NUTS()\n\n\n\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\nn_predictors\n\n\n\n\n3\n\n\n\ngroup\n\n\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0])\n\n\n\nn_groups\n\n\n\n\n1\n\n\n\nimport pymc3 as pm\npm.debug.eval_univariate?\n\n\n\n\nmodel.free_RVs\n\n\n\n\n[group_effects, sg, effects, s]\n\n\n\nlp = lambda x, y: model.logp({'group_effects': [[[0, 0, 0]]], 'sg': x, 'effects':[[y, y, y]], 's': [.05]})\nlp = np.vectorize(lp)\n\n\n\n\nimport seaborn as sns\n\n\n\n\nx, y = np.mgrid[0.001:0.1:.001, -.05:.05:.001]\nplt.contourf(x, y, lp(x, y))\n\n\n\n\nmatplotlib.contour.QuadContourSet instance at 0x7ffe77d0ccf8\n\n\n\n\n\n\nbest of 3: 6.94 s per loop\n\n\n%prun -q -D sample_nuts.prof sample(3e3, step, model=model)\n\n\n\n\n [-----------------100%-----------------] 3000 of 3000 complete in 74.1 sec \n*** Profile stats marshalled to file u'sample_nuts.prof'.\n\n\n\n%prun -q -D sample.prof sample(3e3, step, start, model=model)\n\n\n\n\n [-----------------100%-----------------] 3000 of 3000 complete in 80.0 sec \n*** Profile stats marshalled to file u'sample.prof'.\n\n\n\ntraceplot(trace);",
            "title": "Hierarchical model"
        },
        {
            "location": "/GLM-linear/",
            "text": "The Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nAuthor: Thomas Wiecki\n\n\nThis tutorial appeared as a post in a small series on Bayesian GLMs on my blog:\n\n\n\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nThis world is far from Normal(ly distributed): Robust Regression in PyMC3\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\nIn this blog post I will talk about:\n\n\n\n\nHow the Bayesian Revolution in many scientific disciplines is hindered by poor usability of current Probabilistic Programming languages.\n\n\nA gentle introduction to Bayesian linear regression and how it differs from the frequentist approach.\n\n\nA preview of \nPyMC3\n (currently in alpha) and its new GLM submodule I wrote to allow creation and estimation of Bayesian GLMs as easy as frequentist GLMs in R.\n\n\n\n\nReady? Lets get started!\n\n\nThere is a huge paradigm shift underway in many scientific disciplines: The Bayesian Revolution. \n\n\nWhile the theoretical benefits of Bayesian over Frequentist stats have been discussed at length elsewhere (see \nFurther Reading\n below), there is a major obstacle that hinders wider adoption -- \nusability\n (this is one of the reasons DARPA wrote out a huge grant to \nimprove Probabilistic Programming\n). \n\n\nThis is mildly ironic because the beauty of Bayesian statistics is their generality. Frequentist stats have a bazillion different tests for every different scenario. In Bayesian land you define your model exactly as you think is appropriate and hit the \nInference Button(TM)\n (i.e. running the magical MCMC sampling algorithm).\n\n\nYet when I ask my colleagues why they use frequentist stats (even though they would like to use Bayesian stats) the answer is that software packages like SPSS or R make it very easy to run all those individuals tests with a single command (and more often then not, they don't know the exact model and inference method being used).\n\n\nWhile there are great Bayesian software packages like \nJAGS\n, \nBUGS\n, \nStan\n and \nPyMC\n, they are written for Bayesians statisticians who know very well what model they want to build. \n\n\nUnfortunately, \n\"the vast majority of statistical analysis is not performed by statisticians\"\n -- so what we really need are tools for \nscientists\n and not for statisticians.\n\n\nIn the interest of putting my code where my mouth is I wrote a submodule for the upcoming \nPyMC3\n that makes construction of Bayesian Generalized Linear Models (GLMs) as easy as Frequentist ones in R.\n\n\nLinear Regression\n\n\nWhile future blog posts will explore more complex models, I will start here with the simplest GLM -- linear regression.\nIn general, frequentists think about Linear Regression as follows:\n\n\n$$ Y = X\\beta + \\epsilon $$\n\n\nwhere \n$Y$\n is the output we want to predict (or \ndependent\n variable), \n$X$\n is our predictor (or \nindependent\n variable), and \n$\\beta$\n are the coefficients (or parameters) of the model we want to estimate. \n$\\epsilon$\n is an error term which is assumed to be normally distributed. \n\n\nWe can then use Ordinary Least Squares or Maximum Likelihood to find the best fitting \n$\\beta$\n.\n\n\nProbabilistic Reformulation\n\n\nBayesians take a probabilistic view of the world and express this model in terms of probability distributions. Our above linear regression can be rewritten to yield:\n\n\n$$ Y \\sim \\mathcal{N}(X \\beta, \\sigma^2) $$\n\n\nIn words, we view \n$Y$\n as a random variable (or random vector) of which each element (data point) is distributed according to a Normal distribution. The mean of this normal distribution is provided by our linear predictor with variance \n$\\sigma^2$\n.\n\n\nWhile this is essentially the same model, there are two critical advantages of Bayesian estimation:\n\n\n\n\nPriors: We can quantify any prior knowledge we might have by placing priors on the paramters. For example, if we think that \n$\\sigma$\n is likely to be small we would choose a prior with more probability mass on low values.\n\n\nQuantifying uncertainty: We do not get a single estimate of \n$\\beta$\n as above but instead a complete posterior distribution about how likely different values of \n$\\beta$\n are. For example, with few data points our uncertainty in \n$\\beta$\n will be very high and we'd be getting very wide posteriors.\n\n\n\n\nBayesian GLMs in PyMC3\n\n\nWith the new GLM module in PyMC3 it is very easy to build this and much more complex models.\n\n\nFirst, lets import the required modules.\n\n\n%matplotlib inline\n\nfrom pymc3 import  *\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\nGenerating data\n\n\nCreate some toy data to play around with and scatter-plot it. \n\n\nEssentially we are creating a regression line defined by intercept and slope and add data points by sampling from a Normal with the mean set to the regression line.\n\n\nsize = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\ndata = dict(x=x, y=y)\n\n\n\n\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x, y, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);\n\n\n\n\n\n\nEstimating the model\n\n\nLets fit a Bayesian linear regression model to this data. As you can see, model specifications in \nPyMC3\n are wrapped in a \nwith\n statement. \n\n\nHere we use the awesome new \nNUTS sampler\n (our Inference Button) to draw 2000 posterior samples.\n\n\nwith Model() as model: # model specifications in PyMC3 are wrapped in a with-statement\n    # Define priors\n    sigma = HalfCauchy('sigma', beta=10, testval=1.)\n    intercept = Normal('Intercept', 0, sd=20)\n    x_coeff = Normal('x', 0, sd=20)\n\n    # Define likelihood\n    likelihood = Normal('y', mu=intercept + x_coeff * x, \n                        sd=sigma, observed=y)\n\n    # Inference!\n    start = find_MAP() # Find starting value by optimization\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, start=start, progressbar=False) # draw 2000 posterior samples using NUTS sampling\n\n\n\n\nThis should be fairly readable for people who know probabilistic programming. However, would my non-statistican friend know what all this does? Moreover, recall that this is an extremely simple model that would be one line in R. Having multiple, potentially transformed regressors, interaction terms or link-functions would also make this much more complex and error prone. \n\n\nThe new \nglm()\n function instead takes a \nPatsy\n linear model specifier from which it creates a design matrix. \nglm()\n then adds random variables for each of the coefficients and an appopriate likelihood to the model. \n\n\nwith Model() as model:\n    # specify glm and pass in data. The resulting linear model, its likelihood and \n    # and all its parameters are automatically added to our model.\n    glm.glm('y ~ x', data)\n    start = find_MAP()\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, progressbar=False) # draw 2000 posterior samples using NUTS sampling\n\n\n\n\nMuch shorter, but this code does the exact same thing as the above model specification (you can change priors and everything else too if we wanted). \nglm()\n parses the \nPatsy\n model string, adds random variables for each regressor (\nIntercept\n and slope \nx\n in this case), adds a likelihood (by default, a Normal is chosen), and all other variables (\nsigma\n). Finally, \nglm()\n then initializes the parameters to a good starting point by estimating a frequentist linear model using \nstatsmodels\n.\n\n\nIf you are not familiar with R's syntax, \n'y ~ x'\n specifies that we have an output variable \ny\n that we want to estimate as a linear function of \nx\n.\n\n\nAnalyzing the model\n\n\nBayesian inference does not give us only one best fitting line (as maximum likelihood does) but rather a whole posterior distribution of likely parameters. Lets plot the posterior distribution of our parameters and the individual samples we drew.\n\n\nplt.figure(figsize=(7, 7))\ntraceplot(trace[100:])\nplt.tight_layout();\n\n\n\n\nmatplotlib.figure.Figure at 0x7f574ca72b10\n\n\n\n\n\n\nThe left side shows our marginal posterior -- for each parameter value on the x-axis we get a probability on the y-axis that tells us how likely that parameter value is.\n\n\nThere are a couple of things to see here. The first is that our sampling chains for the individual parameters (left side) seem well converged and stationary (there are no large drifts or other odd patterns).\n\n\nSecondly, the maximum posterior estimate of each variable (the peak in the left side distributions) is very close to the true parameters used to generate the data (\nx\n is the regression coefficient and \nsigma\n is the standard deviation of our normal).\n\n\nIn the GLM we thus do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. Here we are using the \nglm.plot_posterior_predictive()\n convenience function for this.\n\n\nplt.figure(figsize=(7, 7))\nplt.plot(x, y, 'x', label='data')\nglm.plot_posterior_predictive(trace, samples=100, \n                              label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, label='true regression line', lw=3., c='y')\n\nplt.title('Posterior predictive regression lines')\nplt.legend(loc=0)\nplt.xlabel('x')\nplt.ylabel('y');\n\n\n\n\n\n\nAs you can see, our estimated regression lines are very similar to the true regression line. But since we only have limited data we have \nuncertainty\n in our estimates, here expressed by the variability of the lines.\n\n\nSummary\n\n\n\n\nUsability is currently a huge hurdle for wider adoption of Bayesian statistics.\n\n\nPyMC3\n allows GLM specification with convenient syntax borrowed from R.\n\n\nPosterior predictive plots allow us to evaluate fit and our uncertainty in it.\n\n\n\n\nFurther reading\n\n\nThis is the first post of a small series on Bayesian GLMs I am preparing. Next week I will describe how the Student T distribution can be used to perform robust linear regression.\n\n\nThen there are also other good resources on Bayesian statistics:\n\n\n\n\nThe excellent book \nDoing Bayesian Data Analysis by John Kruschke\n.\n\n\nAndrew Gelman's blog\n\n\nBaeu Cronins blog post on Probabilistic Programming",
            "title": "Linear Regression"
        },
        {
            "location": "/GLM-linear/#the-inference-button-bayesian-glms-made-easy-with-pymc3",
            "text": "Author: Thomas Wiecki  This tutorial appeared as a post in a small series on Bayesian GLMs on my blog:   The Inference Button: Bayesian GLMs made easy with PyMC3  This world is far from Normal(ly distributed): Robust Regression in PyMC3  The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3   In this blog post I will talk about:   How the Bayesian Revolution in many scientific disciplines is hindered by poor usability of current Probabilistic Programming languages.  A gentle introduction to Bayesian linear regression and how it differs from the frequentist approach.  A preview of  PyMC3  (currently in alpha) and its new GLM submodule I wrote to allow creation and estimation of Bayesian GLMs as easy as frequentist GLMs in R.   Ready? Lets get started!  There is a huge paradigm shift underway in many scientific disciplines: The Bayesian Revolution.   While the theoretical benefits of Bayesian over Frequentist stats have been discussed at length elsewhere (see  Further Reading  below), there is a major obstacle that hinders wider adoption --  usability  (this is one of the reasons DARPA wrote out a huge grant to  improve Probabilistic Programming ).   This is mildly ironic because the beauty of Bayesian statistics is their generality. Frequentist stats have a bazillion different tests for every different scenario. In Bayesian land you define your model exactly as you think is appropriate and hit the  Inference Button(TM)  (i.e. running the magical MCMC sampling algorithm).  Yet when I ask my colleagues why they use frequentist stats (even though they would like to use Bayesian stats) the answer is that software packages like SPSS or R make it very easy to run all those individuals tests with a single command (and more often then not, they don't know the exact model and inference method being used).  While there are great Bayesian software packages like  JAGS ,  BUGS ,  Stan  and  PyMC , they are written for Bayesians statisticians who know very well what model they want to build.   Unfortunately,  \"the vast majority of statistical analysis is not performed by statisticians\"  -- so what we really need are tools for  scientists  and not for statisticians.  In the interest of putting my code where my mouth is I wrote a submodule for the upcoming  PyMC3  that makes construction of Bayesian Generalized Linear Models (GLMs) as easy as Frequentist ones in R.",
            "title": "The Inference Button: Bayesian GLMs made easy with PyMC3"
        },
        {
            "location": "/GLM-linear/#linear-regression",
            "text": "While future blog posts will explore more complex models, I will start here with the simplest GLM -- linear regression.\nIn general, frequentists think about Linear Regression as follows:  $$ Y = X\\beta + \\epsilon $$  where  $Y$  is the output we want to predict (or  dependent  variable),  $X$  is our predictor (or  independent  variable), and  $\\beta$  are the coefficients (or parameters) of the model we want to estimate.  $\\epsilon$  is an error term which is assumed to be normally distributed.   We can then use Ordinary Least Squares or Maximum Likelihood to find the best fitting  $\\beta$ .",
            "title": "Linear Regression"
        },
        {
            "location": "/GLM-linear/#probabilistic-reformulation",
            "text": "Bayesians take a probabilistic view of the world and express this model in terms of probability distributions. Our above linear regression can be rewritten to yield:  $$ Y \\sim \\mathcal{N}(X \\beta, \\sigma^2) $$  In words, we view  $Y$  as a random variable (or random vector) of which each element (data point) is distributed according to a Normal distribution. The mean of this normal distribution is provided by our linear predictor with variance  $\\sigma^2$ .  While this is essentially the same model, there are two critical advantages of Bayesian estimation:   Priors: We can quantify any prior knowledge we might have by placing priors on the paramters. For example, if we think that  $\\sigma$  is likely to be small we would choose a prior with more probability mass on low values.  Quantifying uncertainty: We do not get a single estimate of  $\\beta$  as above but instead a complete posterior distribution about how likely different values of  $\\beta$  are. For example, with few data points our uncertainty in  $\\beta$  will be very high and we'd be getting very wide posteriors.",
            "title": "Probabilistic Reformulation"
        },
        {
            "location": "/GLM-linear/#bayesian-glms-in-pymc3",
            "text": "With the new GLM module in PyMC3 it is very easy to build this and much more complex models.  First, lets import the required modules.  %matplotlib inline\n\nfrom pymc3 import  *\n\nimport numpy as np\nimport matplotlib.pyplot as plt  Generating data  Create some toy data to play around with and scatter-plot it.   Essentially we are creating a regression line defined by intercept and slope and add data points by sampling from a Normal with the mean set to the regression line.  size = 200\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\ndata = dict(x=x, y=y)  fig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x, y, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);   Estimating the model  Lets fit a Bayesian linear regression model to this data. As you can see, model specifications in  PyMC3  are wrapped in a  with  statement.   Here we use the awesome new  NUTS sampler  (our Inference Button) to draw 2000 posterior samples.  with Model() as model: # model specifications in PyMC3 are wrapped in a with-statement\n    # Define priors\n    sigma = HalfCauchy('sigma', beta=10, testval=1.)\n    intercept = Normal('Intercept', 0, sd=20)\n    x_coeff = Normal('x', 0, sd=20)\n\n    # Define likelihood\n    likelihood = Normal('y', mu=intercept + x_coeff * x, \n                        sd=sigma, observed=y)\n\n    # Inference!\n    start = find_MAP() # Find starting value by optimization\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, start=start, progressbar=False) # draw 2000 posterior samples using NUTS sampling  This should be fairly readable for people who know probabilistic programming. However, would my non-statistican friend know what all this does? Moreover, recall that this is an extremely simple model that would be one line in R. Having multiple, potentially transformed regressors, interaction terms or link-functions would also make this much more complex and error prone.   The new  glm()  function instead takes a  Patsy  linear model specifier from which it creates a design matrix.  glm()  then adds random variables for each of the coefficients and an appopriate likelihood to the model.   with Model() as model:\n    # specify glm and pass in data. The resulting linear model, its likelihood and \n    # and all its parameters are automatically added to our model.\n    glm.glm('y ~ x', data)\n    start = find_MAP()\n    step = NUTS(scaling=start) # Instantiate MCMC sampling algorithm\n    trace = sample(2000, step, progressbar=False) # draw 2000 posterior samples using NUTS sampling  Much shorter, but this code does the exact same thing as the above model specification (you can change priors and everything else too if we wanted).  glm()  parses the  Patsy  model string, adds random variables for each regressor ( Intercept  and slope  x  in this case), adds a likelihood (by default, a Normal is chosen), and all other variables ( sigma ). Finally,  glm()  then initializes the parameters to a good starting point by estimating a frequentist linear model using  statsmodels .  If you are not familiar with R's syntax,  'y ~ x'  specifies that we have an output variable  y  that we want to estimate as a linear function of  x .  Analyzing the model  Bayesian inference does not give us only one best fitting line (as maximum likelihood does) but rather a whole posterior distribution of likely parameters. Lets plot the posterior distribution of our parameters and the individual samples we drew.  plt.figure(figsize=(7, 7))\ntraceplot(trace[100:])\nplt.tight_layout();  matplotlib.figure.Figure at 0x7f574ca72b10    The left side shows our marginal posterior -- for each parameter value on the x-axis we get a probability on the y-axis that tells us how likely that parameter value is.  There are a couple of things to see here. The first is that our sampling chains for the individual parameters (left side) seem well converged and stationary (there are no large drifts or other odd patterns).  Secondly, the maximum posterior estimate of each variable (the peak in the left side distributions) is very close to the true parameters used to generate the data ( x  is the regression coefficient and  sigma  is the standard deviation of our normal).  In the GLM we thus do not only have one best fitting regression line, but many. A posterior predictive plot takes multiple samples from the posterior (intercepts and slopes) and plots a regression line for each of them. Here we are using the  glm.plot_posterior_predictive()  convenience function for this.  plt.figure(figsize=(7, 7))\nplt.plot(x, y, 'x', label='data')\nglm.plot_posterior_predictive(trace, samples=100, \n                              label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, label='true regression line', lw=3., c='y')\n\nplt.title('Posterior predictive regression lines')\nplt.legend(loc=0)\nplt.xlabel('x')\nplt.ylabel('y');   As you can see, our estimated regression lines are very similar to the true regression line. But since we only have limited data we have  uncertainty  in our estimates, here expressed by the variability of the lines.",
            "title": "Bayesian GLMs in PyMC3"
        },
        {
            "location": "/GLM-linear/#summary",
            "text": "Usability is currently a huge hurdle for wider adoption of Bayesian statistics.  PyMC3  allows GLM specification with convenient syntax borrowed from R.  Posterior predictive plots allow us to evaluate fit and our uncertainty in it.   Further reading  This is the first post of a small series on Bayesian GLMs I am preparing. Next week I will describe how the Student T distribution can be used to perform robust linear regression.  Then there are also other good resources on Bayesian statistics:   The excellent book  Doing Bayesian Data Analysis by John Kruschke .  Andrew Gelman's blog  Baeu Cronins blog post on Probabilistic Programming",
            "title": "Summary"
        },
        {
            "location": "/GLM-robust/",
            "text": "This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n\n\nAuthor: Thomas Wiecki\n\n\nThis tutorial first appeard as a post in small series on Bayesian GLMs on my blog:\n\n\n\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nThis world is far from Normal(ly distributed): Robust Regression in PyMC3\n\n\nThe Best Of Both Worlds: Hierarchical Linear Regression in PyMC3\n\n\n\n\nIn this blog post I will write about:\n\n\n\n\nHow a few outliers can largely affect the fit of linear regression models.\n\n\nHow replacing the normal likelihood with Student T distribution produces robust regression.\n\n\nHow this can easily be done with \nPyMC3\n and its new \nglm\n module by passing a \nfamily\n object.\n\n\n\n\nThis is the second part of a series on Bayesian GLMs (click \nhere for part I about linear regression\n). In this prior post I described how minimizing the squared distance of the regression line is the same as maximizing the likelihood of a Normal distribution with the mean coming from the regression line. This latter probabilistic expression allows us to easily formulate a Bayesian linear regression model.\n\n\nThis worked splendidly on simulated data. The problem with simulated data though is that it's, well, simulated. In the real world things tend to get more messy and assumptions like normality are easily violated by a few outliers. \n\n\nLets see what happens if we add some outliers to our simulated data from the last post.\n\n\nAgain, import our modules.\n\n\n%matplotlib inline\n\nimport pymc3 as pm\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport theano\n\n\n\n\nCreate some toy data but also add some outliers.\n\n\nsize = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\n# Add outliers\nx_out = np.append(x, [.1, .15, .2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = dict(x=x_out, y=y_out)\n\n\n\n\nPlot the data together with the true regression line (the three points in the upper left corner are the outliers we added).\n\n\nfig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x_out, y_out, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);\n\n\n\n\n\n\nRobust Regression\n\n\nLets see what happens if we estimate our Bayesian linear regression model using the \nglm()\n function as before. This function takes a \nPatsy\n string to describe the linear model and adds a Normal likelihood by default. \n\n\nwith pm.Model() as model:\n    pm.glm.glm('y ~ x', data)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(2000, step, progressbar=False)\n\n\n\n\n/home/wiecki/envs/pymc3/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *\n\n\n\nTo evaluate the fit, I am plotting the posterior predictive regression lines by taking regression parameters from the posterior distribution and plotting a regression line for each (this is all done inside of \nplot_posterior_predictive()\n).\n\n\nplt.subplot(111, xlabel='x', ylabel='y', \n            title='Posterior predictive regression lines')\nplt.plot(x_out, y_out, 'x', label='data')\npm.glm.plot_posterior_predictive(trace, samples=100, \n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\n\nplt.legend(loc=0);\n\n\n\n\n\n\nAs you can see, the fit is quite skewed and we have a fair amount of uncertainty in our estimate as indicated by the wide range of different posterior predictive regression lines. Why is this? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.\n\n\nA Frequentist would estimate a \nRobust Regression\n and use a non-quadratic distance measure to evaluate the fit.\n\n\nBut what's a Bayesian to do? Since the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the \nStudent T distribution\n which has heavier tails as shown next (I read about this trick in \n\"The Kruschke\"\n, aka the puppy-book; but I think \nGelman\n was the first to formulate this).\n\n\nLets look at those two distributions to get a feel for them.\n\n\nnormal_dist = pm.Normal.dist(mu=0, sd=1)\nt_dist = pm.T.dist(mu=0, lam=1, nu=1)\nx_eval = np.linspace(-8, 8, 300)\nplt.plot(x_eval, theano.tensor.exp(normal_dist.logp(x_eval)).eval(), label='Normal', lw=2.)\nplt.plot(x_eval, theano.tensor.exp(t_dist.logp(x_eval)).eval(), label='Student T', lw=2.)\nplt.xlabel('x')\nplt.ylabel('Probability density')\nplt.legend();\n\n\n\n\n\n\nAs you can see, the probability of values far away from the mean (0 in this case) are much more likely under the \nT\n distribution than under the Normal distribution.\n\n\nTo define the usage of a T distribution in \nPyMC3\n we can pass a family object -- \nT\n -- that specifies that our data is Student T-distributed (see \nglm.families\n for more choices). Note that this is the same syntax as \nR\n and \nstatsmodels\n use.\n\n\nwith pm.Model() as model_robust:\n    family = pm.glm.families.T()\n    pm.glm.glm('y ~ x', data, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace_robust = pm.sample(2000, step, progressbar=False)\n\nplt.figure(figsize=(5, 5))\nplt.plot(x_out, y_out, 'x')\npm.glm.plot_posterior_predictive(trace_robust,\n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\nplt.legend();\n\n\n\n\n\n\nThere, much better! The outliers are barely influencing our estimation at all because our likelihood function assumes that outliers are much more probable than under the Normal distribution.\n\n\nSummary\n\n\n\n\nPyMC3\n's \nglm()\n function allows you to pass in a \nfamily\n object that contains information about the likelihood.\n\n\nBy changing the likelihood from a Normal distribution to a Student T distribution -- which has more mass in the tails -- we can perform \nRobust Regression\n.\n\n\n\n\nThe next post will be about logistic regression in PyMC3 and what the posterior and oatmeal have in common.\n\n\nExtensions\n: \n\n\n\n\nThe Student-T distribution has, besides the mean and variance, a third parameter called \ndegrees of freedom\n that describes how much mass should be put into the tails. Here it is set to 1 which gives maximum mass to the tails (setting this to infinity results in a Normal distribution!). One could easily place a prior on this rather than fixing it which I leave as an exercise for the reader ;).\n\n\nT distributions can be used as priors as well. I will show this in a future post on hierarchical GLMs.\n\n\nHow do we test if our data is normal or violates that assumption in an important way? Check out this \ngreat blog post\n by Allen Downey.",
            "title": "Robust Regression"
        },
        {
            "location": "/GLM-robust/#this-world-is-far-from-normally-distributed-bayesian-robust-regression-in-pymc3",
            "text": "Author: Thomas Wiecki  This tutorial first appeard as a post in small series on Bayesian GLMs on my blog:   The Inference Button: Bayesian GLMs made easy with PyMC3  This world is far from Normal(ly distributed): Robust Regression in PyMC3  The Best Of Both Worlds: Hierarchical Linear Regression in PyMC3   In this blog post I will write about:   How a few outliers can largely affect the fit of linear regression models.  How replacing the normal likelihood with Student T distribution produces robust regression.  How this can easily be done with  PyMC3  and its new  glm  module by passing a  family  object.   This is the second part of a series on Bayesian GLMs (click  here for part I about linear regression ). In this prior post I described how minimizing the squared distance of the regression line is the same as maximizing the likelihood of a Normal distribution with the mean coming from the regression line. This latter probabilistic expression allows us to easily formulate a Bayesian linear regression model.  This worked splendidly on simulated data. The problem with simulated data though is that it's, well, simulated. In the real world things tend to get more messy and assumptions like normality are easily violated by a few outliers.   Lets see what happens if we add some outliers to our simulated data from the last post.  Again, import our modules.  %matplotlib inline\n\nimport pymc3 as pm\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nimport theano  Create some toy data but also add some outliers.  size = 100\ntrue_intercept = 1\ntrue_slope = 2\n\nx = np.linspace(0, 1, size)\n# y = a + b*x\ntrue_regression_line = true_intercept + true_slope * x\n# add noise\ny = true_regression_line + np.random.normal(scale=.5, size=size)\n\n# Add outliers\nx_out = np.append(x, [.1, .15, .2])\ny_out = np.append(y, [8, 6, 9])\n\ndata = dict(x=x_out, y=y_out)  Plot the data together with the true regression line (the three points in the upper left corner are the outliers we added).  fig = plt.figure(figsize=(7, 7))\nax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\nax.plot(x_out, y_out, 'x', label='sampled data')\nax.plot(x, true_regression_line, label='true regression line', lw=2.)\nplt.legend(loc=0);",
            "title": "This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3"
        },
        {
            "location": "/GLM-robust/#robust-regression",
            "text": "Lets see what happens if we estimate our Bayesian linear regression model using the  glm()  function as before. This function takes a  Patsy  string to describe the linear model and adds a Normal likelihood by default.   with pm.Model() as model:\n    pm.glm.glm('y ~ x', data)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(2000, step, progressbar=False)  /home/wiecki/envs/pymc3/local/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *  To evaluate the fit, I am plotting the posterior predictive regression lines by taking regression parameters from the posterior distribution and plotting a regression line for each (this is all done inside of  plot_posterior_predictive() ).  plt.subplot(111, xlabel='x', ylabel='y', \n            title='Posterior predictive regression lines')\nplt.plot(x_out, y_out, 'x', label='data')\npm.glm.plot_posterior_predictive(trace, samples=100, \n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\n\nplt.legend(loc=0);   As you can see, the fit is quite skewed and we have a fair amount of uncertainty in our estimate as indicated by the wide range of different posterior predictive regression lines. Why is this? The reason is that the normal distribution does not have a lot of mass in the tails and consequently, an outlier will affect the fit strongly.  A Frequentist would estimate a  Robust Regression  and use a non-quadratic distance measure to evaluate the fit.  But what's a Bayesian to do? Since the problem is the light tails of the Normal distribution we can instead assume that our data is not normally distributed but instead distributed according to the  Student T distribution  which has heavier tails as shown next (I read about this trick in  \"The Kruschke\" , aka the puppy-book; but I think  Gelman  was the first to formulate this).  Lets look at those two distributions to get a feel for them.  normal_dist = pm.Normal.dist(mu=0, sd=1)\nt_dist = pm.T.dist(mu=0, lam=1, nu=1)\nx_eval = np.linspace(-8, 8, 300)\nplt.plot(x_eval, theano.tensor.exp(normal_dist.logp(x_eval)).eval(), label='Normal', lw=2.)\nplt.plot(x_eval, theano.tensor.exp(t_dist.logp(x_eval)).eval(), label='Student T', lw=2.)\nplt.xlabel('x')\nplt.ylabel('Probability density')\nplt.legend();   As you can see, the probability of values far away from the mean (0 in this case) are much more likely under the  T  distribution than under the Normal distribution.  To define the usage of a T distribution in  PyMC3  we can pass a family object --  T  -- that specifies that our data is Student T-distributed (see  glm.families  for more choices). Note that this is the same syntax as  R  and  statsmodels  use.  with pm.Model() as model_robust:\n    family = pm.glm.families.T()\n    pm.glm.glm('y ~ x', data, family=family)\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace_robust = pm.sample(2000, step, progressbar=False)\n\nplt.figure(figsize=(5, 5))\nplt.plot(x_out, y_out, 'x')\npm.glm.plot_posterior_predictive(trace_robust,\n                                 label='posterior predictive regression lines')\nplt.plot(x, true_regression_line, \n         label='true regression line', lw=3., c='y')\nplt.legend();   There, much better! The outliers are barely influencing our estimation at all because our likelihood function assumes that outliers are much more probable than under the Normal distribution.",
            "title": "Robust Regression"
        },
        {
            "location": "/GLM-robust/#summary",
            "text": "PyMC3 's  glm()  function allows you to pass in a  family  object that contains information about the likelihood.  By changing the likelihood from a Normal distribution to a Student T distribution -- which has more mass in the tails -- we can perform  Robust Regression .   The next post will be about logistic regression in PyMC3 and what the posterior and oatmeal have in common.  Extensions :    The Student-T distribution has, besides the mean and variance, a third parameter called  degrees of freedom  that describes how much mass should be put into the tails. Here it is set to 1 which gives maximum mass to the tails (setting this to infinity results in a Normal distribution!). One could easily place a prior on this rather than fixing it which I leave as an exercise for the reader ;).  T distributions can be used as priors as well. I will show this in a future post on hierarchical GLMs.  How do we test if our data is normal or violates that assumption in an important way? Check out this  great blog post  by Allen Downey.",
            "title": "Summary"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/",
            "text": "PyMC3 Examples\n\n\nRobust Regression with Outlier Detection\n\n\n@author: Jonathan Sedar\n\n@email: jon@sedar.co\n\n@date: Mon 21 Dec 2015  \n\n\nA minimal reproducable example of Robust Regression with Outlier Detection using Hogg 2010 Signal vs Noise method.\n\n\n\n\nThis is a complementary approach to the Student-T robust regression as illustrated in Thomas Wiecki's notebook in the \nPyMC3 documentation\n, that approach is also compared here.\n\n\nThis model returns a robust estimate of linear coefficients and an indication of which datapoints (if any) are outliers.\n\n\nThe likelihood evaluation is essentially a copy of eqn 17 in \"Data analysis recipes: Fitting a model to data\" - \nHogg 2010\n.\n\n\nThe model is adapted specifically from Jake Vanderplas' \nimplementation\n (3rd model tested).\n\n\nThe dataset is tiny and hardcoded into this Notebook. It contains errors in both the x and y, but we will deal here with only errors in y.\n\n\n\n\nNote:\n\n\n\n\nPython 3.4 project using latest available \nPyMC3\n\n\nDeveloped using \nContinuumIO Anaconda\n distribution on a Macbook Pro 3GHz i7, 16GB RAM, OSX 10.10.5.\n\n\nDuring development I've found that 3 data points are always indicated as outliers, but the remaining ordering of datapoints by decreasing outlier-hood is slightly unstable between runs: the posterior surface appears to have a small number of solutions with similar probability. \n\n\nFinally, if runs become unstable or Theano throws weird errors, try clearing the cache \n$\n theano-cache clear\n and rerunning the notebook.\n\n\n\n\nPackage Requirements (shown as a conda-env YAML):\n\n\n$\n less conda_env_pymc3_examples.yml\n\nname: pymc3_examples\n    channels:\n      - defaults\n    dependencies:\n      - python=3.4\n      - ipython\n      - ipython-notebook\n      - ipython-qtconsole\n      - numpy\n      - scipy\n      - matplotlib\n      - pandas\n      - seaborn\n      - patsy  \n      - pip\n\n$\n conda env create --file conda_env_pymc3_examples.yml\n\n$\n source activate pymc3_examples\n\n$\n pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3\n\n\n\n\n\nSetup\n\n\n%matplotlib inline\n%qtconsole --colors=linux\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import optimize\nimport pymc3 as pm\nimport theano as thno\nimport theano.tensor as T \n\n# configure some basic options\nsns.set(style=\ndarkgrid\n, palette=\nmuted\n)\npd.set_option('display.notebook_repr_html', True)\nplt.rcParams['figure.figsize'] = 12, 8\nnp.random.seed(0)\n\n\n\n\nLoad and Prepare Data\n\n\nWe'll use the Hogg 2010 data available at  https://github.com/astroML/astroML/blob/master/astroML/datasets/hogg2010test.py\n\n\nIt's a very small dataset so for convenience, it's hardcoded below\n\n\n## cut \n pasted directly from the fetch_hogg2010test() function\n## identical to the original dataset as hardcoded in the Hogg 2010 paper\n\ndfhogg = pd.DataFrame(np.array([[1, 201, 592, 61, 9, -0.84],\n                                 [2, 244, 401, 25, 4, 0.31],\n                                 [3, 47, 583, 38, 11, 0.64],\n                                 [4, 287, 402, 15, 7, -0.27],\n                                 [5, 203, 495, 21, 5, -0.33],\n                                 [6, 58, 173, 15, 9, 0.67],\n                                 [7, 210, 479, 27, 4, -0.02],\n                                 [8, 202, 504, 14, 4, -0.05],\n                                 [9, 198, 510, 30, 11, -0.84],\n                                 [10, 158, 416, 16, 7, -0.69],\n                                 [11, 165, 393, 14, 5, 0.30],\n                                 [12, 201, 442, 25, 5, -0.46],\n                                 [13, 157, 317, 52, 5, -0.03],\n                                 [14, 131, 311, 16, 6, 0.50],\n                                 [15, 166, 400, 34, 6, 0.73],\n                                 [16, 160, 337, 31, 5, -0.52],\n                                 [17, 186, 423, 42, 9, 0.90],\n                                 [18, 125, 334, 26, 8, 0.40],\n                                 [19, 218, 533, 16, 6, -0.78],\n                                 [20, 146, 344, 22, 5, -0.56]]),\n                   columns=['id','x','y','sigma_y','sigma_x','rho_xy'])\n\n\n## for convenience zero-base the 'id' and use as index\ndfhogg['id'] = dfhogg['id'] - 1\ndfhogg.set_index('id', inplace=True)\n\n## standardize (mean center and divide by 1 sd)\ndfhoggs = (dfhogg[['x','y']] - dfhogg[['x','y']].mean(0)) / dfhogg[['x','y']].std(0)\ndfhoggs['sigma_y'] = dfhogg['sigma_y'] / dfhogg['y'].std(0)\ndfhoggs['sigma_x'] = dfhogg['sigma_x'] / dfhogg['x'].std(0)\n\n## scatterplot the standardized data\ng = sns.FacetGrid(dfhoggs, size=8)\n_ = g.map(plt.errorbar, 'x', 'y', 'sigma_y', 'sigma_x', marker=\no\n, ls='')\n_ = g.axes[0][0].set_ylim((-3,3))\n_ = g.axes[0][0].set_xlim((-3,3))\n\nplt.subplots_adjust(top=0.92)\n_ = g.fig.suptitle('Scatterplot of Hogg 2010 dataset after standardization', fontsize=16)\n\n\n\n\n\n\nObserve\n:  \n\n\n\n\nEven judging just by eye, you can see these datapoints mostly fall on / around a straight line with positive gradient\n\n\nIt looks like a few of the datapoints may be outliers from such a line\n\n\n\n\n\n\n\n\nCreate Conventional OLS Model\n\n\nThe \nlinear model\n is really simple and conventional:\n\n\n$$\\bf{y} = \\beta^{T} \\bf{X} + \\bf{\\sigma}$$\n\n\nwhere:  \n\n\n$\\beta$\n = coefs = \n$\\{1, \\beta_{j \\in X_{j}}\\}$\n\n\n$\\sigma$\n = the measured error in \n$y$\n in the dataset \nsigma_y\n\n\nDefine model\n\n\nNOTE:\n\n+ We're using a simple linear OLS model with Normally distributed priors so that it behaves like a ridge regression\n\n\nwith pm.Model() as mdl_ols:\n\n    ## Define weakly informative Normal priors to give Ridge regression\n    b0 = pm.Normal('b0_intercept', mu=0, sd=100)\n    b1 = pm.Normal('b1_slope', mu=0, sd=100)\n\n    ## Define linear model\n    yest = b0 + b1 * dfhoggs['x']\n\n    ## Use y error from dataset, convert into theano variable\n    sigma_y = thno.shared(np.asarray(dfhoggs['sigma_y'],\n                            dtype=thno.config.floatX), name='sigma_y')\n\n    ## Define Normal likelihood\n    likelihood = pm.Normal('likelihood', mu=yest, sd=sigma_y**2, observed=dfhoggs['y'])\n\n\n\n\n\nSample\n\n\nwith mdl_ols:\n\n    ## find MAP using Powell, seems to be more robust\n    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)\n\n    ## take samples\n    traces_ols = pm.sample(2000, start=start_MAP, step=pm.NUTS(), progressbar=True)\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 4555.263214\n         Iterations: 3\n         Function evaluations: 110\n [-----------------100%-----------------] 2000 of 2000 complete in 0.6 sec\n\n\n\nView Traces\n\n\nNOTE\n: I'll 'burn' the traces to only retain the final 1000 samples\n\n\n_ = pm.traceplot(traces_ols[-1000:], figsize=(12,len(traces_ols.varnames)*1.5),\n                lines={k: v['mean'] for k, v in pm.df_summary(traces_ols[-1000:]).iterrows()})\n\n\n\n\n\n\nNOTE:\n We'll illustrate this OLS fit and compare to the datapoints in the final plot\n\n\n\n\n\n\nCreate Robust Model: Student-T Method\n\n\nI've added this brief section in order to directly compare the Student-T based method exampled in Thomas Wiecki's notebook in the \nPyMC3 documentation\n\n\nInstead of using a Normal distribution for the likelihood, we use a Student-T, which has fatter tails. In theory this allows outliers to have a smaller mean square error in the likelihood, and thus have less influence on the regression estimation. This method does not produce inlier / outlier flags but is simpler and faster to run than the Signal Vs Noise model below, so a comparison seems worthwhile.\n\n\nNote:\n we'll constrain the Student-T 'degrees of freedom' parameter \nnu\n to be an integer, but otherwise leave it as just another stochastic to be inferred: no need for prior knowledge.\n\n\nDefine Model\n\n\nwith pm.Model() as mdl_studentt:\n\n    ## Define weakly informative Normal priors to give Ridge regression\n    b0 = pm.Normal('b0_intercept', mu=0, sd=100)\n    b1 = pm.Normal('b1_slope', mu=0, sd=100)\n\n    ## Define linear model\n    yest = b0 + b1 * dfhoggs['x']\n\n    ## Use y error from dataset, convert into theano variable\n    sigma_y = thno.shared(np.asarray(dfhoggs['sigma_y'],\n                            dtype=thno.config.floatX), name='sigma_y')\n\n    ## define prior for Student T degrees of freedom\n    nu = pm.DiscreteUniform('nu', lower=1, upper=100)\n\n    ## Define Student T likelihood\n    likelihood = pm.StudentT('likelihood', mu=yest, sd=sigma_y**2, nu=nu\n                           ,observed=dfhoggs['y'])\n\n\n\n\n\nSample\n\n\nwith mdl_studentt:\n\n    ## find MAP using Powell, seems to be more robust\n    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)\n\n    ## two-step sampling to allow Metropolis for nu (which is discrete)\n    step1 = pm.NUTS([b0, b1])\n    step2 = pm.Metropolis([nu])\n\n    ## take samples\n    traces_studentt = pm.sample(2000, start=start_MAP, step=[step1, step2], progressbar=True)\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 430.443483\n         Iterations: 3\n         Function evaluations: 86\n [-----------------100%-----------------] 2000 of 2000 complete in 1.3 sec\n\n\n\nView Traces\n\n\n_ = pm.traceplot(traces_studentt[-1000:]\n            ,figsize=(12,len(traces_studentt.varnames)*1.5)\n            ,lines={k: v['mean'] for k, v in pm.df_summary(traces_studentt[-1000:]).iterrows()})\n\n\n\n\n\n\nObserve:\n\n\n\n\nBoth parameters \nb0\n and \nb1\n show quite a skew to the right, possibly this is the action of a few samples regressing closer to the OLS estimate which is towards the left\n\n\nThe \nnu\n parameter seems very happy to stick at \nnu = 1\n, indicating that a fat-tailed Student-T likelihood has a better fit than a thin-tailed (Normal-like) Student-T likelihood.\n\n\nThe inference sampling also ran very quickly, almost as quickly as the conventional OLS\n\n\n\n\nNOTE:\n We'll illustrate this Student-T fit and compare to the datapoints in the final plot\n\n\n\n\n\n\nCreate Robust Model with Outliers: Hogg Method\n\n\nPlease read the paper (Hogg 2010) and Jake Vanderplas' code for more complete information about the modelling technique.\n\n\nThe general idea is to create a 'mixture' model whereby datapoints can be described by either the linear model (inliers) or a modified linear model with different mean and larger variance (outliers).\n\n\nThe likelihood is evaluated over a mixture of two likelihoods, one for 'inliers', one for 'outliers'. A Bernouilli distribution is used to randomly assign datapoints in N to either the inlier or outlier groups, and we sample the model as usual to infer robust model parameters and inlier / outlier flags:\n\n\n$$\n\\mathcal{logL} = \\sum_{i}^{i=N} log \\left[ \\frac{(1 - B_{i})}{\\sqrt{2 \\pi \\sigma_{in}^{2}}} exp \\left( - \\frac{(x_{i} - \\mu_{in})^{2}}{2\\sigma_{in}^{2}} \\right) \\right] + \\sum_{i}^{i=N} log \\left[ \\frac{B_{i}}{\\sqrt{2 \\pi (\\sigma_{in}^{2} + \\sigma_{out}^{2})}} exp \\left( - \\frac{(x_{i}- \\mu_{out})^{2}}{2(\\sigma_{in}^{2} + \\sigma_{out}^{2})} \\right) \\right]\n$$\n\n\nwhere:\n\n\n$\\bf{B}$\n is Bernoulli-distibuted \n$B_{i} \\in [0_{(inlier)},1_{(outlier)}]$\n\n\nDefine model\n\n\ndef logp_signoise(yobs, is_outlier, yest_in, sigma_y_in, yest_out, sigma_y_out):\n    '''\n    Define custom loglikelihood for inliers vs outliers. \n    NOTE: in this particular case we don't need to use theano's @as_op \n    decorator because (as stated by Twiecki in conversation) that's only \n    required if the likelihood cannot be expressed as a theano expression.\n    We also now get the gradient computation for free.\n    '''   \n\n    # likelihood for inliers\n    pdfs_in = T.exp(-(yobs - yest_in + 1e-6)**2 / (2 * sigma_y_in**2)) \n    pdfs_in /= T.sqrt(2 * np.pi * sigma_y_in**2)\n    logL_in = T.sum(T.log(pdfs_in) * (1 - is_outlier))\n\n    # likelihood for outliers\n    pdfs_out = T.exp(-(yobs - yest_out + 1e-6)**2 / (2 * (sigma_y_in**2 + sigma_y_out**2))) \n    pdfs_out /= T.sqrt(2 * np.pi * (sigma_y_in**2 + sigma_y_out**2))\n    logL_out = T.sum(T.log(pdfs_out) * is_outlier)\n\n    return logL_in + logL_out\n\n\n\n\n\nwith pm.Model() as mdl_signoise:\n\n    ## Define weakly informative Normal priors to give Ridge regression\n    b0 = pm.Normal('b0_intercept', mu=0, sd=100)\n    b1 = pm.Normal('b1_slope', mu=0, sd=100)\n\n    ## Define linear model\n    yest_in = b0 + b1 * dfhoggs['x']\n\n    ## Define weakly informative priors for the mean and variance of outliers\n    yest_out = pm.Normal('yest_out', mu=0, sd=100)\n    sigma_y_out = pm.HalfNormal('sigma_y_out', sd=100)\n\n    ## Define Bernoulli inlier / outlier flags according to a hyperprior \n    ## fraction of outliers, itself constrained to [0,.5] for symmetry\n    frac_outliers = pm.Uniform('frac_outliers', lower=0., upper=.5)\n    is_outlier = pm.Bernoulli('is_outlier', p=frac_outliers, shape=dfhoggs.shape[0])  \n\n    ## Extract observed y and sigma_y from dataset, encode as theano objects\n    yobs = thno.shared(np.asarray(dfhoggs['y'], dtype=thno.config.floatX), name='yobs')\n    sigma_y_in = thno.shared(np.asarray(dfhoggs['sigma_y']\n                                , dtype=thno.config.floatX), name='sigma_y_in')\n\n    ## Use custom likelihood using DensityDist\n    likelihood = pm.DensityDist('likelihood', logp_signoise,\n                        observed={'yobs':yobs, 'is_outlier':is_outlier,\n                                  'yest_in':yest_in, 'sigma_y_in':sigma_y_in,\n                                  'yest_out':yest_out, 'sigma_y_out':sigma_y_out})\n\n\n\n\n\nSample\n\n\nwith mdl_signoise:\n\n    ## two-step sampling to create Bernoulli inlier/outlier flags\n    step1 = pm.NUTS([frac_outliers, yest_out, sigma_y_out, b0, b1])\n    step2 = pm.BinaryMetropolis([is_outlier])\n\n    ## find MAP using Powell, seems to be more robust\n    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)\n\n    ## take samples\n    traces_signoise = pm.sample(2000, start=start_MAP, step=[step1,step2], progressbar=True)\n\n\n\n\nOptimization terminated successfully.\n         Current function value: 155.449990\n         Iterations: 3\n         Function evaluations: 202\n [-----------------100%-----------------] 2000 of 2000 complete in 180.3 sec\n\n\n\nView Traces\n\n\n_ = pm.traceplot(traces_signoise[-1000:], figsize=(12,len(traces_signoise.varnames)*1.5),\n            lines={k: v['mean'] for k, v in pm.df_summary(traces_signoise[-1000:]).iterrows()})\n\n\n\n\n\n\nNOTE:\n:\n\n\n\n\nDuring development I've found that 3 datapoints id=[1,2,3] are always indicated as outliers, but the remaining ordering of datapoints by decreasing outlier-hood is unstable between runs: the posterior surface appears to have a small number of solutions with very similar probability.\n\n\nThe NUTS sampler seems to work okay, and indeed it's a nice opportunity to demonstrate a custom likelihood which is possible to express as a theano function (thus allowing a gradient-based sampler like NUTS). However, with a more complicated dataset, I would spend time understanding this instability and potentially prefer using more samples under Metropolis-Hastings.\n\n\n\n\n\n\n\n\nDeclare Outliers and Compare Plots\n\n\nView ranges for inliers / outlier predictions\n\n\nAt each step of the traces, each datapoint may be either an inlier or outlier. We hope that the datapoints spend an unequal time being one state or the other, so let's take a look at the simple count of states for each of the 20 datapoints.\n\n\noutlier_melt = pd.melt(pd.DataFrame(traces_signoise['is_outlier', -1000:],\n                                   columns=['[{}]'.format(int(d)) for d in dfhoggs.index]),\n                      var_name='datapoint_id', value_name='is_outlier')\nax0 = sns.pointplot(y='datapoint_id', x='is_outlier', data=outlier_melt,\n                   kind='point', join=False, ci=None, size=4, aspect=2)\n\n_ = ax0.vlines([0,1], 0, 19, ['b','r'], '--')\n\n_ = ax0.set_xlim((-0.1,1.1))\n_ = ax0.set_xticks(np.arange(0, 1.1, 0.1))\n_ = ax0.set_xticklabels(['{:.0%}'.format(t) for t in np.arange(0,1.1,0.1)])\n\n_ = ax0.yaxis.grid(True, linestyle='-', which='major', color='w', alpha=0.4)\n_ = ax0.set_title('Prop. of the trace where datapoint is an outlier')\n_ = ax0.set_xlabel('Prop. of the trace where is_outlier == 1')\n\n\n\n\n\n\nObserve\n:\n\n\n\n\nThe plot above shows the number of samples in the traces in which each datapoint is marked as an outlier, expressed as a percentage.\n\n\nIn particular, 3 points [1, 2, 3] spend \n=95% of their time as outliers\n\n\nContrastingly, points at the other end of the plot close to 0% are our strongest inliers.\n\n\nFor comparison, the mean posterior value of \nfrac_outliers\n is ~0.35, corresponding to roughly 7 of the 20 datapoints. You can see these 7 datapoints in the plot above, all those with a value \n50% or thereabouts.\n\n\nHowever, only 3 of these points are outliers \n=95% of the time. \n\n\nSee note above regarding instability between runs.\n\n\n\n\nThe 95% cutoff we choose is subjective and arbitrary, but I prefer it for now, so let's declare these 3 to be outliers and see how it looks compared to Jake Vanderplas' outliers, which were declared in a slightly different way as points with means above 0.68.\n\n\nDeclare outliers\n\n\nNote:\n\n+ I will declare outliers to be datapoints that have value == 1 at the 5-percentile cutoff, i.e. in the percentiles from 5 up to 100, their values are 1. \n+ Try for yourself altering cutoff to larger values, which leads to an objective ranking of outlier-hood.\n\n\ncutoff = 5\ndfhoggs['outlier'] = np.percentile(traces_signoise[-1000:]['is_outlier'],cutoff, axis=0)\ndfhoggs['outlier'].value_counts()\n\n\n\n\n0    17\n1     3\nName: outlier, dtype: int64\n\n\n\nPosterior Prediction Plots for OLS vs StudentT vs SignalNoise\n\n\ng = sns.FacetGrid(dfhoggs, size=8, hue='outlier', hue_order=[True,False],\n                  palette='Set1', legend_out=False)\n\nlm = lambda x, samp: samp['b0_intercept'] + samp['b1_slope'] * x\n\npm.glm.plot_posterior_predictive(traces_ols[-1000:],\n        eval=np.linspace(-3, 3, 10), lm=lm, samples=200, color='#00AA00', alpha=.1)\n\npm.glm.plot_posterior_predictive(traces_studentt[-1000:], lm=lm,\n        eval=np.linspace(-3, 3, 10), samples=200, color='#FF9900', alpha=.3)\n\npm.glm.plot_posterior_predictive(traces_signoise[-1000:], lm=lm,\n        eval=np.linspace(-3, 3, 10), samples=200,\n        color=sns.palettes.color_palette('Set1')[1], alpha=.3)\n\n_ = g.map(plt.errorbar, 'x', 'y', 'sigma_y', 'sigma_x', marker=\no\n, ls='').add_legend()\n\n_ = g.axes[0][0].annotate('OLS Fit: Green\\nStudent-T Fit: Orange\\nSignal Vs Noise Fit: Blue',\n                          size='x-large', xy=(1,0), xycoords='axes fraction',\n                          xytext=(-160,10), textcoords='offset points')\n_ = g.axes[0][0].set_ylim((-3,3))\n\n\n\n\n\n\nObserve\n:\n\n\n\n\n\n\nThe posterior preditive fit for:\n\n\n\n\nthe \nOLS model\n is shown in \nGreen\n and as expected, it doesn't appear to fit the majority of our datapoints very well, skewed by outliers\n\n\nthe \nRobust Student-T model\n is shown in \nOrange\n and does appear to fit the 'main axis' of datapoints quite well, ignoring outliers\n\n\nthe \nRobust Signal vs Noise model\n is shown in \nBlue\n and also appears to fit the 'main axis' of datapoints rather well, ignoring outliers.\n\n\n\n\n\n\n\n\nWe see that the \nRobust Signal vs Noise model\n also yields specific estimates of \nwhich\n datapoints are outliers:\n\n\n\n\n17 'inlier' datapoints, in \nBlue\n and\n\n\n3 'outlier' datapoints shown in \nRed\n.\n\n\nFrom a simple visual inspection, the classification seems fair, and agrees with Jake Vanderplas' findings.\n\n\n\n\n\n\n\n\nOverall, it seems that:\n\n\n\n\nthe \nSignal vs Noise model\n behaves as promised, yielding a robust regression estimate and explicit labelling of inliers / outliers, but\n\n\nthe \nSignal vs Noise model\n is quite complex and whilst the regression seems robust and stable, the actual inlier / outlier labelling seems slightly unstable\n\n\nif you simply want a robust regression without inlier / outlier labelling, the \nStudent-T model\n may be a good compromise, offering a simple model, quick sampling, and a very similar estimate.",
            "title": "Robust Regression with outlier detection"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#robust-regression-with-outlier-detection",
            "text": "@author: Jonathan Sedar \n@email: jon@sedar.co \n@date: Mon 21 Dec 2015    A minimal reproducable example of Robust Regression with Outlier Detection using Hogg 2010 Signal vs Noise method.   This is a complementary approach to the Student-T robust regression as illustrated in Thomas Wiecki's notebook in the  PyMC3 documentation , that approach is also compared here.  This model returns a robust estimate of linear coefficients and an indication of which datapoints (if any) are outliers.  The likelihood evaluation is essentially a copy of eqn 17 in \"Data analysis recipes: Fitting a model to data\" -  Hogg 2010 .  The model is adapted specifically from Jake Vanderplas'  implementation  (3rd model tested).  The dataset is tiny and hardcoded into this Notebook. It contains errors in both the x and y, but we will deal here with only errors in y.   Note:   Python 3.4 project using latest available  PyMC3  Developed using  ContinuumIO Anaconda  distribution on a Macbook Pro 3GHz i7, 16GB RAM, OSX 10.10.5.  During development I've found that 3 data points are always indicated as outliers, but the remaining ordering of datapoints by decreasing outlier-hood is slightly unstable between runs: the posterior surface appears to have a small number of solutions with similar probability.   Finally, if runs become unstable or Theano throws weird errors, try clearing the cache  $  theano-cache clear  and rerunning the notebook.   Package Requirements (shown as a conda-env YAML):  $  less conda_env_pymc3_examples.yml\n\nname: pymc3_examples\n    channels:\n      - defaults\n    dependencies:\n      - python=3.4\n      - ipython\n      - ipython-notebook\n      - ipython-qtconsole\n      - numpy\n      - scipy\n      - matplotlib\n      - pandas\n      - seaborn\n      - patsy  \n      - pip\n\n$  conda env create --file conda_env_pymc3_examples.yml\n\n$  source activate pymc3_examples\n\n$  pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3",
            "title": "Robust Regression with Outlier Detection"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#setup",
            "text": "%matplotlib inline\n%qtconsole --colors=linux\n\nimport warnings\nwarnings.filterwarnings('ignore')  import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import optimize\nimport pymc3 as pm\nimport theano as thno\nimport theano.tensor as T \n\n# configure some basic options\nsns.set(style= darkgrid , palette= muted )\npd.set_option('display.notebook_repr_html', True)\nplt.rcParams['figure.figsize'] = 12, 8\nnp.random.seed(0)",
            "title": "Setup"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#load-and-prepare-data",
            "text": "We'll use the Hogg 2010 data available at  https://github.com/astroML/astroML/blob/master/astroML/datasets/hogg2010test.py  It's a very small dataset so for convenience, it's hardcoded below  ## cut   pasted directly from the fetch_hogg2010test() function\n## identical to the original dataset as hardcoded in the Hogg 2010 paper\n\ndfhogg = pd.DataFrame(np.array([[1, 201, 592, 61, 9, -0.84],\n                                 [2, 244, 401, 25, 4, 0.31],\n                                 [3, 47, 583, 38, 11, 0.64],\n                                 [4, 287, 402, 15, 7, -0.27],\n                                 [5, 203, 495, 21, 5, -0.33],\n                                 [6, 58, 173, 15, 9, 0.67],\n                                 [7, 210, 479, 27, 4, -0.02],\n                                 [8, 202, 504, 14, 4, -0.05],\n                                 [9, 198, 510, 30, 11, -0.84],\n                                 [10, 158, 416, 16, 7, -0.69],\n                                 [11, 165, 393, 14, 5, 0.30],\n                                 [12, 201, 442, 25, 5, -0.46],\n                                 [13, 157, 317, 52, 5, -0.03],\n                                 [14, 131, 311, 16, 6, 0.50],\n                                 [15, 166, 400, 34, 6, 0.73],\n                                 [16, 160, 337, 31, 5, -0.52],\n                                 [17, 186, 423, 42, 9, 0.90],\n                                 [18, 125, 334, 26, 8, 0.40],\n                                 [19, 218, 533, 16, 6, -0.78],\n                                 [20, 146, 344, 22, 5, -0.56]]),\n                   columns=['id','x','y','sigma_y','sigma_x','rho_xy'])\n\n\n## for convenience zero-base the 'id' and use as index\ndfhogg['id'] = dfhogg['id'] - 1\ndfhogg.set_index('id', inplace=True)\n\n## standardize (mean center and divide by 1 sd)\ndfhoggs = (dfhogg[['x','y']] - dfhogg[['x','y']].mean(0)) / dfhogg[['x','y']].std(0)\ndfhoggs['sigma_y'] = dfhogg['sigma_y'] / dfhogg['y'].std(0)\ndfhoggs['sigma_x'] = dfhogg['sigma_x'] / dfhogg['x'].std(0)\n\n## scatterplot the standardized data\ng = sns.FacetGrid(dfhoggs, size=8)\n_ = g.map(plt.errorbar, 'x', 'y', 'sigma_y', 'sigma_x', marker= o , ls='')\n_ = g.axes[0][0].set_ylim((-3,3))\n_ = g.axes[0][0].set_xlim((-3,3))\n\nplt.subplots_adjust(top=0.92)\n_ = g.fig.suptitle('Scatterplot of Hogg 2010 dataset after standardization', fontsize=16)   Observe :     Even judging just by eye, you can see these datapoints mostly fall on / around a straight line with positive gradient  It looks like a few of the datapoints may be outliers from such a line",
            "title": "Load and Prepare Data"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#create-conventional-ols-model",
            "text": "The  linear model  is really simple and conventional:  $$\\bf{y} = \\beta^{T} \\bf{X} + \\bf{\\sigma}$$  where:    $\\beta$  = coefs =  $\\{1, \\beta_{j \\in X_{j}}\\}$  $\\sigma$  = the measured error in  $y$  in the dataset  sigma_y  Define model  NOTE: \n+ We're using a simple linear OLS model with Normally distributed priors so that it behaves like a ridge regression  with pm.Model() as mdl_ols:\n\n    ## Define weakly informative Normal priors to give Ridge regression\n    b0 = pm.Normal('b0_intercept', mu=0, sd=100)\n    b1 = pm.Normal('b1_slope', mu=0, sd=100)\n\n    ## Define linear model\n    yest = b0 + b1 * dfhoggs['x']\n\n    ## Use y error from dataset, convert into theano variable\n    sigma_y = thno.shared(np.asarray(dfhoggs['sigma_y'],\n                            dtype=thno.config.floatX), name='sigma_y')\n\n    ## Define Normal likelihood\n    likelihood = pm.Normal('likelihood', mu=yest, sd=sigma_y**2, observed=dfhoggs['y'])  Sample  with mdl_ols:\n\n    ## find MAP using Powell, seems to be more robust\n    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)\n\n    ## take samples\n    traces_ols = pm.sample(2000, start=start_MAP, step=pm.NUTS(), progressbar=True)  Optimization terminated successfully.\n         Current function value: 4555.263214\n         Iterations: 3\n         Function evaluations: 110\n [-----------------100%-----------------] 2000 of 2000 complete in 0.6 sec  View Traces  NOTE : I'll 'burn' the traces to only retain the final 1000 samples  _ = pm.traceplot(traces_ols[-1000:], figsize=(12,len(traces_ols.varnames)*1.5),\n                lines={k: v['mean'] for k, v in pm.df_summary(traces_ols[-1000:]).iterrows()})   NOTE:  We'll illustrate this OLS fit and compare to the datapoints in the final plot",
            "title": "Create Conventional OLS Model"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#create-robust-model-student-t-method",
            "text": "I've added this brief section in order to directly compare the Student-T based method exampled in Thomas Wiecki's notebook in the  PyMC3 documentation  Instead of using a Normal distribution for the likelihood, we use a Student-T, which has fatter tails. In theory this allows outliers to have a smaller mean square error in the likelihood, and thus have less influence on the regression estimation. This method does not produce inlier / outlier flags but is simpler and faster to run than the Signal Vs Noise model below, so a comparison seems worthwhile.  Note:  we'll constrain the Student-T 'degrees of freedom' parameter  nu  to be an integer, but otherwise leave it as just another stochastic to be inferred: no need for prior knowledge.  Define Model  with pm.Model() as mdl_studentt:\n\n    ## Define weakly informative Normal priors to give Ridge regression\n    b0 = pm.Normal('b0_intercept', mu=0, sd=100)\n    b1 = pm.Normal('b1_slope', mu=0, sd=100)\n\n    ## Define linear model\n    yest = b0 + b1 * dfhoggs['x']\n\n    ## Use y error from dataset, convert into theano variable\n    sigma_y = thno.shared(np.asarray(dfhoggs['sigma_y'],\n                            dtype=thno.config.floatX), name='sigma_y')\n\n    ## define prior for Student T degrees of freedom\n    nu = pm.DiscreteUniform('nu', lower=1, upper=100)\n\n    ## Define Student T likelihood\n    likelihood = pm.StudentT('likelihood', mu=yest, sd=sigma_y**2, nu=nu\n                           ,observed=dfhoggs['y'])  Sample  with mdl_studentt:\n\n    ## find MAP using Powell, seems to be more robust\n    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)\n\n    ## two-step sampling to allow Metropolis for nu (which is discrete)\n    step1 = pm.NUTS([b0, b1])\n    step2 = pm.Metropolis([nu])\n\n    ## take samples\n    traces_studentt = pm.sample(2000, start=start_MAP, step=[step1, step2], progressbar=True)  Optimization terminated successfully.\n         Current function value: 430.443483\n         Iterations: 3\n         Function evaluations: 86\n [-----------------100%-----------------] 2000 of 2000 complete in 1.3 sec  View Traces  _ = pm.traceplot(traces_studentt[-1000:]\n            ,figsize=(12,len(traces_studentt.varnames)*1.5)\n            ,lines={k: v['mean'] for k, v in pm.df_summary(traces_studentt[-1000:]).iterrows()})   Observe:   Both parameters  b0  and  b1  show quite a skew to the right, possibly this is the action of a few samples regressing closer to the OLS estimate which is towards the left  The  nu  parameter seems very happy to stick at  nu = 1 , indicating that a fat-tailed Student-T likelihood has a better fit than a thin-tailed (Normal-like) Student-T likelihood.  The inference sampling also ran very quickly, almost as quickly as the conventional OLS   NOTE:  We'll illustrate this Student-T fit and compare to the datapoints in the final plot",
            "title": "Create Robust Model: Student-T Method"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#create-robust-model-with-outliers-hogg-method",
            "text": "Please read the paper (Hogg 2010) and Jake Vanderplas' code for more complete information about the modelling technique.  The general idea is to create a 'mixture' model whereby datapoints can be described by either the linear model (inliers) or a modified linear model with different mean and larger variance (outliers).  The likelihood is evaluated over a mixture of two likelihoods, one for 'inliers', one for 'outliers'. A Bernouilli distribution is used to randomly assign datapoints in N to either the inlier or outlier groups, and we sample the model as usual to infer robust model parameters and inlier / outlier flags:  $$\n\\mathcal{logL} = \\sum_{i}^{i=N} log \\left[ \\frac{(1 - B_{i})}{\\sqrt{2 \\pi \\sigma_{in}^{2}}} exp \\left( - \\frac{(x_{i} - \\mu_{in})^{2}}{2\\sigma_{in}^{2}} \\right) \\right] + \\sum_{i}^{i=N} log \\left[ \\frac{B_{i}}{\\sqrt{2 \\pi (\\sigma_{in}^{2} + \\sigma_{out}^{2})}} exp \\left( - \\frac{(x_{i}- \\mu_{out})^{2}}{2(\\sigma_{in}^{2} + \\sigma_{out}^{2})} \\right) \\right]\n$$  where:  $\\bf{B}$  is Bernoulli-distibuted  $B_{i} \\in [0_{(inlier)},1_{(outlier)}]$  Define model  def logp_signoise(yobs, is_outlier, yest_in, sigma_y_in, yest_out, sigma_y_out):\n    '''\n    Define custom loglikelihood for inliers vs outliers. \n    NOTE: in this particular case we don't need to use theano's @as_op \n    decorator because (as stated by Twiecki in conversation) that's only \n    required if the likelihood cannot be expressed as a theano expression.\n    We also now get the gradient computation for free.\n    '''   \n\n    # likelihood for inliers\n    pdfs_in = T.exp(-(yobs - yest_in + 1e-6)**2 / (2 * sigma_y_in**2)) \n    pdfs_in /= T.sqrt(2 * np.pi * sigma_y_in**2)\n    logL_in = T.sum(T.log(pdfs_in) * (1 - is_outlier))\n\n    # likelihood for outliers\n    pdfs_out = T.exp(-(yobs - yest_out + 1e-6)**2 / (2 * (sigma_y_in**2 + sigma_y_out**2))) \n    pdfs_out /= T.sqrt(2 * np.pi * (sigma_y_in**2 + sigma_y_out**2))\n    logL_out = T.sum(T.log(pdfs_out) * is_outlier)\n\n    return logL_in + logL_out  with pm.Model() as mdl_signoise:\n\n    ## Define weakly informative Normal priors to give Ridge regression\n    b0 = pm.Normal('b0_intercept', mu=0, sd=100)\n    b1 = pm.Normal('b1_slope', mu=0, sd=100)\n\n    ## Define linear model\n    yest_in = b0 + b1 * dfhoggs['x']\n\n    ## Define weakly informative priors for the mean and variance of outliers\n    yest_out = pm.Normal('yest_out', mu=0, sd=100)\n    sigma_y_out = pm.HalfNormal('sigma_y_out', sd=100)\n\n    ## Define Bernoulli inlier / outlier flags according to a hyperprior \n    ## fraction of outliers, itself constrained to [0,.5] for symmetry\n    frac_outliers = pm.Uniform('frac_outliers', lower=0., upper=.5)\n    is_outlier = pm.Bernoulli('is_outlier', p=frac_outliers, shape=dfhoggs.shape[0])  \n\n    ## Extract observed y and sigma_y from dataset, encode as theano objects\n    yobs = thno.shared(np.asarray(dfhoggs['y'], dtype=thno.config.floatX), name='yobs')\n    sigma_y_in = thno.shared(np.asarray(dfhoggs['sigma_y']\n                                , dtype=thno.config.floatX), name='sigma_y_in')\n\n    ## Use custom likelihood using DensityDist\n    likelihood = pm.DensityDist('likelihood', logp_signoise,\n                        observed={'yobs':yobs, 'is_outlier':is_outlier,\n                                  'yest_in':yest_in, 'sigma_y_in':sigma_y_in,\n                                  'yest_out':yest_out, 'sigma_y_out':sigma_y_out})  Sample  with mdl_signoise:\n\n    ## two-step sampling to create Bernoulli inlier/outlier flags\n    step1 = pm.NUTS([frac_outliers, yest_out, sigma_y_out, b0, b1])\n    step2 = pm.BinaryMetropolis([is_outlier])\n\n    ## find MAP using Powell, seems to be more robust\n    start_MAP = pm.find_MAP(fmin=optimize.fmin_powell, disp=True)\n\n    ## take samples\n    traces_signoise = pm.sample(2000, start=start_MAP, step=[step1,step2], progressbar=True)  Optimization terminated successfully.\n         Current function value: 155.449990\n         Iterations: 3\n         Function evaluations: 202\n [-----------------100%-----------------] 2000 of 2000 complete in 180.3 sec  View Traces  _ = pm.traceplot(traces_signoise[-1000:], figsize=(12,len(traces_signoise.varnames)*1.5),\n            lines={k: v['mean'] for k, v in pm.df_summary(traces_signoise[-1000:]).iterrows()})   NOTE: :   During development I've found that 3 datapoints id=[1,2,3] are always indicated as outliers, but the remaining ordering of datapoints by decreasing outlier-hood is unstable between runs: the posterior surface appears to have a small number of solutions with very similar probability.  The NUTS sampler seems to work okay, and indeed it's a nice opportunity to demonstrate a custom likelihood which is possible to express as a theano function (thus allowing a gradient-based sampler like NUTS). However, with a more complicated dataset, I would spend time understanding this instability and potentially prefer using more samples under Metropolis-Hastings.",
            "title": "Create Robust Model with Outliers: Hogg Method"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#declare-outliers-and-compare-plots",
            "text": "View ranges for inliers / outlier predictions  At each step of the traces, each datapoint may be either an inlier or outlier. We hope that the datapoints spend an unequal time being one state or the other, so let's take a look at the simple count of states for each of the 20 datapoints.  outlier_melt = pd.melt(pd.DataFrame(traces_signoise['is_outlier', -1000:],\n                                   columns=['[{}]'.format(int(d)) for d in dfhoggs.index]),\n                      var_name='datapoint_id', value_name='is_outlier')\nax0 = sns.pointplot(y='datapoint_id', x='is_outlier', data=outlier_melt,\n                   kind='point', join=False, ci=None, size=4, aspect=2)\n\n_ = ax0.vlines([0,1], 0, 19, ['b','r'], '--')\n\n_ = ax0.set_xlim((-0.1,1.1))\n_ = ax0.set_xticks(np.arange(0, 1.1, 0.1))\n_ = ax0.set_xticklabels(['{:.0%}'.format(t) for t in np.arange(0,1.1,0.1)])\n\n_ = ax0.yaxis.grid(True, linestyle='-', which='major', color='w', alpha=0.4)\n_ = ax0.set_title('Prop. of the trace where datapoint is an outlier')\n_ = ax0.set_xlabel('Prop. of the trace where is_outlier == 1')   Observe :   The plot above shows the number of samples in the traces in which each datapoint is marked as an outlier, expressed as a percentage.  In particular, 3 points [1, 2, 3] spend  =95% of their time as outliers  Contrastingly, points at the other end of the plot close to 0% are our strongest inliers.  For comparison, the mean posterior value of  frac_outliers  is ~0.35, corresponding to roughly 7 of the 20 datapoints. You can see these 7 datapoints in the plot above, all those with a value  50% or thereabouts.  However, only 3 of these points are outliers  =95% of the time.   See note above regarding instability between runs.   The 95% cutoff we choose is subjective and arbitrary, but I prefer it for now, so let's declare these 3 to be outliers and see how it looks compared to Jake Vanderplas' outliers, which were declared in a slightly different way as points with means above 0.68.  Declare outliers  Note: \n+ I will declare outliers to be datapoints that have value == 1 at the 5-percentile cutoff, i.e. in the percentiles from 5 up to 100, their values are 1. \n+ Try for yourself altering cutoff to larger values, which leads to an objective ranking of outlier-hood.  cutoff = 5\ndfhoggs['outlier'] = np.percentile(traces_signoise[-1000:]['is_outlier'],cutoff, axis=0)\ndfhoggs['outlier'].value_counts()  0    17\n1     3\nName: outlier, dtype: int64",
            "title": "Declare Outliers and Compare Plots"
        },
        {
            "location": "/RobustRegression_OutlierDetection_Hogg/#posterior-prediction-plots-for-ols-vs-studentt-vs-signalnoise",
            "text": "g = sns.FacetGrid(dfhoggs, size=8, hue='outlier', hue_order=[True,False],\n                  palette='Set1', legend_out=False)\n\nlm = lambda x, samp: samp['b0_intercept'] + samp['b1_slope'] * x\n\npm.glm.plot_posterior_predictive(traces_ols[-1000:],\n        eval=np.linspace(-3, 3, 10), lm=lm, samples=200, color='#00AA00', alpha=.1)\n\npm.glm.plot_posterior_predictive(traces_studentt[-1000:], lm=lm,\n        eval=np.linspace(-3, 3, 10), samples=200, color='#FF9900', alpha=.3)\n\npm.glm.plot_posterior_predictive(traces_signoise[-1000:], lm=lm,\n        eval=np.linspace(-3, 3, 10), samples=200,\n        color=sns.palettes.color_palette('Set1')[1], alpha=.3)\n\n_ = g.map(plt.errorbar, 'x', 'y', 'sigma_y', 'sigma_x', marker= o , ls='').add_legend()\n\n_ = g.axes[0][0].annotate('OLS Fit: Green\\nStudent-T Fit: Orange\\nSignal Vs Noise Fit: Blue',\n                          size='x-large', xy=(1,0), xycoords='axes fraction',\n                          xytext=(-160,10), textcoords='offset points')\n_ = g.axes[0][0].set_ylim((-3,3))   Observe :    The posterior preditive fit for:   the  OLS model  is shown in  Green  and as expected, it doesn't appear to fit the majority of our datapoints very well, skewed by outliers  the  Robust Student-T model  is shown in  Orange  and does appear to fit the 'main axis' of datapoints quite well, ignoring outliers  the  Robust Signal vs Noise model  is shown in  Blue  and also appears to fit the 'main axis' of datapoints rather well, ignoring outliers.     We see that the  Robust Signal vs Noise model  also yields specific estimates of  which  datapoints are outliers:   17 'inlier' datapoints, in  Blue  and  3 'outlier' datapoints shown in  Red .  From a simple visual inspection, the classification seems fair, and agrees with Jake Vanderplas' findings.     Overall, it seems that:   the  Signal vs Noise model  behaves as promised, yielding a robust regression estimate and explicit labelling of inliers / outliers, but  the  Signal vs Noise model  is quite complex and whilst the regression seems robust and stable, the actual inlier / outlier labelling seems slightly unstable  if you simply want a robust regression without inlier / outlier labelling, the  Student-T model  may be a good compromise, offering a simple model, quick sampling, and a very similar estimate.",
            "title": "Posterior Prediction Plots for OLS vs StudentT vs SignalNoise"
        },
        {
            "location": "/GLM-hierarchical/",
            "text": "The best of both worlds: Hierarchical Linear Regression in PyMC3\n\n\nAuthors: Danne Elbers, Thomas Wiecki\n\n\nToday's blog post is co-written by \nDanne Elbers\n who is doing her masters thesis with me on computational psychiatry using Bayesian modeling. This post also borrows heavily from a \nNotebook\n by \nChris Fonnesbeck\n.\n\n\nThe power of Bayesian modelling really clicked for me when I was first introduced to hierarchical modelling. In this blog post we will:\n\n\n\n\nprovide and intuitive explanation of hierarchical/multi-level Bayesian modeling;\n\n\nshow how this type of model can easily be built and estimated in \nPyMC3\n;\n\n\ndemonstrate the advantage of using hierarchical Bayesian modelling as opposed to non-hierarchical Bayesian modelling by comparing the two;\n\n\nvisualize the \"shrinkage effect\" (explained below); and\n\n\nhighlight connections to the frequentist version of this model.\n\n\n\n\nHaving multiple sets of related measurements comes up all the time. In mathematical psychology, for example, you test multiple subjects on the same task. We then want to estimate a computational/mathematical model that describes the behavior on the task by a set of parameters. We could thus fit a model to each subject individually, assuming they share no similarities; or, pool all the data and estimate one model assuming all subjects are identical. Hierarchical modeling allows the best of both worlds by modeling subjects' similarities but also allowing estimiation of individual parameters. As an aside, software from our lab, \nHDDM\n, allows hierarchical Bayesian estimation of a widely used decision making model in psychology. In this blog post, however, we will use a more classical example of \nhierarchical linear regression\n to predict radon levels in houses.\n\n\nThis is the 3rd blog post on the topic of Bayesian modeling in PyMC3, see here for the previous two:\n\n\n\n\nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n  \n\n\n\n\nThe data set\n\n\nGelman et al.'s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all counties of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil.\nHere we'll investigate this differences and try to make predictions of radonlevels in different counties based on the county itself and the presence of a basement. In this example we'll look at Minnesota, a state that contains 85 counties in which different measurements are taken, ranging from 2 to 116 measurements per county. \n\n\n\n\nFirst, we'll load the data: \n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('data/radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\nn_counties = len(data.county.unique())\n\n\n\n\nThe relevant part of the data we will model looks as follows:\n\n\ndata[['county', 'log_radon', 'floor']].head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ncounty\n\n      \nlog_radon\n\n      \nfloor\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n AITKIN\n\n      \n 0.832909\n\n      \n 1\n\n    \n\n    \n\n      \n1\n\n      \n AITKIN\n\n      \n 0.832909\n\n      \n 0\n\n    \n\n    \n\n      \n2\n\n      \n AITKIN\n\n      \n 1.098612\n\n      \n 0\n\n    \n\n    \n\n      \n3\n\n      \n AITKIN\n\n      \n 0.095310\n\n      \n 0\n\n    \n\n    \n\n      \n4\n\n      \n  ANOKA\n\n      \n 1.163151\n\n      \n 0\n\n    \n\n  \n\n\n\n\n\n\n\nAs you can see, we have multiple \nradon\n measurements (log-converted to be on the real line) -- one row for each house -- in a \ncounty\n and whether the house has a basement (\nfloor\n == 0) or not (\nfloor\n == 1). We are interested in whether having a basement increases the \nradon\n measured in the house. \n\n\nThe Models\n\n\nPooling of measurements\n\n\nNow you might say: \"That's easy! I'll just pool all my data and estimate one big regression to asses the influence of a basement across all counties\". In math-speak that model would be:\n\n\n$$radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon$$\n \n\n\nWhere \n$i$\n represents the measurement, \n$c$\n the county and floor contains a 0 or 1 if the house has a basement or not, respectively. If you need a refresher on Linear Regressions in \nPyMC\n, check out my \nprevious blog post\n. Critically, we are only estimating \none\n intercept and \none\n slope for all measurements over all counties pooled together as illustrated in the graphic below (\n$\\theta$\n represents \n$(\\alpha, \\beta)$\n in our case and \n$y_i$\n are the measurements of the \n$i$\nth county).\n\n\n\n\nUnpooled measurements: separate regressions\n\n\nBut what if we are interested in whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say \"OK then, I'll just estimate \n$n$\n (number of counties) different regressions -- one for each county\". In math-speak that model would be:\n\n\n$$radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c$$\n\n\nNote that we added the subindex \n$c$\n so we are estimating \n$n$\n different \n$\\alpha$\ns and \n$\\beta$\ns -- one for each county.\n\n\n\n\nThis is the extreme opposite model; where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever. As we show below, this type of model can be very noisy when we have little data per county, as is the case in this data set.\n\n\nPartial pooling: Hierarchical Regression aka, the best of both worlds\n\n\nFortunately, there is a middle ground to both of these extremes. Specifically, we may assume that while \n$\\alpha$\ns and \n$\\beta$\ns are different for each county as in the unpooled case, the coefficients all share similarity. We can model this by assuming that each individual coefficient comes from a common group distribution:\n\n\n$$\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$\n\n\n$$\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)$$\n\n\nWe thus assume the intercepts \n$\\alpha$\n and slopes \n$\\beta$\n to come from a normal distribution centered around their respective group mean \n$\\mu$\n with a certain standard deviation \n$\\sigma^2$\n, the values (or rather posteriors) of which we also estimate. That's why this is called a multilevel, hierarchical or partial-pooling modeling.\n\n\n\n\nHow do we estimate such a complex model you might ask? Well, that's the beauty of Probabilistic Programming -- we just formulate the model we want and press our \nInference Button(TM)\n. \n\n\n(Note that the above is not a complete Bayesian model specification as we haven't defined priors or hyperpriors (i.e. priors for the group distribution, \n$\\mu$\n and \n$\\sigma$\n). These will be used in the model implementation below but only distract here.)\n\n\nProbabilistic Programming\n\n\nUnpooled/non-hierarchical model\n\n\nTo really highlight the effect of the hierarchical linear regression we'll first estimate the non-hierarchical, unpooled Bayesian model from above (separate regressions). For each county we estimate a completely separate model. As we have no prior information on what the intercept or regressions could be, we will be using a normal distribution centered around 0 with a wide standard-deviation to describe the intercept and regressions. We'll assume the measurements are normally distributed with noise \n$\\epsilon$\n on which we place a uniform distribution.  \n\n\nindiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.ix[data.county == county_name]\n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n\n    with pm.Model() as individual_model:\n        # Intercept prior (variance == sd**2)\n        a = pm.Normal('alpha', mu=0, sd=100**2)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sd=100**2)\n\n        # Model error prior\n        eps = pm.Uniform('eps', lower=0, upper=100)\n\n        # Linear model\n        radon_est = a + b * c_floor_measure\n\n        # Data likelihood\n        radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        step = pm.NUTS()\n        trace = pm.sample(2000, step=step, progressbar=False)\n\n    # keep trace for later analysis\n    indiv_traces[county_name] = trace\n\n\n\n\nHierarchical Model\n\n\nInstead of creating models separatley, the hierarchical model creates group parameters that consider the countys not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county's \n$\\alpha$\n and \n$\\beta$\n.\n\n\nwith pm.Model() as hierarchical_model:\n    # Hyperpriors for group nodes\n    mu_a = pm.Normal('mu_alpha', mu=0., sd=100**2)\n    sigma_a = pm.Uniform('sigma_alpha', lower=0, upper=100)\n    mu_b = pm.Normal('mu_beta', mu=0., sd=100**2)\n    sigma_b = pm.Uniform('sigma_beta', lower=0, upper=100)\n\n    # Intercept for each county, distributed around group mean mu_a\n    # Above we just set mu and sd to a fixed value while here we\n    # plug in a common group distribution for all a and b (which are\n    # vectors of length n_counties).\n    a = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=n_counties)\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=n_counties)\n\n    # Model error\n    eps = pm.Uniform('eps', lower=0, upper=100)\n\n    # Model prediction of radon level\n    # a[county_idx] translates to a[0, 0, 0, 1, 1, ...],\n    # we thus link multiple household measures of a county\n    # to its coefficients.\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n\n    # Data likelihood\n    radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=data.log_radon)\n\n\n\n\n# Inference button (TM)!\nwith hierarchical_model:\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    hierarchical_trace = pm.sample(2000, step, start=start, progressbar=False)\n\n\n\n\n# Plotting the hierarchical model trace -its found values- from 500 iterations onwards (right side plot) \n# and its accumulated marginal values (left side plot) \npm.traceplot(hierarchical_trace[500:]);\n\n\n\n\n\n\nThe marginal posteriors in the left column are highly informative. \nmu_a\n tells us the group mean (log) radon levels. \nmu_b\n tells us that having no basement decreases radon levels significantly (no mass above zero). We can also see by looking at the marginals for \na\n that there is quite some differences in radon levels between counties (each 'rainbow' color corresponds to a single county); the different widths are related to how much confidence we have in each paramter estimate -- the more measurements per county, the higher our confidence will be.\n\n\nPosterior Predictive Check\n\n\nThe Root Mean Square Deviation\n\n\nTo find out which of the models explains the data better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.\n\n\nWhen computing the RMSD (code not shown) we get the following result:\n\n\n\n\nindividual/non-hierarchical model: 0.13\n\n\nhierarchical model: 0.08\n\n\n\n\nAs can be seen above the hierarchical model performs better than the non-hierarchical model in predicting the radon values. Following this, we'll plot some examples of county's showing the actual radon measurements, the hierarchial predictions and the non-hierarchical predictions. \n\n\nselection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.ix[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][500::10], indiv_traces[c]['beta'][500::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.1)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][500::10].mean() + indiv_traces[c]['beta'][500::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][500::10][z], hierarchical_trace['beta'][500::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.1)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][500::10][z].mean() + hierarchical_trace['beta'][500::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'no basement'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')\n\n\n\n\n\n\nIn the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.\n\n\nWhen looking at the county 'CASS' we see that the non-hierarchical estimation is strongly biased: as this county's data contains only households with a basement the estimated regression produces the non-sensical result of a giant negative slope meaning that we would expect negative radon levels in a house without basement!\n\n\nMoreover, in the example county's 'CROW WING' and 'FREEBORN' the non-hierarchical model appears to react more strongly than the hierarchical model to the existance of outliers in the dataset ('CROW WING': no basement upper right. 'FREEBORN': basement upper left). Assuming that there should be a higher amount of radon gas measurable in households with basements opposed to those without, the county 'CROW WING''s non-hierachical model seems off. Having the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa.\n\n\nShrinkage\n\n\nShrinkage describes the process by which our estimates are \"pulled\" towards the group-mean as a result of the common group distribution -- county-coefficients very far away from the group mean have very low probability under the normality assumption, moving them closer to the group mean gives them higher probability. In the non-hierachical model every county is allowed to differ completely from the others by just using each county's data, resulting in a model more prone to outliers (as shown above). \n\n\nhier_a = hierarchical_trace['alpha'][500:].mean(axis=0)\nhier_b = hierarchical_trace['beta'][500:].mean(axis=0)\nindv_a = [indiv_traces[c]['alpha'][500:].mean() for c in county_names]\nindv_b = [indiv_traces[c]['beta'][500:].mean() for c in county_names]\n\n\n\n\nfig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, xlabel='Intercept', ylabel='Floor Measure', \n                     title='Hierarchical vs. Non-hierarchical Bayes', \n                     xlim=(0, 3), ylim=(-3, 3))\n\nax.scatter(indv_a,indv_b, s=26, alpha=0.4, label = 'non-hierarchical')\nax.scatter(hier_a,hier_b, c='red', s=26, alpha=0.4, label = 'hierarchical')\nfor i in range(len(indv_b)):  \n    ax.arrow(indv_a[i], indv_b[i], hier_a[i] - indv_a[i], hier_b[i] - indv_b[i], \n             fc=\nk\n, ec=\nk\n, length_includes_head=True, alpha=0.4, head_width=.04)\nax.legend();\n\n\n\n\n\n\nIn the shrinkage plot above we show the coefficients of each county's non-hierarchical posterior mean (blue) and the hierarchical posterior mean (red). To show the effect of shrinkage on a single coefficient-pair (alpha and beta) we connect the blue and red points belonging to the same county by an arrow. Some non-hierarchical posteriors are so far out that we couldn't display them in this plot (it makes the axes too wide). Interestingly, all hierarchical posteriors of the floor-measure seem to be around -0.6 indicating that having a basement in almost all county's is a clear indicator for heightened radon levels. The intercept (which we take for type of soil) appears to differ among countys. This information would have been difficult to find if we had only used the non-hierarchial model.\n\n\nCritically, many effects that look quite large and significant in the non-hiearchical model actually turn out to be much smaller when we take the group distribution into account (this point can also well be seen in plot \nIn[12]\n in \nChris' NB\n). Shrinkage can thus be viewed as a form of smart regularization that helps reduce false-positives!\n\n\nConnections to Frequentist statistics\n\n\nThis type of hierarchical, partial pooling model is known as a \nrandom effects model\n in frequentist terms. Interestingly, if we placed uniform priors on the group mean and variance in the above model, the resulting Bayesian model would be equivalent to a random effects model. One might imagine that the difference between a model with uniform or wide normal hyperpriors should not have a huge impact. However, \nGelman says\n encourages use of weakly-informative priors (like we did above) over flat priors.\n\n\nSummary\n\n\nIn this post, co-authored by Danne Elbers, we showed how a multi-level hierarchical Bayesian model gives the best of both worlds when we have multiple sets of measurements we expect to have similarity. The naive approach either pools all data together and ignores the individual differences, or treats each set as completely separate leading to noisy estimates, as shown above. By assuming that each individual data set (each county in our case) is distributed according to a group distribution -- which we simultaneously estimate -- we benefit from increased statistical power and smart regularization via the shrinkage effect. Probabilistic Programming in \nPyMC\n then makes Bayesian estimation of this model trivial.\n\n\nAs a follow-up we could also include other states into our model. For this we could add yet another layer to the hierarchy where each state is pooled at the country level. Finally, readers of my blog will notice that we didn't use \nglm()\n here as it does not play nice with hierarchical models yet.\n\n\nReferences\n\n\n\n\nThe underlying Notebook of this blog post\n\n\nBlog post: \nThe Inference Button: Bayesian GLMs made easy with PyMC3\n\n\nBlog post: \nThis world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3\n  \n\n\nChris Fonnesbeck repo containing a more extensive analysis\n\n\nBlog post: \nShrinkage in multi-level hierarchical models\n by John Kruschke\n\n\nGelman, A.; Carlin; Stern; and Rubin, D., 2007, \"Replication data for: Bayesian Data Analysis, Second Edition\", \n\n\nGelman, A., \n Hill, J. (2006). \nData Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.\n\n\nGelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432\u2013435.\n\n\n\n\nAcknowledgements\n\n\nThanks to \nImri Sofer\n for feedback and teaching us about the connections to random-effects models and \nDan Dillon\n for useful comments on an earlier draft.",
            "title": "Hierarchical linear regression"
        },
        {
            "location": "/GLM-hierarchical/#the-best-of-both-worlds-hierarchical-linear-regression-in-pymc3",
            "text": "Authors: Danne Elbers, Thomas Wiecki  Today's blog post is co-written by  Danne Elbers  who is doing her masters thesis with me on computational psychiatry using Bayesian modeling. This post also borrows heavily from a  Notebook  by  Chris Fonnesbeck .  The power of Bayesian modelling really clicked for me when I was first introduced to hierarchical modelling. In this blog post we will:   provide and intuitive explanation of hierarchical/multi-level Bayesian modeling;  show how this type of model can easily be built and estimated in  PyMC3 ;  demonstrate the advantage of using hierarchical Bayesian modelling as opposed to non-hierarchical Bayesian modelling by comparing the two;  visualize the \"shrinkage effect\" (explained below); and  highlight connections to the frequentist version of this model.   Having multiple sets of related measurements comes up all the time. In mathematical psychology, for example, you test multiple subjects on the same task. We then want to estimate a computational/mathematical model that describes the behavior on the task by a set of parameters. We could thus fit a model to each subject individually, assuming they share no similarities; or, pool all the data and estimate one model assuming all subjects are identical. Hierarchical modeling allows the best of both worlds by modeling subjects' similarities but also allowing estimiation of individual parameters. As an aside, software from our lab,  HDDM , allows hierarchical Bayesian estimation of a widely used decision making model in psychology. In this blog post, however, we will use a more classical example of  hierarchical linear regression  to predict radon levels in houses.  This is the 3rd blog post on the topic of Bayesian modeling in PyMC3, see here for the previous two:   The Inference Button: Bayesian GLMs made easy with PyMC3  This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3",
            "title": "The best of both worlds: Hierarchical Linear Regression in PyMC3"
        },
        {
            "location": "/GLM-hierarchical/#the-data-set",
            "text": "Gelman et al.'s (2007) radon dataset is a classic for hierarchical modeling. In this dataset the amount of the radioactive gas radon has been measured among different households in all counties of several states. Radon gas is known to be the highest cause of lung cancer in non-smokers. It is believed to be more strongly present in households containing a basement and to differ in amount present among types of soil.\nHere we'll investigate this differences and try to make predictions of radonlevels in different counties based on the county itself and the presence of a basement. In this example we'll look at Minnesota, a state that contains 85 counties in which different measurements are taken, ranging from 2 to 116 measurements per county.    First, we'll load the data:   %matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc3 as pm \nimport pandas as pd\n\ndata = pd.read_csv('data/radon.csv')\n\ncounty_names = data.county.unique()\ncounty_idx = data['county_code'].values\nn_counties = len(data.county.unique())  The relevant part of the data we will model looks as follows:  data[['county', 'log_radon', 'floor']].head()   \n   \n     \n       \n       county \n       log_radon \n       floor \n     \n   \n   \n     \n       0 \n        AITKIN \n        0.832909 \n        1 \n     \n     \n       1 \n        AITKIN \n        0.832909 \n        0 \n     \n     \n       2 \n        AITKIN \n        1.098612 \n        0 \n     \n     \n       3 \n        AITKIN \n        0.095310 \n        0 \n     \n     \n       4 \n         ANOKA \n        1.163151 \n        0 \n     \n      As you can see, we have multiple  radon  measurements (log-converted to be on the real line) -- one row for each house -- in a  county  and whether the house has a basement ( floor  == 0) or not ( floor  == 1). We are interested in whether having a basement increases the  radon  measured in the house.",
            "title": "The data set"
        },
        {
            "location": "/GLM-hierarchical/#the-models",
            "text": "",
            "title": "The Models"
        },
        {
            "location": "/GLM-hierarchical/#pooling-of-measurements",
            "text": "Now you might say: \"That's easy! I'll just pool all my data and estimate one big regression to asses the influence of a basement across all counties\". In math-speak that model would be:  $$radon_{i, c} = \\alpha + \\beta*\\text{floor}_{i, c} + \\epsilon$$    Where  $i$  represents the measurement,  $c$  the county and floor contains a 0 or 1 if the house has a basement or not, respectively. If you need a refresher on Linear Regressions in  PyMC , check out my  previous blog post . Critically, we are only estimating  one  intercept and  one  slope for all measurements over all counties pooled together as illustrated in the graphic below ( $\\theta$  represents  $(\\alpha, \\beta)$  in our case and  $y_i$  are the measurements of the  $i$ th county).",
            "title": "Pooling of measurements"
        },
        {
            "location": "/GLM-hierarchical/#unpooled-measurements-separate-regressions",
            "text": "But what if we are interested in whether different counties actually have different relationships (slope) and different base-rates of radon (intercept)? Then you might say \"OK then, I'll just estimate  $n$  (number of counties) different regressions -- one for each county\". In math-speak that model would be:  $$radon_{i, c} = \\alpha_{c} + \\beta_{c}*\\text{floor}_{i, c} + \\epsilon_c$$  Note that we added the subindex  $c$  so we are estimating  $n$  different  $\\alpha$ s and  $\\beta$ s -- one for each county.   This is the extreme opposite model; where above we assumed all counties are exactly the same, here we are saying that they share no similarities whatsoever. As we show below, this type of model can be very noisy when we have little data per county, as is the case in this data set.",
            "title": "Unpooled measurements: separate regressions"
        },
        {
            "location": "/GLM-hierarchical/#partial-pooling-hierarchical-regression-aka-the-best-of-both-worlds",
            "text": "Fortunately, there is a middle ground to both of these extremes. Specifically, we may assume that while  $\\alpha$ s and  $\\beta$ s are different for each county as in the unpooled case, the coefficients all share similarity. We can model this by assuming that each individual coefficient comes from a common group distribution:  $$\\alpha_{c} \\sim \\mathcal{N}(\\mu_{\\alpha}, \\sigma_{\\alpha}^2)$$  $$\\beta_{c} \\sim \\mathcal{N}(\\mu_{\\beta}, \\sigma_{\\beta}^2)$$  We thus assume the intercepts  $\\alpha$  and slopes  $\\beta$  to come from a normal distribution centered around their respective group mean  $\\mu$  with a certain standard deviation  $\\sigma^2$ , the values (or rather posteriors) of which we also estimate. That's why this is called a multilevel, hierarchical or partial-pooling modeling.   How do we estimate such a complex model you might ask? Well, that's the beauty of Probabilistic Programming -- we just formulate the model we want and press our  Inference Button(TM) .   (Note that the above is not a complete Bayesian model specification as we haven't defined priors or hyperpriors (i.e. priors for the group distribution,  $\\mu$  and  $\\sigma$ ). These will be used in the model implementation below but only distract here.)",
            "title": "Partial pooling: Hierarchical Regression aka, the best of both worlds"
        },
        {
            "location": "/GLM-hierarchical/#probabilistic-programming",
            "text": "",
            "title": "Probabilistic Programming"
        },
        {
            "location": "/GLM-hierarchical/#unpoolednon-hierarchical-model",
            "text": "To really highlight the effect of the hierarchical linear regression we'll first estimate the non-hierarchical, unpooled Bayesian model from above (separate regressions). For each county we estimate a completely separate model. As we have no prior information on what the intercept or regressions could be, we will be using a normal distribution centered around 0 with a wide standard-deviation to describe the intercept and regressions. We'll assume the measurements are normally distributed with noise  $\\epsilon$  on which we place a uniform distribution.    indiv_traces = {}\nfor county_name in county_names:\n    # Select subset of data belonging to county\n    c_data = data.ix[data.county == county_name]\n    c_log_radon = c_data.log_radon\n    c_floor_measure = c_data.floor.values\n\n    with pm.Model() as individual_model:\n        # Intercept prior (variance == sd**2)\n        a = pm.Normal('alpha', mu=0, sd=100**2)\n        # Slope prior\n        b = pm.Normal('beta', mu=0, sd=100**2)\n\n        # Model error prior\n        eps = pm.Uniform('eps', lower=0, upper=100)\n\n        # Linear model\n        radon_est = a + b * c_floor_measure\n\n        # Data likelihood\n        radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=c_log_radon)\n\n        # Inference button (TM)!\n        step = pm.NUTS()\n        trace = pm.sample(2000, step=step, progressbar=False)\n\n    # keep trace for later analysis\n    indiv_traces[county_name] = trace",
            "title": "Unpooled/non-hierarchical model"
        },
        {
            "location": "/GLM-hierarchical/#hierarchical-model",
            "text": "Instead of creating models separatley, the hierarchical model creates group parameters that consider the countys not as completely different but as having an underlying similarity. These distributions are subsequently used to influence the distribution of each county's  $\\alpha$  and  $\\beta$ .  with pm.Model() as hierarchical_model:\n    # Hyperpriors for group nodes\n    mu_a = pm.Normal('mu_alpha', mu=0., sd=100**2)\n    sigma_a = pm.Uniform('sigma_alpha', lower=0, upper=100)\n    mu_b = pm.Normal('mu_beta', mu=0., sd=100**2)\n    sigma_b = pm.Uniform('sigma_beta', lower=0, upper=100)\n\n    # Intercept for each county, distributed around group mean mu_a\n    # Above we just set mu and sd to a fixed value while here we\n    # plug in a common group distribution for all a and b (which are\n    # vectors of length n_counties).\n    a = pm.Normal('alpha', mu=mu_a, sd=sigma_a, shape=n_counties)\n    # Intercept for each county, distributed around group mean mu_a\n    b = pm.Normal('beta', mu=mu_b, sd=sigma_b, shape=n_counties)\n\n    # Model error\n    eps = pm.Uniform('eps', lower=0, upper=100)\n\n    # Model prediction of radon level\n    # a[county_idx] translates to a[0, 0, 0, 1, 1, ...],\n    # we thus link multiple household measures of a county\n    # to its coefficients.\n    radon_est = a[county_idx] + b[county_idx] * data.floor.values\n\n    # Data likelihood\n    radon_like = pm.Normal('radon_like', mu=radon_est, sd=eps, observed=data.log_radon)  # Inference button (TM)!\nwith hierarchical_model:\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    hierarchical_trace = pm.sample(2000, step, start=start, progressbar=False)  # Plotting the hierarchical model trace -its found values- from 500 iterations onwards (right side plot) \n# and its accumulated marginal values (left side plot) \npm.traceplot(hierarchical_trace[500:]);   The marginal posteriors in the left column are highly informative.  mu_a  tells us the group mean (log) radon levels.  mu_b  tells us that having no basement decreases radon levels significantly (no mass above zero). We can also see by looking at the marginals for  a  that there is quite some differences in radon levels between counties (each 'rainbow' color corresponds to a single county); the different widths are related to how much confidence we have in each paramter estimate -- the more measurements per county, the higher our confidence will be.",
            "title": "Hierarchical Model"
        },
        {
            "location": "/GLM-hierarchical/#posterior-predictive-check",
            "text": "",
            "title": "Posterior Predictive Check"
        },
        {
            "location": "/GLM-hierarchical/#the-root-mean-square-deviation",
            "text": "To find out which of the models explains the data better we can calculate the Root Mean Square Deviaton (RMSD). This posterior predictive check revolves around recreating the data based on the parameters found at different moments in the chain. The recreated or predicted values are subsequently compared to the real data points, the model that predicts data points closer to the original data is considered the better one. Thus, the lower the RMSD the better.  When computing the RMSD (code not shown) we get the following result:   individual/non-hierarchical model: 0.13  hierarchical model: 0.08   As can be seen above the hierarchical model performs better than the non-hierarchical model in predicting the radon values. Following this, we'll plot some examples of county's showing the actual radon measurements, the hierarchial predictions and the non-hierarchical predictions.   selection = ['CASS', 'CROW WING', 'FREEBORN']\nfig, axis = plt.subplots(1, 3, figsize=(12, 6), sharey=True, sharex=True)\naxis = axis.ravel()\nfor i, c in enumerate(selection):\n    c_data = data.ix[data.county == c]\n    c_data = c_data.reset_index(drop = True)\n    z = list(c_data['county_code'])[0]\n\n    xvals = np.linspace(-0.2, 1.2)\n    for a_val, b_val in zip(indiv_traces[c]['alpha'][500::10], indiv_traces[c]['beta'][500::10]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'b', alpha=.1)\n    axis[i].plot(xvals, indiv_traces[c]['alpha'][500::10].mean() + indiv_traces[c]['beta'][500::10].mean() * xvals, \n                 'b', alpha=1, lw=2., label='individual')\n    for a_val, b_val in zip(hierarchical_trace['alpha'][500::10][z], hierarchical_trace['beta'][500::10][z]):\n        axis[i].plot(xvals, a_val + b_val * xvals, 'g', alpha=.1)\n    axis[i].plot(xvals, hierarchical_trace['alpha'][500::10][z].mean() + hierarchical_trace['beta'][500::10][z].mean() * xvals, \n                 'g', alpha=1, lw=2., label='hierarchical')\n    axis[i].scatter(c_data.floor + np.random.randn(len(c_data))*0.01, c_data.log_radon, \n                    alpha=1, color='k', marker='.', s=80, label='original data')\n    axis[i].set_xticks([0,1])\n    axis[i].set_xticklabels(['basement', 'no basement'])\n    axis[i].set_ylim(-1, 4)\n    axis[i].set_title(c)\n    if not i%3:\n        axis[i].legend()\n        axis[i].set_ylabel('log radon level')   In the above plot we have the data points in black of three selected counties. The thick lines represent the mean estimate of the regression line of the individual (blue) and hierarchical model (in green). The thinner lines are regression lines of individual samples from the posterior and give us a sense of how variable the estimates are.  When looking at the county 'CASS' we see that the non-hierarchical estimation is strongly biased: as this county's data contains only households with a basement the estimated regression produces the non-sensical result of a giant negative slope meaning that we would expect negative radon levels in a house without basement!  Moreover, in the example county's 'CROW WING' and 'FREEBORN' the non-hierarchical model appears to react more strongly than the hierarchical model to the existance of outliers in the dataset ('CROW WING': no basement upper right. 'FREEBORN': basement upper left). Assuming that there should be a higher amount of radon gas measurable in households with basements opposed to those without, the county 'CROW WING''s non-hierachical model seems off. Having the group-distribution constrain the coefficients we get meaningful estimates in all cases as we apply what we learn from the group to the individuals and vice-versa.",
            "title": "The Root Mean Square Deviation"
        },
        {
            "location": "/GLM-hierarchical/#shrinkage",
            "text": "Shrinkage describes the process by which our estimates are \"pulled\" towards the group-mean as a result of the common group distribution -- county-coefficients very far away from the group mean have very low probability under the normality assumption, moving them closer to the group mean gives them higher probability. In the non-hierachical model every county is allowed to differ completely from the others by just using each county's data, resulting in a model more prone to outliers (as shown above).   hier_a = hierarchical_trace['alpha'][500:].mean(axis=0)\nhier_b = hierarchical_trace['beta'][500:].mean(axis=0)\nindv_a = [indiv_traces[c]['alpha'][500:].mean() for c in county_names]\nindv_b = [indiv_traces[c]['beta'][500:].mean() for c in county_names]  fig = plt.figure(figsize=(10, 10))\nax = fig.add_subplot(111, xlabel='Intercept', ylabel='Floor Measure', \n                     title='Hierarchical vs. Non-hierarchical Bayes', \n                     xlim=(0, 3), ylim=(-3, 3))\n\nax.scatter(indv_a,indv_b, s=26, alpha=0.4, label = 'non-hierarchical')\nax.scatter(hier_a,hier_b, c='red', s=26, alpha=0.4, label = 'hierarchical')\nfor i in range(len(indv_b)):  \n    ax.arrow(indv_a[i], indv_b[i], hier_a[i] - indv_a[i], hier_b[i] - indv_b[i], \n             fc= k , ec= k , length_includes_head=True, alpha=0.4, head_width=.04)\nax.legend();   In the shrinkage plot above we show the coefficients of each county's non-hierarchical posterior mean (blue) and the hierarchical posterior mean (red). To show the effect of shrinkage on a single coefficient-pair (alpha and beta) we connect the blue and red points belonging to the same county by an arrow. Some non-hierarchical posteriors are so far out that we couldn't display them in this plot (it makes the axes too wide). Interestingly, all hierarchical posteriors of the floor-measure seem to be around -0.6 indicating that having a basement in almost all county's is a clear indicator for heightened radon levels. The intercept (which we take for type of soil) appears to differ among countys. This information would have been difficult to find if we had only used the non-hierarchial model.  Critically, many effects that look quite large and significant in the non-hiearchical model actually turn out to be much smaller when we take the group distribution into account (this point can also well be seen in plot  In[12]  in  Chris' NB ). Shrinkage can thus be viewed as a form of smart regularization that helps reduce false-positives!",
            "title": "Shrinkage"
        },
        {
            "location": "/GLM-hierarchical/#connections-to-frequentist-statistics",
            "text": "This type of hierarchical, partial pooling model is known as a  random effects model  in frequentist terms. Interestingly, if we placed uniform priors on the group mean and variance in the above model, the resulting Bayesian model would be equivalent to a random effects model. One might imagine that the difference between a model with uniform or wide normal hyperpriors should not have a huge impact. However,  Gelman says  encourages use of weakly-informative priors (like we did above) over flat priors.",
            "title": "Connections to Frequentist statistics"
        },
        {
            "location": "/GLM-hierarchical/#summary",
            "text": "In this post, co-authored by Danne Elbers, we showed how a multi-level hierarchical Bayesian model gives the best of both worlds when we have multiple sets of measurements we expect to have similarity. The naive approach either pools all data together and ignores the individual differences, or treats each set as completely separate leading to noisy estimates, as shown above. By assuming that each individual data set (each county in our case) is distributed according to a group distribution -- which we simultaneously estimate -- we benefit from increased statistical power and smart regularization via the shrinkage effect. Probabilistic Programming in  PyMC  then makes Bayesian estimation of this model trivial.  As a follow-up we could also include other states into our model. For this we could add yet another layer to the hierarchy where each state is pooled at the country level. Finally, readers of my blog will notice that we didn't use  glm()  here as it does not play nice with hierarchical models yet.",
            "title": "Summary"
        },
        {
            "location": "/GLM-hierarchical/#references",
            "text": "The underlying Notebook of this blog post  Blog post:  The Inference Button: Bayesian GLMs made easy with PyMC3  Blog post:  This world is far from Normal(ly distributed): Bayesian Robust Regression in PyMC3     Chris Fonnesbeck repo containing a more extensive analysis  Blog post:  Shrinkage in multi-level hierarchical models  by John Kruschke  Gelman, A.; Carlin; Stern; and Rubin, D., 2007, \"Replication data for: Bayesian Data Analysis, Second Edition\",   Gelman, A.,   Hill, J. (2006).  Data Analysis Using Regression and Multilevel/Hierarchical Models (1st ed.). Cambridge University Press.  Gelman, A. (2006). Multilevel (Hierarchical) modeling: what it can and cannot do. Technometrics, 48(3), 432\u2013435.",
            "title": "References"
        },
        {
            "location": "/GLM-hierarchical/#acknowledgements",
            "text": "Thanks to  Imri Sofer  for feedback and teaching us about the connections to random-effects models and  Dan Dillon  for useful comments on an earlier draft.",
            "title": "Acknowledgements"
        },
        {
            "location": "/pmf-pymc/",
            "text": "Probabilistic Matrix Factorization for Making Personalized Recommendations\n\n\nThe model discussed in this analysis was developed by Ruslan Salakhutdinov and Andriy Mnih. All of the code and supporting text, when not referenced, is the original work of \nMack Sweeney\n.\n\n\nMotivation\n\n\nSay I download a handbook of a hundred jokes, and I'd like to know very quickly which ones will be my favorite. So maybe I read a few, I laugh, I read a few more, I stop laughing, and I indicate on a scale of -10 to 10 how funny I thought each joke was. Maybe I do this for 5 jokes out of the 100. Now I go to the back of the book, and there's a little program included for calculating my preferences for all the other jokes. I enter in my preference numbers and shazam! The program spits out a list of all 100 jokes, sorted in the order I'll like them. That certainly would be nice. Today we'll write a program that does exactly this.\n\n\nWe'll start out by getting some intuition for how our model will work. Then we'll formalize our intuition. Afterwards, we'll examine the dataset we are going to use. Once we have some notion of what our data looks like, we'll define some baseline methods for predicting preferences for jokes. Following that, we'll look at Probabilistic Matrix Factorization (PMF), which is a more sophisticated Bayesian method for predicting preferences. Having detailed the PMF model, we'll use PyMC3 for MAP estimation and MCMC inference. Finally, we'll compare the results obtained with PMF to those obtained from our baseline methods and discuss the outcome.\n\n\nIntuition\n\n\nNormally if we want recommendations for something, we try to find people who are similar to us and ask their opinions. If Bob, Alice, and Monty are all similar to me, and they all like knock-knock jokes, I'll probably like knock-knock jokes. Now this isn't always true. It depends on what we consider to be \"similar\". In order to get the best bang for our buck, we really want to look for people who have the most similar sense of humor. Humor being a complex beast, we'd probably like to break it down into something more understandable. We might try to characterize each joke in terms of various factors. Perhaps jokes can be dry, sarcastic, crude, sexual, political, etc. Now imagine we go through our handbook of jokes and assign each joke a rating in each of the categories. How dry is it? How sarcastic is it? How much does it use sexual innuendos? Perhaps we use numbers between 0 and 1 for each category. Intuitively, we might call this the joke's humor profile.\n\n\nNow let's suppose we go back to those 5 jokes we rated. At this point, we can get a richer picture of our own preferences by looking at the humor profiles of each of the jokes we liked and didn't like. Perhaps we take the averages across the 5 humor profiles and call this our ideal type of joke. In other words, we have computed some notion of our inherent \npreferences\n for various types of jokes. Suppose Bob, Alice, and Monty all do the same. Now we can compare our preferences and determine how similar each of us really are. I might find that Bob is the most similar and the other two are still more similar than other people, but not as much as Bob. So I want recommendations from all three people, but when I make my final decision, I'm going to put more weight on Bob's recommendation than those I get from Alice and Monty.\n\n\nWhile the above procedure sounds fairly effective as is, it also reveals an unexpected additional source of information. If we rated a particular joke highly, and we know its humor profile, we can compare with the profiles of other jokes. If we find one with very close numbers, it is probable we'll also enjoy this joke. Both this approach and the one above are commonly known as \nneighborhood approaches\n. Techniques that leverage both of these approaches simultaneously are often called \ncollaborative filtering\n \n[1]\n. The first approach we talked about uses user-user similarity, while the second uses item-item similarity. Ideally, we'd like to use both sources of information. The idea is we have a lot of items available to us, and we'd like to work together with others to filter the list of items down to those we'll each like best. My list should have the items I'll like best at the top and those I'll like least at the bottom. Everyone else wants the same. If I get together with a bunch of other people, we all read 5 jokes, and we have some efficient computational process to determine similarity, we can very quickly order the jokes to our liking.\n\n\nFormalization\n\n\nLet's take some time to make the intuitive notions we've been discussing more concrete. We have a set of \n$M$\n jokes, or \nitems\n (\n$M = 100$\n in our example above). We also have \n$N$\n people, whom we'll call \nusers\n of our recommender system. For each item, we'd like to find a \n$D$\n dimensional factor composition (humor profile above) to describe the item. Ideally, we'd like to do this without actually going through and manually labeling all of the jokes. Manual labeling would be both slow and error-prone, as different people will likely label jokes differently. So we model each joke as a \n$D$\n dimensional vector, which is its latent factor composition. Furthermore, we expect each user to have some preferences, but without our manual labeling and averaging procedure, we have to rely on the latent factor compositions to learn \n$D$\n dimensional latent preference vectors for each user. The only thing we get to observe is the \n$N \\times M$\n ratings matrix \n$R$\n provided by the users. Entry \n$R_{ij}$\n is the rating user \n$i$\n gave to item \n$j$\n. Many of these entries may be missing, since most users will not have rated all 100 jokes. Our goal is to fill in the missing values with predicted ratings based on the latent variables \n$U$\n and \n$V$\n. We denote the predicted ratings by \n$R_{ij}^*$\n. We also define an indicator matrix \n$I$\n, with entry \n$I_{ij} = 0$\n if \n$R_{ij}$\n is missing and \n$I_{ij} = 1$\n otherwise.\n\n\nSo we have an \n$N \\times D$\n matrix of user preferences which we'll call \n$U$\n and an \n$M \\times D$\n factor composition matrix we'll call \n$V$\n. We also have a \n$N \\times M$\n rating matrix we'll call \n$R$\n. We can think of each row \n$U_i$\n as indications of how much each user prefers each of the \n$D$\n latent factors. Each row \n$V_j$\n can be thought of as how much each item can be described by each of the latent factors. In order to make a recommendation, we need a suitable prediction function which maps a user preference vector \n$U_i$\n and an item latent factor vector \n$V_j$\n to a predicted ranking. The choice of this prediction function is an important modeling decision, and a variety of prediction functions have been used. Perhaps the most common is the dot product of the two vectors, \n$U_i \\cdot V_j$\n \n[1]\n.\n\n\nTo better understand CF techniques, let us explore a particular example. Imagine we are seeking to recommend jokes using a model which infers five latent factors, \n$V_j$\n, for \n$j = 1,2,3,4,5$\n. In reality, the latent factors are often unexplainable in a straightforward manner, and most models make no attempt to understand what information is being captured by each factor.  However, for the purposes of explanation, let us assume the five latent factors might end up capturing the humor profile we were discussing above. So our five latent factors are: dry, sarcastic, crude, sexual, and political. Then for a particular user \n$i$\n, imagine we infer a preference vector \n$U_i = \n0.2, 0.1, 0.3, 0.1, 0.3\n$\n. Also, for a particular item \n$j$\n, we infer these values for the latent factors: \n$V_j = \n0.5, 0.5, 0.25, 0.8, 0.9\n$\n. Using the dot product as the prediction function, we would calculate 0.575 as the ranking for that item, which is more or less a neutral preference given our -10 to 10 rating scale.\n\n\n$$0.2 \\times 0.5 + 0.1 \\times 0.5 + 0.3 \\times 0.25 + 0.1 \\times 0.8 + 0.3\n\\times 0.9 = 0.575$$\n\n\nData\n\n\nThe \nv1 Jester dataset\n provides something very much like the handbook of jokes we have been discussing. The original version of this dataset was constructed in conjunction with the development of the \nEigentaste recommender system\n \n[2]\n. At this point in time, v1 contains over 4.1 million continuous ratings in the range [-10, 10] of 100 jokes from 73,421 users. These ratings were collected between Apr. 1999 and May 2003. In order to reduce the training time of the model for illustrative purposes, 1,000 users who have rated all 100 jokes will be selected randomly. We will implement a model that is suitable for collaborative filtering on this data and evaluate it in terms of root mean squared error (RMSE) to validate the results.\n\n\nLet's begin by exploring our data. We want to get a general feel for what it looks like and a sense for what sort of patterns it might contain.\n\n\n% matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv('data/jester-dataset-v1-dense-first-1000.csv')\ndata.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \n1\n\n      \n2\n\n      \n3\n\n      \n4\n\n      \n5\n\n      \n6\n\n      \n7\n\n      \n8\n\n      \n9\n\n      \n10\n\n      \n...\n\n      \n91\n\n      \n92\n\n      \n93\n\n      \n94\n\n      \n95\n\n      \n96\n\n      \n97\n\n      \n98\n\n      \n99\n\n      \n100\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n 4.08\n\n      \n-0.29\n\n      \n 6.36\n\n      \n 4.37\n\n      \n-2.38\n\n      \n-9.66\n\n      \n-0.73\n\n      \n-5.34\n\n      \n 8.88\n\n      \n 9.22\n\n      \n...\n\n      \n 2.82\n\n      \n-4.95\n\n      \n-0.29\n\n      \n 7.86\n\n      \n-0.19\n\n      \n-2.14\n\n      \n 3.06\n\n      \n 0.34\n\n      \n-4.32\n\n      \n 1.07\n\n    \n\n    \n\n      \n1\n\n      \n-6.17\n\n      \n-3.54\n\n      \n 0.44\n\n      \n-8.50\n\n      \n-7.09\n\n      \n-4.32\n\n      \n-8.69\n\n      \n-0.87\n\n      \n-6.65\n\n      \n-1.80\n\n      \n...\n\n      \n-3.54\n\n      \n-6.89\n\n      \n-0.68\n\n      \n-2.96\n\n      \n-2.18\n\n      \n-3.35\n\n      \n 0.05\n\n      \n-9.08\n\n      \n-5.05\n\n      \n-3.45\n\n    \n\n    \n\n      \n2\n\n      \n 6.84\n\n      \n 3.16\n\n      \n 9.17\n\n      \n-6.21\n\n      \n-8.16\n\n      \n-1.70\n\n      \n 9.27\n\n      \n 1.41\n\n      \n-5.19\n\n      \n-4.42\n\n      \n...\n\n      \n 7.23\n\n      \n-1.12\n\n      \n-0.10\n\n      \n-5.68\n\n      \n-3.16\n\n      \n-3.35\n\n      \n 2.14\n\n      \n-0.05\n\n      \n 1.31\n\n      \n 0.00\n\n    \n\n    \n\n      \n3\n\n      \n-3.79\n\n      \n-3.54\n\n      \n-9.42\n\n      \n-6.89\n\n      \n-8.74\n\n      \n-0.29\n\n      \n-5.29\n\n      \n-8.93\n\n      \n-7.86\n\n      \n-1.60\n\n      \n...\n\n      \n 4.37\n\n      \n-0.29\n\n      \n 4.17\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-0.29\n\n      \n-3.40\n\n      \n-4.95\n\n    \n\n    \n\n      \n4\n\n      \n 1.31\n\n      \n 1.80\n\n      \n 2.57\n\n      \n-2.38\n\n      \n 0.73\n\n      \n 0.73\n\n      \n-0.97\n\n      \n 5.00\n\n      \n-7.23\n\n      \n-1.36\n\n      \n...\n\n      \n 1.46\n\n      \n 1.70\n\n      \n 0.29\n\n      \n-3.30\n\n      \n 3.45\n\n      \n 5.44\n\n      \n 4.08\n\n      \n 2.48\n\n      \n 4.51\n\n      \n 4.66\n\n    \n\n  \n\n\n\n\n5 rows \u00d7 100 columns\n\n\n\n\n\n# Extract the ratings from the DataFrame\nall_ratings = np.ndarray.flatten(data.values)\nratings = pd.Series(all_ratings)\n\n# Plot histogram and density.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\nratings.plot(kind='density', ax=ax1, grid=False)\nax1.set_ylim(0, 0.08)\nax1.set_xlim(-11, 11)\n\n# Plot histogram\nratings.plot(kind='hist', ax=ax2, bins=20, grid=False)\nax2.set_xlim(-11, 11)\nplt.show()\n\n\n\n\n\n\nratings.describe()\n\n\n\n\ncount    100000.000000\nmean          0.996219\nstd           5.265215\nmin          -9.950000\n25%          -2.860000\n50%           1.650000\n75%           5.290000\nmax           9.420000\ndtype: float64\n\n\n\nThis must be a decent batch of jokes. From our exploration above, we know most ratings are in the range -1 to 10, and positive ratings are more likely than negative ratings. Let's look at the means for each joke to see if we have any particularly good (or bad) humor here.\n\n\njoke_means = data.mean(axis=0)\njoke_means.plot(kind='bar', grid=False, figsize=(16, 6),\n                title=\nMean Ratings for All 100 Jokes\n)\n\n\n\n\nmatplotlib.axes._subplots.AxesSubplot at 0x7fc46a3c5fd0\n\n\n\n\n\n\nWhile the majority of the jokes generally get positive feedback from users, there are definitely a few that stand out as poor humor. Let's take a look at the worst and best joke, just for fun.\n\n\nimport os\n\n# Worst and best joke?\nworst_joke_id = joke_means.argmin()\nbest_joke_id = joke_means.argmax()\n\n# Let's see for ourselves. Load the jokes.\njoke_dir = 'data/jokes'\nfiles = [os.path.join(joke_dir, fname) for fname in os.listdir(joke_dir)]\njokes = [fname for fname in files if fname.endswith('txt')]\nnums = [filter(lambda c: c.isdigit(), fname) for fname in jokes]\njoke_dict = {k: v for k, v in zip(nums, jokes)}\n\ndef read_joke(joke_id):\n    fname = joke_dict[joke_id]\n    with open(fname) as f:\n        return f.read()\n\nprint 'The worst joke:\\n---------------\\n%s\\n' % read_joke(worst_joke_id)\nprint 'The best joke:\\n--------------\\n%s' % read_joke(best_joke_id)\n\n\n\n\nThe worst joke:\n---------------\nA Joke \nHow many teddybears does it take to change a lightbulb?\nIt takes only one teddybear, but it takes a whole lot of lightbulbs.\n\nThe best joke:\n--------------\nA Joke \nA radio conversation of a US naval ship with Canadian authorities ...\n\nAmericans: Please divert your course 15 degrees to the North to avoid a collision.\nCanadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.\nAmericans: This is the Captain of a US Navy ship.  I say again, divert YOUR course.\nCanadians: No. I say again, you divert YOUR course.\nAmericans: This is the aircraft carrier USS LINCOLN, the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers, three cruisers and numerous support vessels. I demand that you change your course 15 degrees north, that's ONE FIVE DEGREES NORTH, or counter-measures will be undertaken to ensure the safety of this ship.\nCanadians: This is a lighthouse. Your call.\n\n\n\nMake sense to me. We now know there are definite popularity differences between the jokes. Some of them are simply funnier than others, and some are downright lousy. Looking at the joke means allowed us to discover these general trends. Perhaps there are similar trends across users. It might be the case that some users are simply more easily humored than others. Let's take a look.\n\n\nuser_means = data.mean(axis=1)\nfig, ax = plt.subplots(figsize=(16, 6))\nuser_means.plot(kind='bar', grid=False, ax=ax,\n                title=\nMean Ratings for All 1000 Users\n)\nax.set_xticklabels('')  # 1000 labels is nonsensical\nfig.show()\n\n\n\n\n\n\nWe see even more significant trends here. Some users rate nearly everything highly, and some (though not as many) rate nearly everything negatively. These observations will come in handy when considering models to use for predicting user preferences on unseen jokes.\n\n\nMethods\n\n\nHaving explored the data, we're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read.\n\n\nBaselines\n\n\nEvery good analysis needs some kind of baseline methods to compare against. It's difficult to claim we've produced good results if we have no reference point for what defines \"good\". We'll define three very simple baseline methods and find the RMSE using these methods. Our goal will be to obtain lower RMSE scores with whatever model we produce.\n\n\nUniform Random Baseline\n\n\nOur first baseline is about as dead stupid as you can get. Every place we see a missing value in \n$R$\n, we'll simply fill it with a number drawn uniformly at random in the range [-10, 10]. We expect this method to do the worst by far.\n\n\n$$R_{ij}^* \\sim Uniform$$\n\n\nGlobal Mean Baseline\n\n\nThis method is only slightly better than the last. Wherever we have a missing value, we'll fill it in with the mean of all observed ratings.\n\n\n$$\\text{global_mean} = \\frac{1}{N \\times M} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij}(R_{ij})$$\n\n\n$$R_{ij}^* = \\text{global_mean}$$\n\n\nMean of Means Baseline\n\n\nNow we're going to start getting a bit smarter. We imagine some users might be easily amused, and inclined to rate all jokes more highly. Other users might be the opposite. Additionally, some jokes might simply be more witty than others, so all users might rate some jokes more highly than others in general. We can clearly see this in our graph of the joke means above. We'll attempt to capture these general trends through per-user and per-joke rating means. We'll also incorporate the global mean to smooth things out a bit. So if we see a missing value in cell \n$R_{ij}$\n, we'll average the global mean with the mean of \n$U_i$\n and the mean of \n$V_j$\n and use that value to fill it in.\n\n\n$$\\text{user_means} = \\frac{1}{M} \\sum_{j=1}^M I_{ij}(R_{ij})$$\n\n\n$$\\text{joke_means} = \\frac{1}{N} \\sum_{i=1}^N I_{ij}(R_{ij})$$\n\n\n$$R_{ij}^* = \\frac{1}{3} \\left(\\text{user_means}_i + \\text{ joke_means}_j + \\text{ global_mean} \\right)$$\n\n\nfrom collections import OrderedDict\n\n\n# Create a base class with scaffolding for our 3 baselines.\n\ndef split_title(title):\n    \nChange \nBaselineMethod\n to \nBaseline Method\n.\n\n    words = []\n    tmp = [title[0]]\n    for c in title[1:]:\n        if c.isupper():\n            words.append(''.join(tmp))\n            tmp = [c]\n        else:\n            tmp.append(c)\n    words.append(''.join(tmp))\n    return ' '.join(words)\n\n\nclass Baseline(object):\n    \nCalculate baseline predictions.\n\n\n    def __init__(self, train_data):\n        \nSimple heuristic-based transductive learning to fill in missing\n        values in data matrix.\n\n        self.predict(train_data.copy())\n\n    def predict(self, train_data):\n        raise NotImplementedError(\n            'baseline prediction not implemented for base class')\n\n    def rmse(self, test_data):\n        \nCalculate root mean squared error for predictions on test data.\n\n        return rmse(test_data, self.predicted)\n\n    def __str__(self):\n        return split_title(self.__class__.__name__)\n\n\n\n# Implement the 3 baselines.\n\nclass UniformRandomBaseline(Baseline):\n    \nFill missing values with uniform random values.\n\n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        pmin, pmax = masked_train.min(), masked_train.max()\n        N = nan_mask.sum()\n        train_data[nan_mask] = np.random.uniform(pmin, pmax, N)\n        self.predicted = train_data\n\n\nclass GlobalMeanBaseline(Baseline):\n    \nFill in missing values using the global mean.\n\n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        train_data[nan_mask] = train_data[~nan_mask].mean()\n        self.predicted = train_data\n\n\nclass MeanOfMeansBaseline(Baseline):\n    \nFill in missing values using mean of user/item/global means.\n\n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        global_mean = masked_train.mean()\n        user_means = masked_train.mean(axis=1)\n        item_means = masked_train.mean(axis=0)\n        self.predicted = train_data.copy()\n        n, m = train_data.shape\n        for i in xrange(n):\n            for j in xrange(m):\n                if np.ma.isMA(item_means[j]):\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i]))\n                else:\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i], item_means[j]))\n\n\nbaseline_methods = OrderedDict()\nbaseline_methods['ur'] = UniformRandomBaseline\nbaseline_methods['gm'] = GlobalMeanBaseline\nbaseline_methods['mom'] = MeanOfMeansBaseline\n\n\n\n\nProbabilistic Matrix Factorization\n\n\nProbabilistic Matrix Factorization (PMF)\n [3] is a probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective. The ratings \n$R$\n are modeled as draws from a Gaussian distribution.  The mean for \n$R_{ij}$\n is \n$U_i V_j^T$\n. The precision \n$\\alpha$\n is a fixed parameter that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance. Complexity is controlled by placing zero-mean spherical Gaussian priors on \n$U$\n and \n$V$\n. In other words, each row of \n$U$\n is drawn from a multivariate Gaussian with mean \n$\\mu = 0$\n and precision which is some multiple of the identity matrix \n$I$\n. Those multiples are \n$\\alpha_U$\n for \n$U$\n and \n$\\alpha_V$\n for \n$V$\n. So our model is defined by:\n\n\n$\\newcommand\\given[1][]{\\:#1\\vert\\:}$\n\n\n\\begin{equation}\nP(R \\given U, V, \\alpha^2) = \n    \\prod_{i=1}^N \\prod_{j=1}^M\n        \\left[ \\mathcal{N}(R_{ij} \\given U_i V_j^T, \\alpha^{-1}) \\right]^{I_{ij}}\n\\end{equation}\n\n\n\\begin{equation}\nP(U \\given \\alpha_U^2) =\n    \\prod_{i=1}^N \\mathcal{N}(U_i \\given 0, \\alpha_U^{-1} \\boldsymbol{I})\n\\end{equation}\n\n\n\\begin{equation}\nP(V \\given \\alpha_U^2) =\n    \\prod_{j=1}^M \\mathcal{N}(V_j \\given 0, \\alpha_V^{-1} \\boldsymbol{I})\n\\end{equation}\n\n\nGiven small precision parameters, the priors on \n$U$\n and \n$V$\n ensure our latent variables do not grow too far from 0. This prevents overly strong user preferences and item factor compositions from being learned. This is commonly known as complexity control, where the complexity of the model here is measured by the magnitude of the latent variables. Controlling complexity like this helps prevent overfitting, which allows the model to generalize better for unseen data. We must also choose an appropriate \n$\\alpha$\n value for the normal distribution for \n$R$\n. So the challenge becomes choosing appropriate values for \n$\\alpha_U$\n, \n$\\alpha_V$\n, and \n$\\alpha$\n. This challenge can be tackled with the soft weight-sharing methods discussed by \nNowland and Hinton, 1992\n [4]. However, for the purposes of this analysis, we will stick to using point estimates obtained from our data.\n\n\nimport time\nimport logging\nimport pymc3 as pm\nimport theano\nimport scipy as sp\n\n\n# Enable on-the-fly graph computations, but ignore \n# absence of intermediate test values.\ntheano.config.compute_test_value = 'ignore'\n\n# Set up logging.\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n\nclass PMF(object):\n    \nProbabilistic Matrix Factorization model using pymc3.\n\n\n    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(-10, 10)):\n        \nBuild the Probabilistic Matrix Factorization model using pymc3.\n\n        :param np.ndarray train: The training data to use for learning the model.\n        :param int dim: Dimensionality of the model; number of latent factors.\n        :param int alpha: Fixed precision for the likelihood function.\n        :param float std: Amount of noise to use for model initialization.\n        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n            These bounds will simply be used to cap the estimates produced for R.\n\n        \n\n        self.dim = dim\n        self.alpha = alpha\n        self.std = np.sqrt(1.0 / alpha)\n        self.bounds = bounds\n        self.data = train.copy()\n        n, m = self.data.shape\n\n        # Perform mean value imputation\n        nan_mask = np.isnan(self.data)\n        self.data[nan_mask] = self.data[~nan_mask].mean()\n\n        # Low precision reflects uncertainty; prevents overfitting.\n        # Set to the mean variance across users and items.\n        self.alpha_u = 1 / self.data.var(axis=1).mean()\n        self.alpha_v = 1 / self.data.var(axis=0).mean()\n\n        # Specify the model.\n        logging.info('building the PMF model')\n        with pm.Model() as pmf:\n            U = pm.MvNormal(\n                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n            V = pm.MvNormal(\n                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n            R = pm.Normal(\n                'R', mu=theano.tensor.dot(U, V.T), tau=self.alpha * np.ones((n, m)),\n                observed=self.data)\n\n        logging.info('done building the PMF model') \n        self.model = pmf\n\n    def __str__(self):\n        return self.name\n\n\n\n\n\nWe'll also need functions for calculating the MAP and performing sampling on our PMF model. When the observation noise variance \n$\\alpha$\n and the prior variances \n$\\alpha_U$\n and \n$\\alpha_V$\n are all kept fixed, maximizing the log posterior is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms.\n\n\n$$\nE = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - U_i V_j^T)^2 +\n    \\frac{\\lambda_U}{2} \\sum_{i=1}^N \\|U\\|_{Fro}^2 +\n    \\frac{\\lambda_V}{2} \\sum_{j=1}^M \\|V\\|_{Fro}^2,\n$$\n\n\nwhere \n$\\lambda_U = \\alpha_U / \\alpha$\n, \n$\\lambda_V = \\alpha_V / \\alpha$\n, and \n$\\|\\cdot\\|_{Fro}^2$\n denotes the Frobenius norm [3]. Minimizing this objective function gives a local minimum, which is essentially a maximum a posteriori (MAP) estimate. While it is possible to use a fast Stochastic Gradient Descent procedure to find this MAP, we'll be finding it using the utilities built into \npymc3\n. In particular, we'll use \nfind_MAP\n with Powell optimization (\nscipy.optimize.fmin_powell\n). Having found this MAP estimate, we can use it as our starting point for MCMC sampling.\n\n\nSince it is a reasonably complex model, we expect the MAP estimation to take some time. So let's save it after we've found it. Note that we define a function for finding the MAP below, assuming it will receive a namespace with some variables in it. Then we attach that function to the PMF class, where it will have such a namespace after initialization. The PMF class is defined in pieces this way so I can say a few things between each piece to make it clearer.\n\n\ntry:\n    import ujson as json\nexcept ImportError:\n    import json\n\n\n# First define functions to save our MAP estimate after it is found.\n# We adapt these from `pymc3`'s `backends` module, where the original\n# code is used to save the traces from MCMC samples.\ndef save_np_vars(vars, savedir):\n    \nSave a dictionary of numpy variables to `savedir`. We assume\n    the directory does not exist; an OSError will be raised if it does.\n    \n\n    logging.info('writing numpy vars to directory: %s' % savedir)\n    os.mkdir(savedir)\n    shapes = {}\n    for varname in vars:\n        data = vars[varname]\n        var_file = os.path.join(savedir, varname + '.txt')\n        np.savetxt(var_file, data.reshape(-1, data.size))\n        shapes[varname] = data.shape\n\n        ## Store shape information for reloading.\n        shape_file = os.path.join(savedir, 'shapes.json')\n        with open(shape_file, 'w') as sfh:\n            json.dump(shapes, sfh)\n\n\ndef load_np_vars(savedir):\n    \nLoad numpy variables saved with `save_np_vars`.\n\n    shape_file = os.path.join(savedir, 'shapes.json')\n    with open(shape_file, 'r') as sfh:\n        shapes = json.load(sfh)\n\n    vars = {}\n    for varname, shape in shapes.items():\n        var_file = os.path.join(savedir, varname + '.txt')\n        vars[varname] = np.loadtxt(var_file).reshape(shape)\n\n    return vars\n\n\n# Now define the MAP estimation infrastructure.\ndef _map_dir(self):\n    basename = 'pmf-map-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _find_map(self):\n    \nFind mode of posterior using Powell optimization.\n\n    tstart = time.time()\n    with self.model:\n        logging.info('finding PMF MAP using Powell optimization...')\n        self._map = pm.find_MAP(fmin=sp.optimize.fmin_powell, disp=True)\n\n    elapsed = int(time.time() - tstart)\n    logging.info('found PMF MAP in %d seconds' % elapsed)\n\n    # This is going to take a good deal of time to find, so let's save it.\n    save_np_vars(self._map, self.map_dir)\n\ndef _load_map(self):\n    self._map = load_np_vars(self.map_dir)\n\ndef _map(self):\n    try:\n        return self._map\n    except:\n        if os.path.isdir(self.map_dir):\n            self.load_map()\n        else:\n            self.find_map()\n        return self._map\n\n\n# Update our class with the new MAP infrastructure.\nPMF.find_map = _find_map\nPMF.load_map = _load_map\nPMF.map_dir = property(_map_dir)\nPMF.map = property(_map)\n\n\n\n\nSo now our PMF class has a \nmap\n \nproperty\n which will either be found using Powell optimization or loaded from a previous optimization. Once we have the MAP, we can use it as a starting point for our MCMC sampler. We'll need a sampling function in order to draw MCMC samples to approximate the posterior distribution of the PMF model.\n\n\n# Draw MCMC samples.\ndef _trace_dir(self):\n    basename = 'pmf-mcmc-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _draw_samples(self, nsamples=1000, njobs=2):\n    # First make sure the trace_dir does not already exist.\n    if os.path.isdir(self.trace_dir):\n        raise OSError(\n            'trace directory %s already exists. Please move or delete.' % self.trace_dir)\n    start = self.map  # use our MAP as the starting point\n    with self.model:\n        logging.info('drawing %d samples using %d jobs' % (nsamples, njobs))\n        step = pm.NUTS(scaling=start)\n        backend = pm.backends.Text(self.trace_dir)\n        logging.info('backing up trace to directory: %s' % self.trace_dir)\n        self.trace = pm.sample(nsamples, step, start=start, njobs=njobs, trace=backend)\n\ndef _load_trace(self):\n    with self.model:\n        self.trace = pm.backends.text.load(self.trace_dir)\n\n\n# Update our class with the sampling infrastructure.\nPMF.trace_dir = property(_trace_dir)\nPMF.draw_samples = _draw_samples\nPMF.load_trace = _load_trace\n\n\n\n\nWe could define some kind of default trace property like we did for the MAP, but that would mean using possibly nonsensical values for \nnsamples\n and \nnjobs\n. Better to leave it as a non-optional call to \ndraw_samples\n. Finally, we'll need a function to make predictions using our inferred values for \n$U$\n and \n$V$\n. For user \n$i$\n and joke \n$j$\n, a prediction is generated by drawing from \n$\\mathcal{N}(U_i V_j^T, \\alpha)$\n. To generate predictions from the sampler, we generate an \n$R$\n matrix for each \n$U$\n and \n$V$\n sampled, then we combine these by averaging over the \n$K$\n samples.\n\n\n\\begin{equation}\nP(R_{ij}^* \\given R, \\alpha, \\alpha_U, \\alpha_V) \\approx\n    \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(U_i V_j^T, \\alpha)\n\\end{equation}\n\n\nWe'll want to inspect the individual \n$R$\n matrices before averaging them for diagnostic purposes. So we'll write code for the averaging piece during evaluation. The function below simply draws an \n$R$\n matrix given a \n$U$\n and \n$V$\n and the fixed \n$\\alpha$\n stored in the PMF object.\n\n\ndef _predict(self, U, V):\n    \nEstimate R from the given values of U and V.\n\n    R = np.dot(U, V.T)\n    n, m = R.shape\n    sample_R = np.array([\n        [np.random.normal(R[i,j], self.std) for j in xrange(m)]\n        for i in xrange(n)\n    ])\n\n    # bound ratings\n    low, high = self.bounds\n    sample_R[sample_R \n low] = low\n    sample_R[sample_R \n high] = high\n    return sample_R\n\n\nPMF.predict = _predict\n\n\n\n\nOne final thing to note: the dot products in this model are often constrained using a logistic function \n$g(x) = 1/(1 + exp(-x))$\n, that bounds the predictions to the range [0, 1]. To facilitate this bounding, the ratings are also mapped to the range [0, 1] using \n$t(x) = (x + min) / range$\n. The authors of PMF also introduced a constrained version which performs better on users with less ratings [3]. Both models are generally improvements upon the basic model presented here. However, in the interest of time and space, these will not be implemented here.\n\n\nEvaluation\n\n\nMetrics\n\n\nIn order to understand how effective our models are, we'll need to be able to evaluate them. We'll be evaluating in terms of root mean squared error (RMSE), which looks like this:\n\n\n\\begin{equation}\nRMSE = \\sqrt{ \\frac{ \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - R_{ij}^*)^2 }\n                   { \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} } }\n\\end{equation}\n\n\nIn this case, the RMSE can be thought of as the standard deviation of our predictions from the actual user preferences.\n\n\n# Define our evaluation function.\ndef rmse(test_data, predicted):\n    \nCalculate root mean squared error.\n    Ignoring missing values in the test data.\n    \n\n    I = ~np.isnan(test_data)   # indicator for missing values\n    N = I.sum()                # number of non-missing values\n    sqerror = abs(test_data - predicted) ** 2  # squared error array\n    mse = sqerror[I].sum() / N                 # mean squared error\n    return np.sqrt(mse)                        # RMSE\n\n\n\n\nTraining Data vs. Test Data\n\n\nThe next thing we need to do is split our data into a training set and a test set. Matrix factorization techniques use \ntransductive learning\n rather than inductive learning. So we produce a test set by taking a random sample of the cells in the full \n$N \\times M$\n data matrix. The values selected as test samples are replaced with \nnan\n values in a copy of the original data matrix to produce the training set. Since we'll be producing random splits, let's also write out the train/test sets generated. This will allow us to replicate our results. We'd like to be able to idenfity which split is which, so we'll take a hash of the indices selected for testing and use that to save the data.\n\n\nimport hashlib\n\n\n# Define a function for splitting train/test data.\ndef split_train_test(data, percent_test=10):\n    \nSplit the data into train/test sets.\n    :param int percent_test: Percentage of data to use for testing. Default 10.\n    \n\n    n, m = data.shape             # # users, # jokes\n    N = n * m                     # # cells in matrix\n    test_size = N / percent_test  # use 10% of data as test set\n    train_size = N - test_size    # and remainder for training\n\n    # Prepare train/test ndarrays.\n    train = data.copy().values\n    test = np.ones(data.shape) * np.nan\n\n    # Draw random sample of training data to use for testing.\n    tosample = np.where(~np.isnan(train))       # ignore nan values in data\n    idx_pairs = zip(tosample[0], tosample[1])   # tuples of row/col index pairs\n    indices = np.arange(len(idx_pairs))         # indices of index pairs\n    sample = np.random.choice(indices, replace=False, size=test_size)\n\n    # Transfer random sample from train set to test set.\n    for idx in sample:\n        idx_pair = idx_pairs[idx]\n        test[idx_pair] = train[idx_pair]  # transfer to test set\n        train[idx_pair] = np.nan          # remove from train set\n\n    # Verify everything worked properly\n    assert(np.isnan(train).sum() == test_size)\n    assert(np.isnan(test).sum() == train_size)\n\n    # Finally, hash the indices and save the train/test sets.\n    index_string = ''.join(map(str, np.sort(sample)))\n    name = hashlib.sha1(index_string).hexdigest()\n    savedir = os.path.join('data', name)\n    save_np_vars({'train': train, 'test': test}, savedir)\n\n    # Return train set, test set, and unique hash of indices.\n    return train, test, name\n\n\ndef load_train_test(name):\n    \nLoad the train/test sets.\n\n    savedir = os.path.join('data', name)\n    vars = load_np_vars(savedir)\n    return vars['train'], vars['test']\n\n# train, test, name = split_train_test(data)\n\n\n\n\nIn order to facilitate reproducibility, I've produced a train/test split using the code above which we'll now use for all the evaluations below.\n\n\ntrain, test = load_train_test('6bb8d06c69c0666e6da14c094d4320d115f1ffc8')\n\n\n\n\nResults\n\n\n# Let's see the results:\nbaselines = {}\nfor name in baseline_methods:\n    Method = baseline_methods[name]\n    method = Method(train)\n    baselines[name] = method.rmse(test)\n    print '%s RMSE:\\t%.5f' % (method, baselines[name])\n\n\n\n\n\nUniform Random Baseline RMSE:   7.77062\nGlobal Mean Baseline RMSE:  5.25004\nMean Of Means Baseline RMSE:    4.79832\n\n\n\nAs expected: the uniform random baseline is the worst by far, the global mean baseline is next best, and the mean of means method is our best baseline. Now let's see how PMF stacks up.\n\n\n# We use a fixed precision for the likelihood.\n# This reflects uncertainty in the dot product.\n# We choose 2 in the footsteps Salakhutdinov\n# Mnihof.\nALPHA = 2\n\n# The dimensionality D; the number of latent factors.\n# We can adjust this higher to try to capture more subtle\n# characteristics of each joke. However, the higher it is,\n# the more expensive our inference procedures will be.\n# Specifically, we have D(N + M) latent variables. For our\n# Jester dataset, this means we have D(1100), so for 5\n# dimensions, we are sampling 5500 latent variables.\nDIM = 5\n\n\npmf = PMF(train, DIM, ALPHA, std=0.05)\n\n\n\n\nINFO:root:building the PMF model\nINFO:root:done building the PMF model\n\n\n\nPredictions Using MAP\n\n\n# Find MAP for PMF.\npmf.find_map()\n# pmf.load_map()\n\n\n\n\nINFO:root:finding MAP using Powell optimization...\nINFO:root:found MAP in 2575 seconds\n\n\nOptimization terminated successfully.\n         Current function value: 1553644.881552\n         Iterations: 33\n         Function evaluations: 1644948\n\n\n\nExcellent. The first thing we want to do is make sure the MAP estimate we obtained is reasonable. We can do this by computing RMSE on the predicted ratings obtained from the MAP values of \n$U$\n and \n$V$\n. First we define a function for generating the predicted ratings \n$R$\n from \n$U$\n and \n$V$\n. We ensure the actual rating bounds are enforced by setting all values below -10 to -10 and all values above 10 to 10. Finally, we compute RMSE for both the training set and the test set. We expect the test RMSE to be higher. The difference between the two gives some idea of how much we have overfit. Some difference is always expected, but a very low RMSE on the training set with a high RMSE on the test set is a definite sign of overfitting.\n\n\ndef eval_map(pmf_model, train, test):\n    U = pmf_model.map['U']\n    V = pmf_model.map['V']\n\n    # Make predictions and calculate RMSE on train \n test sets.\n    predictions = pmf_model.predict(U, V)\n    train_rmse = rmse(train, predictions)\n    test_rmse = rmse(test, predictions)\n    overfit = test_rmse - train_rmse\n\n    # Print report.\n    print 'PMF MAP training RMSE: %.5f' % train_rmse\n    print 'PMF MAP testing RMSE:  %.5f' % test_rmse\n    print 'Train/test difference: %.5f' % overfit\n\n    return test_rmse\n\n\n# Add eval function to PMF class.\nPMF.eval_map = eval_map\n\n\n\n\n# Evaluate PMF MAP estimates.\npmf_map_rmse = pmf.eval_map(train, test)\npmf_improvement = baselines['mom'] - pmf_map_rmse\nprint 'PMF MAP Improvement:   %.5f' % pmf_improvement\n\n\n\n\nPMF MAP training RMSE: 4.00824\nPMF MAP testing RMSE:  4.02974\nTrain/test difference: 0.02150\nPMF MAP Improvement:   0.76858\n\n\n\nSo we see a pretty nice improvement here when compared to our best baseline, which was the mean of means method. We also have a fairly small difference in the RMSE values between the train and the test sets. This indicates that the point estimates for \n$\\alpha_U$\n and \n$\\alpha_V$\n that we calculated from our data are doing a good job of controlling model complexity. Now let's see if we can improve our estimates by approximating our posterior distribution with MCMC sampling. We'll draw 1000 samples and back them up using the \npymc3.backend.Text\n backend.\n\n\nPredictions using MCMC\n\n\n# Draw MCMC samples.\npmf.draw_samples(5000, njobs=3)\n\n# uncomment to load previous trace rather than drawing new samples.\n# pmf.load_trace()\n\n\n\n\nINFO:root:drawing 5000 samples using 3 jobs\n/home/mack/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *\nINFO:root:backing up trace to directory: data/pmf-mcmc-d5\n\n\n [-----------------100%-----------------] 5001 of 5000 complete in 7506.2 sec\n\n\n\nDiagnostics and Posterior Predictive Check\n\n\nThe next step is to check how many samples we should discard as burn-in. Normally, we'd do this using a traceplot to get some idea of where the sampled variables start to converge. In this case, we have high-dimensional samples, so we need to find a way to approximate them. One way was proposed by \nSalakhutdinov and Mnih, p.886\n. We can calculate the Frobenius norms of \n$U$\n and \n$V$\n at each step and monitor those for convergence. This essentially gives us some idea when the average magnitude of the latent variables is stabilizing. The equations for the Frobenius norms of \n$U$\n and \n$V$\n are shown below. We will use \nnumpy\n's \nlinalg\n package to calculate these.\n\n\n$$\n\\|U\\|_{Fro}^2 = \\sqrt{\\sum_{i=1}^N \\sum_{d=1}^D |U_{id}|^2}, \\hspace{40pt}\n\\|V\\|_{Fro}^2 = \\sqrt{\\sum_{j=1}^M \\sum_{d=1}^D |V_{jd}|^2}\n$$\n\n\ndef _norms(pmf_model, monitor=('U', 'V'), ord='fro'):\n    \nReturn norms of latent variables at each step in the\n    sample trace. These can be used to monitor convergence\n    of the sampler.\n    \n\n    monitor = ('U', 'V')\n    norms = {var: [] for var in monitor}\n    for sample in pmf_model.trace:\n        for var in monitor:\n            norms[var].append(np.linalg.norm(sample[var], ord))\n    return norms\n\n\ndef _traceplot(pmf_model):\n    \nPlot Frobenius norms of U and V as a function of sample #.\n\n    trace_norms = pmf_model.norms()\n    u_series = pd.Series(trace_norms['U'])\n    v_series = pd.Series(trace_norms['V'])\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    u_series.plot(kind='line', ax=ax1, grid=False,\n                  title=\n$\\|U\\|_{Fro}^2$ at Each Sample\n)\n    v_series.plot(kind='line', ax=ax2, grid=False,\n                  title=\n$\\|V\\|_{Fro}^2$ at Each Sample\n)\n    ax1.set_xlabel(\nSample Number\n)\n    ax2.set_xlabel(\nSample Number\n)\n\n\nPMF.norms = _norms\nPMF.traceplot = _traceplot\n\n\n\n\npmf.traceplot()\n\n\n\n\n\n\nIt appears we get convergence of \n$U$\n and \n$V$\n after about 200 samples. When testing for convergence, we also want to see convergence of the particular statistics we are looking for, since different characteristics of the posterior may converge at different rates. Let's also do a traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample.\n\n\ndef _running_rmse(pmf_model, test_data, train_data, burn_in=0, plot=True):\n    \nCalculate RMSE for each step of the trace to monitor convergence.\n    \n\n    burn_in = burn_in if len(pmf_model.trace) \n= burn_in else 0\n    results = {'per-step-train': [], 'running-train': [],\n               'per-step-test': [], 'running-test': []}\n    R = np.zeros(test_data.shape)\n    for cnt, sample in enumerate(pmf_model.trace[burn_in:]):\n        sample_R = pmf_model.predict(sample['U'], sample['V'])\n        R += sample_R\n        running_R = R / (cnt + 1)\n        results['per-step-train'].append(rmse(train_data, sample_R))\n        results['running-train'].append(rmse(train_data, running_R))\n        results['per-step-test'].append(rmse(test_data, sample_R))\n        results['running-test'].append(rmse(test_data, running_R))\n\n    results = pd.DataFrame(results)\n\n    if plot:\n        results.plot(\n            kind='line', grid=False, figsize=(15, 7),\n            title='Per-step and Running RMSE From Posterior Predictive')\n\n    # Return the final predictions, and the RMSE calculations\n    return running_R, results\n\n\nPMF.running_rmse = _running_rmse\n\n\n\n\npredicted, results = pmf.running_rmse(test, train, burn_in=200)\n\n\n\n\n\n\n# And our final RMSE?\nfinal_test_rmse = results['running-test'].values[-1]\nfinal_train_rmse = results['running-train'].values[-1]\nprint 'Posterior predictive train RMSE: %.5f' % final_train_rmse\nprint 'Posterior predictive test RMSE:  %.5f' % final_test_rmse\nprint 'Train/test difference:           %.5f' % (final_test_rmse - final_train_rmse)\nprint 'Improvement from MAP:            %.5f' % (pmf_map_rmse - final_test_rmse)\nprint 'Improvement from Mean of Means:  %.5f' % (baselines['mom'] - final_test_rmse)\n\n\n\n\nPosterior predictive train RMSE: 3.92230\nPosterior predictive test RMSE:  4.18027\nTrain/test difference:           0.25797\nImprovement from MAP:            -0.15052\nImprovement from Mean of Means:  0.61806\n\n\n\nWe have some interesting results here. As expected, our MCMC sampler provides lower error on the training set. However, it seems it does so at the cost of overfitting the data. This results in a decrease in test RMSE as compared to the MAP, even though it is still much better than our best baseline. So why might this be the case? Recall that we used point estimates for our precision paremeters \n$\\alpha_U$\n and \n$\\alpha_V$\n and we chose a fixed precision \n$\\alpha$\n. It is quite likely that by doing this, we constrained our posterior in a way that biased it towards the training data. In reality, the variance in the user ratings and the joke ratings is unlikely to be equal to the means of sample variances we used. Also, the most reasonable observation precision \n$\\alpha$\n is likely different as well.\n\n\nSummary of Results\n\n\nLet's summarize our results.\n\n\nsize = 100  # RMSE doesn't really change after 100th sample anyway.\nall_results = pd.DataFrame({\n    'uniform random': np.repeat(baselines['ur'], size),\n    'global means': np.repeat(baselines['gm'], size),\n    'mean of means': np.repeat(baselines['mom'], size),\n    'PMF MAP': np.repeat(pmf_map_rmse, size),\n    'PMF MCMC': results['running-test'][:size],\n})\nfig, ax = plt.subplots(figsize=(10, 5))\nall_results.plot(kind='line', grid=False, ax=ax,\n                 title='RMSE for all methods')\nax.set_xlabel(\nNumber of Samples\n)\nax.set_ylabel(\nRMSE\n)\n\n\n\n\nmatplotlib.text.Text at 0x7f5327451a10\n\n\n\n\n\n\nSummary\n\n\nWe set out to predict user preferences for unseen jokes. First we discussed the intuitive notion behind the user-user and item-item neighborhood approaches to collaborative filtering. Then we formalized our intuitions. With a firm understanding of our problem context, we moved on to exploring our subset of the Jester data. After discovering some general patterns, we defined three baseline methods: uniform random, global mean, and mean of means. With the goal of besting our baseline methods, we implemented the basic version of Probabilistic Matrix Factorization (PMF) using \npymc3\n.\n\n\nOur results demonstrate that the mean of means method is our best baseline on our prediction task. As expected, we are able to obtain a significant decrease in RMSE using the PMF MAP estimate obtained via Powell optimization. We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space using the Frobenius norms of the sampled variables. The traceplots using this method seem to indicate that our sampler converged to the posterior. Results using this posterior showed that attempting to improve the MAP estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters \n$\\alpha$\n, \n$\\alpha_U$\n, and \n$\\alpha_V$\n.\n\n\nAs a followup to this analysis, it would be interesting to also implement the logistic and constrained versions of PMF. We expect both models to outperform the basic PMF model. We could also implement the \nfully Bayesian version of PMF\n (BPMF), which places hyperpriors on the model parameters to automatically learn ideal mean and precision parameters for \n$U$\n and \n$V$\n. This would likely resolve the issue we faced in this analysis. We would expect BPMF to improve upon the MAP estimation produced here by learning more suitable hyperparameters and parameters. For a basic (but working!) implementation of BPMF in \npymc3\n, see \nthis gist\n.\n\n\nIf you made it this far, then congratulations! You now have some idea of how to build a basic recommender system. These same ideas and methods can be used on many different recommendation tasks. Items can be movies, products, advertisements, courses, or even other people. Any time you can build yourself a user-item matrix with user preferences in the cells, you can use these types of collaborative filtering algorithms to predict the missing values. If you want to learn more about recommender systems, the first reference is a good place to start.\n\n\nReferences\n\n\n\n\nY. Koren, R. Bell, and C. Volinsky, \u201cMatrix Factorization Techniques for Recommender Systems,\u201d Computer, vol. 42, no. 8, pp. 30\u201337, Aug. 2009.\n\n\nK. Goldberg, T. Roeder, D. Gupta, and C. Perkins, \u201cEigentaste: A constant time collaborative filtering algorithm,\u201d Information Retrieval, vol. 4, no. 2, pp. 133\u2013151, 2001.\n\n\nA. Mnih and R. Salakhutdinov, \u201cProbabilistic matrix factorization,\u201d in Advances in neural information processing systems, 2007, pp. 1257\u20131264.\n\n\nS. J. Nowlan and G. E. Hinton, \u201cSimplifying Neural Networks by Soft Weight-sharing,\u201d Neural Comput., vol. 4, no. 4, pp. 473\u2013493, Jul. 1992.\n\n\nR. Salakhutdinov and A. Mnih, \u201cBayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo,\u201d in Proceedings of the 25th International Conference on Machine Learning, New York, NY, USA, 2008, pp. 880\u2013887.",
            "title": "Probabilistic Matrix Factorization"
        },
        {
            "location": "/pmf-pymc/#probabilistic-matrix-factorization-for-making-personalized-recommendations",
            "text": "The model discussed in this analysis was developed by Ruslan Salakhutdinov and Andriy Mnih. All of the code and supporting text, when not referenced, is the original work of  Mack Sweeney .",
            "title": "Probabilistic Matrix Factorization for Making Personalized Recommendations"
        },
        {
            "location": "/pmf-pymc/#motivation",
            "text": "Say I download a handbook of a hundred jokes, and I'd like to know very quickly which ones will be my favorite. So maybe I read a few, I laugh, I read a few more, I stop laughing, and I indicate on a scale of -10 to 10 how funny I thought each joke was. Maybe I do this for 5 jokes out of the 100. Now I go to the back of the book, and there's a little program included for calculating my preferences for all the other jokes. I enter in my preference numbers and shazam! The program spits out a list of all 100 jokes, sorted in the order I'll like them. That certainly would be nice. Today we'll write a program that does exactly this.  We'll start out by getting some intuition for how our model will work. Then we'll formalize our intuition. Afterwards, we'll examine the dataset we are going to use. Once we have some notion of what our data looks like, we'll define some baseline methods for predicting preferences for jokes. Following that, we'll look at Probabilistic Matrix Factorization (PMF), which is a more sophisticated Bayesian method for predicting preferences. Having detailed the PMF model, we'll use PyMC3 for MAP estimation and MCMC inference. Finally, we'll compare the results obtained with PMF to those obtained from our baseline methods and discuss the outcome.",
            "title": "Motivation"
        },
        {
            "location": "/pmf-pymc/#intuition",
            "text": "Normally if we want recommendations for something, we try to find people who are similar to us and ask their opinions. If Bob, Alice, and Monty are all similar to me, and they all like knock-knock jokes, I'll probably like knock-knock jokes. Now this isn't always true. It depends on what we consider to be \"similar\". In order to get the best bang for our buck, we really want to look for people who have the most similar sense of humor. Humor being a complex beast, we'd probably like to break it down into something more understandable. We might try to characterize each joke in terms of various factors. Perhaps jokes can be dry, sarcastic, crude, sexual, political, etc. Now imagine we go through our handbook of jokes and assign each joke a rating in each of the categories. How dry is it? How sarcastic is it? How much does it use sexual innuendos? Perhaps we use numbers between 0 and 1 for each category. Intuitively, we might call this the joke's humor profile.  Now let's suppose we go back to those 5 jokes we rated. At this point, we can get a richer picture of our own preferences by looking at the humor profiles of each of the jokes we liked and didn't like. Perhaps we take the averages across the 5 humor profiles and call this our ideal type of joke. In other words, we have computed some notion of our inherent  preferences  for various types of jokes. Suppose Bob, Alice, and Monty all do the same. Now we can compare our preferences and determine how similar each of us really are. I might find that Bob is the most similar and the other two are still more similar than other people, but not as much as Bob. So I want recommendations from all three people, but when I make my final decision, I'm going to put more weight on Bob's recommendation than those I get from Alice and Monty.  While the above procedure sounds fairly effective as is, it also reveals an unexpected additional source of information. If we rated a particular joke highly, and we know its humor profile, we can compare with the profiles of other jokes. If we find one with very close numbers, it is probable we'll also enjoy this joke. Both this approach and the one above are commonly known as  neighborhood approaches . Techniques that leverage both of these approaches simultaneously are often called  collaborative filtering   [1] . The first approach we talked about uses user-user similarity, while the second uses item-item similarity. Ideally, we'd like to use both sources of information. The idea is we have a lot of items available to us, and we'd like to work together with others to filter the list of items down to those we'll each like best. My list should have the items I'll like best at the top and those I'll like least at the bottom. Everyone else wants the same. If I get together with a bunch of other people, we all read 5 jokes, and we have some efficient computational process to determine similarity, we can very quickly order the jokes to our liking.",
            "title": "Intuition"
        },
        {
            "location": "/pmf-pymc/#formalization",
            "text": "Let's take some time to make the intuitive notions we've been discussing more concrete. We have a set of  $M$  jokes, or  items  ( $M = 100$  in our example above). We also have  $N$  people, whom we'll call  users  of our recommender system. For each item, we'd like to find a  $D$  dimensional factor composition (humor profile above) to describe the item. Ideally, we'd like to do this without actually going through and manually labeling all of the jokes. Manual labeling would be both slow and error-prone, as different people will likely label jokes differently. So we model each joke as a  $D$  dimensional vector, which is its latent factor composition. Furthermore, we expect each user to have some preferences, but without our manual labeling and averaging procedure, we have to rely on the latent factor compositions to learn  $D$  dimensional latent preference vectors for each user. The only thing we get to observe is the  $N \\times M$  ratings matrix  $R$  provided by the users. Entry  $R_{ij}$  is the rating user  $i$  gave to item  $j$ . Many of these entries may be missing, since most users will not have rated all 100 jokes. Our goal is to fill in the missing values with predicted ratings based on the latent variables  $U$  and  $V$ . We denote the predicted ratings by  $R_{ij}^*$ . We also define an indicator matrix  $I$ , with entry  $I_{ij} = 0$  if  $R_{ij}$  is missing and  $I_{ij} = 1$  otherwise.  So we have an  $N \\times D$  matrix of user preferences which we'll call  $U$  and an  $M \\times D$  factor composition matrix we'll call  $V$ . We also have a  $N \\times M$  rating matrix we'll call  $R$ . We can think of each row  $U_i$  as indications of how much each user prefers each of the  $D$  latent factors. Each row  $V_j$  can be thought of as how much each item can be described by each of the latent factors. In order to make a recommendation, we need a suitable prediction function which maps a user preference vector  $U_i$  and an item latent factor vector  $V_j$  to a predicted ranking. The choice of this prediction function is an important modeling decision, and a variety of prediction functions have been used. Perhaps the most common is the dot product of the two vectors,  $U_i \\cdot V_j$   [1] .  To better understand CF techniques, let us explore a particular example. Imagine we are seeking to recommend jokes using a model which infers five latent factors,  $V_j$ , for  $j = 1,2,3,4,5$ . In reality, the latent factors are often unexplainable in a straightforward manner, and most models make no attempt to understand what information is being captured by each factor.  However, for the purposes of explanation, let us assume the five latent factors might end up capturing the humor profile we were discussing above. So our five latent factors are: dry, sarcastic, crude, sexual, and political. Then for a particular user  $i$ , imagine we infer a preference vector  $U_i =  0.2, 0.1, 0.3, 0.1, 0.3 $ . Also, for a particular item  $j$ , we infer these values for the latent factors:  $V_j =  0.5, 0.5, 0.25, 0.8, 0.9 $ . Using the dot product as the prediction function, we would calculate 0.575 as the ranking for that item, which is more or less a neutral preference given our -10 to 10 rating scale.  $$0.2 \\times 0.5 + 0.1 \\times 0.5 + 0.3 \\times 0.25 + 0.1 \\times 0.8 + 0.3\n\\times 0.9 = 0.575$$",
            "title": "Formalization"
        },
        {
            "location": "/pmf-pymc/#data",
            "text": "The  v1 Jester dataset  provides something very much like the handbook of jokes we have been discussing. The original version of this dataset was constructed in conjunction with the development of the  Eigentaste recommender system   [2] . At this point in time, v1 contains over 4.1 million continuous ratings in the range [-10, 10] of 100 jokes from 73,421 users. These ratings were collected between Apr. 1999 and May 2003. In order to reduce the training time of the model for illustrative purposes, 1,000 users who have rated all 100 jokes will be selected randomly. We will implement a model that is suitable for collaborative filtering on this data and evaluate it in terms of root mean squared error (RMSE) to validate the results.  Let's begin by exploring our data. We want to get a general feel for what it looks like and a sense for what sort of patterns it might contain.  % matplotlib inline\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n\ndata = pd.read_csv('data/jester-dataset-v1-dense-first-1000.csv')\ndata.head()   \n   \n     \n       \n       1 \n       2 \n       3 \n       4 \n       5 \n       6 \n       7 \n       8 \n       9 \n       10 \n       ... \n       91 \n       92 \n       93 \n       94 \n       95 \n       96 \n       97 \n       98 \n       99 \n       100 \n     \n   \n   \n     \n       0 \n        4.08 \n       -0.29 \n        6.36 \n        4.37 \n       -2.38 \n       -9.66 \n       -0.73 \n       -5.34 \n        8.88 \n        9.22 \n       ... \n        2.82 \n       -4.95 \n       -0.29 \n        7.86 \n       -0.19 \n       -2.14 \n        3.06 \n        0.34 \n       -4.32 \n        1.07 \n     \n     \n       1 \n       -6.17 \n       -3.54 \n        0.44 \n       -8.50 \n       -7.09 \n       -4.32 \n       -8.69 \n       -0.87 \n       -6.65 \n       -1.80 \n       ... \n       -3.54 \n       -6.89 \n       -0.68 \n       -2.96 \n       -2.18 \n       -3.35 \n        0.05 \n       -9.08 \n       -5.05 \n       -3.45 \n     \n     \n       2 \n        6.84 \n        3.16 \n        9.17 \n       -6.21 \n       -8.16 \n       -1.70 \n        9.27 \n        1.41 \n       -5.19 \n       -4.42 \n       ... \n        7.23 \n       -1.12 \n       -0.10 \n       -5.68 \n       -3.16 \n       -3.35 \n        2.14 \n       -0.05 \n        1.31 \n        0.00 \n     \n     \n       3 \n       -3.79 \n       -3.54 \n       -9.42 \n       -6.89 \n       -8.74 \n       -0.29 \n       -5.29 \n       -8.93 \n       -7.86 \n       -1.60 \n       ... \n        4.37 \n       -0.29 \n        4.17 \n       -0.29 \n       -0.29 \n       -0.29 \n       -0.29 \n       -0.29 \n       -3.40 \n       -4.95 \n     \n     \n       4 \n        1.31 \n        1.80 \n        2.57 \n       -2.38 \n        0.73 \n        0.73 \n       -0.97 \n        5.00 \n       -7.23 \n       -1.36 \n       ... \n        1.46 \n        1.70 \n        0.29 \n       -3.30 \n        3.45 \n        5.44 \n        4.08 \n        2.48 \n        4.51 \n        4.66 \n     \n     5 rows \u00d7 100 columns   # Extract the ratings from the DataFrame\nall_ratings = np.ndarray.flatten(data.values)\nratings = pd.Series(all_ratings)\n\n# Plot histogram and density.\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\nratings.plot(kind='density', ax=ax1, grid=False)\nax1.set_ylim(0, 0.08)\nax1.set_xlim(-11, 11)\n\n# Plot histogram\nratings.plot(kind='hist', ax=ax2, bins=20, grid=False)\nax2.set_xlim(-11, 11)\nplt.show()   ratings.describe()  count    100000.000000\nmean          0.996219\nstd           5.265215\nmin          -9.950000\n25%          -2.860000\n50%           1.650000\n75%           5.290000\nmax           9.420000\ndtype: float64  This must be a decent batch of jokes. From our exploration above, we know most ratings are in the range -1 to 10, and positive ratings are more likely than negative ratings. Let's look at the means for each joke to see if we have any particularly good (or bad) humor here.  joke_means = data.mean(axis=0)\njoke_means.plot(kind='bar', grid=False, figsize=(16, 6),\n                title= Mean Ratings for All 100 Jokes )  matplotlib.axes._subplots.AxesSubplot at 0x7fc46a3c5fd0    While the majority of the jokes generally get positive feedback from users, there are definitely a few that stand out as poor humor. Let's take a look at the worst and best joke, just for fun.  import os\n\n# Worst and best joke?\nworst_joke_id = joke_means.argmin()\nbest_joke_id = joke_means.argmax()\n\n# Let's see for ourselves. Load the jokes.\njoke_dir = 'data/jokes'\nfiles = [os.path.join(joke_dir, fname) for fname in os.listdir(joke_dir)]\njokes = [fname for fname in files if fname.endswith('txt')]\nnums = [filter(lambda c: c.isdigit(), fname) for fname in jokes]\njoke_dict = {k: v for k, v in zip(nums, jokes)}\n\ndef read_joke(joke_id):\n    fname = joke_dict[joke_id]\n    with open(fname) as f:\n        return f.read()\n\nprint 'The worst joke:\\n---------------\\n%s\\n' % read_joke(worst_joke_id)\nprint 'The best joke:\\n--------------\\n%s' % read_joke(best_joke_id)  The worst joke:\n---------------\nA Joke \nHow many teddybears does it take to change a lightbulb?\nIt takes only one teddybear, but it takes a whole lot of lightbulbs.\n\nThe best joke:\n--------------\nA Joke \nA radio conversation of a US naval ship with Canadian authorities ...\n\nAmericans: Please divert your course 15 degrees to the North to avoid a collision.\nCanadians: Recommend you divert YOUR course 15 degrees to the South to avoid a collision.\nAmericans: This is the Captain of a US Navy ship.  I say again, divert YOUR course.\nCanadians: No. I say again, you divert YOUR course.\nAmericans: This is the aircraft carrier USS LINCOLN, the second largest ship in the United States' Atlantic Fleet. We are accompanied by three destroyers, three cruisers and numerous support vessels. I demand that you change your course 15 degrees north, that's ONE FIVE DEGREES NORTH, or counter-measures will be undertaken to ensure the safety of this ship.\nCanadians: This is a lighthouse. Your call.  Make sense to me. We now know there are definite popularity differences between the jokes. Some of them are simply funnier than others, and some are downright lousy. Looking at the joke means allowed us to discover these general trends. Perhaps there are similar trends across users. It might be the case that some users are simply more easily humored than others. Let's take a look.  user_means = data.mean(axis=1)\nfig, ax = plt.subplots(figsize=(16, 6))\nuser_means.plot(kind='bar', grid=False, ax=ax,\n                title= Mean Ratings for All 1000 Users )\nax.set_xticklabels('')  # 1000 labels is nonsensical\nfig.show()   We see even more significant trends here. Some users rate nearly everything highly, and some (though not as many) rate nearly everything negatively. These observations will come in handy when considering models to use for predicting user preferences on unseen jokes.",
            "title": "Data"
        },
        {
            "location": "/pmf-pymc/#methods",
            "text": "Having explored the data, we're now ready to dig in and start addressing the problem. We want to predict how much each user is going to like all of the jokes he or she has not yet read.",
            "title": "Methods"
        },
        {
            "location": "/pmf-pymc/#baselines",
            "text": "Every good analysis needs some kind of baseline methods to compare against. It's difficult to claim we've produced good results if we have no reference point for what defines \"good\". We'll define three very simple baseline methods and find the RMSE using these methods. Our goal will be to obtain lower RMSE scores with whatever model we produce.  Uniform Random Baseline  Our first baseline is about as dead stupid as you can get. Every place we see a missing value in  $R$ , we'll simply fill it with a number drawn uniformly at random in the range [-10, 10]. We expect this method to do the worst by far.  $$R_{ij}^* \\sim Uniform$$  Global Mean Baseline  This method is only slightly better than the last. Wherever we have a missing value, we'll fill it in with the mean of all observed ratings.  $$\\text{global_mean} = \\frac{1}{N \\times M} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij}(R_{ij})$$  $$R_{ij}^* = \\text{global_mean}$$  Mean of Means Baseline  Now we're going to start getting a bit smarter. We imagine some users might be easily amused, and inclined to rate all jokes more highly. Other users might be the opposite. Additionally, some jokes might simply be more witty than others, so all users might rate some jokes more highly than others in general. We can clearly see this in our graph of the joke means above. We'll attempt to capture these general trends through per-user and per-joke rating means. We'll also incorporate the global mean to smooth things out a bit. So if we see a missing value in cell  $R_{ij}$ , we'll average the global mean with the mean of  $U_i$  and the mean of  $V_j$  and use that value to fill it in.  $$\\text{user_means} = \\frac{1}{M} \\sum_{j=1}^M I_{ij}(R_{ij})$$  $$\\text{joke_means} = \\frac{1}{N} \\sum_{i=1}^N I_{ij}(R_{ij})$$  $$R_{ij}^* = \\frac{1}{3} \\left(\\text{user_means}_i + \\text{ joke_means}_j + \\text{ global_mean} \\right)$$  from collections import OrderedDict\n\n\n# Create a base class with scaffolding for our 3 baselines.\n\ndef split_title(title):\n     Change  BaselineMethod  to  Baseline Method . \n    words = []\n    tmp = [title[0]]\n    for c in title[1:]:\n        if c.isupper():\n            words.append(''.join(tmp))\n            tmp = [c]\n        else:\n            tmp.append(c)\n    words.append(''.join(tmp))\n    return ' '.join(words)\n\n\nclass Baseline(object):\n     Calculate baseline predictions. \n\n    def __init__(self, train_data):\n         Simple heuristic-based transductive learning to fill in missing\n        values in data matrix. \n        self.predict(train_data.copy())\n\n    def predict(self, train_data):\n        raise NotImplementedError(\n            'baseline prediction not implemented for base class')\n\n    def rmse(self, test_data):\n         Calculate root mean squared error for predictions on test data. \n        return rmse(test_data, self.predicted)\n\n    def __str__(self):\n        return split_title(self.__class__.__name__)\n\n\n\n# Implement the 3 baselines.\n\nclass UniformRandomBaseline(Baseline):\n     Fill missing values with uniform random values. \n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        pmin, pmax = masked_train.min(), masked_train.max()\n        N = nan_mask.sum()\n        train_data[nan_mask] = np.random.uniform(pmin, pmax, N)\n        self.predicted = train_data\n\n\nclass GlobalMeanBaseline(Baseline):\n     Fill in missing values using the global mean. \n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        train_data[nan_mask] = train_data[~nan_mask].mean()\n        self.predicted = train_data\n\n\nclass MeanOfMeansBaseline(Baseline):\n     Fill in missing values using mean of user/item/global means. \n\n    def predict(self, train_data):\n        nan_mask = np.isnan(train_data)\n        masked_train = np.ma.masked_array(train_data, nan_mask)\n        global_mean = masked_train.mean()\n        user_means = masked_train.mean(axis=1)\n        item_means = masked_train.mean(axis=0)\n        self.predicted = train_data.copy()\n        n, m = train_data.shape\n        for i in xrange(n):\n            for j in xrange(m):\n                if np.ma.isMA(item_means[j]):\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i]))\n                else:\n                    self.predicted[i,j] = np.mean(\n                        (global_mean, user_means[i], item_means[j]))\n\n\nbaseline_methods = OrderedDict()\nbaseline_methods['ur'] = UniformRandomBaseline\nbaseline_methods['gm'] = GlobalMeanBaseline\nbaseline_methods['mom'] = MeanOfMeansBaseline",
            "title": "Baselines"
        },
        {
            "location": "/pmf-pymc/#probabilistic-matrix-factorization",
            "text": "Probabilistic Matrix Factorization (PMF)  [3] is a probabilistic approach to the collaborative filtering problem that takes a Bayesian perspective. The ratings  $R$  are modeled as draws from a Gaussian distribution.  The mean for  $R_{ij}$  is  $U_i V_j^T$ . The precision  $\\alpha$  is a fixed parameter that reflects the uncertainty of the estimations; the normal distribution is commonly reparameterized in terms of precision, which is the inverse of the variance. Complexity is controlled by placing zero-mean spherical Gaussian priors on  $U$  and  $V$ . In other words, each row of  $U$  is drawn from a multivariate Gaussian with mean  $\\mu = 0$  and precision which is some multiple of the identity matrix  $I$ . Those multiples are  $\\alpha_U$  for  $U$  and  $\\alpha_V$  for  $V$ . So our model is defined by:  $\\newcommand\\given[1][]{\\:#1\\vert\\:}$  \\begin{equation}\nP(R \\given U, V, \\alpha^2) = \n    \\prod_{i=1}^N \\prod_{j=1}^M\n        \\left[ \\mathcal{N}(R_{ij} \\given U_i V_j^T, \\alpha^{-1}) \\right]^{I_{ij}}\n\\end{equation}  \\begin{equation}\nP(U \\given \\alpha_U^2) =\n    \\prod_{i=1}^N \\mathcal{N}(U_i \\given 0, \\alpha_U^{-1} \\boldsymbol{I})\n\\end{equation}  \\begin{equation}\nP(V \\given \\alpha_U^2) =\n    \\prod_{j=1}^M \\mathcal{N}(V_j \\given 0, \\alpha_V^{-1} \\boldsymbol{I})\n\\end{equation}  Given small precision parameters, the priors on  $U$  and  $V$  ensure our latent variables do not grow too far from 0. This prevents overly strong user preferences and item factor compositions from being learned. This is commonly known as complexity control, where the complexity of the model here is measured by the magnitude of the latent variables. Controlling complexity like this helps prevent overfitting, which allows the model to generalize better for unseen data. We must also choose an appropriate  $\\alpha$  value for the normal distribution for  $R$ . So the challenge becomes choosing appropriate values for  $\\alpha_U$ ,  $\\alpha_V$ , and  $\\alpha$ . This challenge can be tackled with the soft weight-sharing methods discussed by  Nowland and Hinton, 1992  [4]. However, for the purposes of this analysis, we will stick to using point estimates obtained from our data.  import time\nimport logging\nimport pymc3 as pm\nimport theano\nimport scipy as sp\n\n\n# Enable on-the-fly graph computations, but ignore \n# absence of intermediate test values.\ntheano.config.compute_test_value = 'ignore'\n\n# Set up logging.\nlogger = logging.getLogger()\nlogger.setLevel(logging.INFO)\n\n\nclass PMF(object):\n     Probabilistic Matrix Factorization model using pymc3. \n\n    def __init__(self, train, dim, alpha=2, std=0.01, bounds=(-10, 10)):\n         Build the Probabilistic Matrix Factorization model using pymc3.\n\n        :param np.ndarray train: The training data to use for learning the model.\n        :param int dim: Dimensionality of the model; number of latent factors.\n        :param int alpha: Fixed precision for the likelihood function.\n        :param float std: Amount of noise to use for model initialization.\n        :param (tuple of int) bounds: (lower, upper) bound of ratings.\n            These bounds will simply be used to cap the estimates produced for R.\n\n         \n        self.dim = dim\n        self.alpha = alpha\n        self.std = np.sqrt(1.0 / alpha)\n        self.bounds = bounds\n        self.data = train.copy()\n        n, m = self.data.shape\n\n        # Perform mean value imputation\n        nan_mask = np.isnan(self.data)\n        self.data[nan_mask] = self.data[~nan_mask].mean()\n\n        # Low precision reflects uncertainty; prevents overfitting.\n        # Set to the mean variance across users and items.\n        self.alpha_u = 1 / self.data.var(axis=1).mean()\n        self.alpha_v = 1 / self.data.var(axis=0).mean()\n\n        # Specify the model.\n        logging.info('building the PMF model')\n        with pm.Model() as pmf:\n            U = pm.MvNormal(\n                'U', mu=0, tau=self.alpha_u * np.eye(dim),\n                shape=(n, dim), testval=np.random.randn(n, dim) * std)\n            V = pm.MvNormal(\n                'V', mu=0, tau=self.alpha_v * np.eye(dim),\n                shape=(m, dim), testval=np.random.randn(m, dim) * std)\n            R = pm.Normal(\n                'R', mu=theano.tensor.dot(U, V.T), tau=self.alpha * np.ones((n, m)),\n                observed=self.data)\n\n        logging.info('done building the PMF model') \n        self.model = pmf\n\n    def __str__(self):\n        return self.name  We'll also need functions for calculating the MAP and performing sampling on our PMF model. When the observation noise variance  $\\alpha$  and the prior variances  $\\alpha_U$  and  $\\alpha_V$  are all kept fixed, maximizing the log posterior is equivalent to minimizing the sum-of-squared-errors objective function with quadratic regularization terms.  $$\nE = \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - U_i V_j^T)^2 +\n    \\frac{\\lambda_U}{2} \\sum_{i=1}^N \\|U\\|_{Fro}^2 +\n    \\frac{\\lambda_V}{2} \\sum_{j=1}^M \\|V\\|_{Fro}^2,\n$$  where  $\\lambda_U = \\alpha_U / \\alpha$ ,  $\\lambda_V = \\alpha_V / \\alpha$ , and  $\\|\\cdot\\|_{Fro}^2$  denotes the Frobenius norm [3]. Minimizing this objective function gives a local minimum, which is essentially a maximum a posteriori (MAP) estimate. While it is possible to use a fast Stochastic Gradient Descent procedure to find this MAP, we'll be finding it using the utilities built into  pymc3 . In particular, we'll use  find_MAP  with Powell optimization ( scipy.optimize.fmin_powell ). Having found this MAP estimate, we can use it as our starting point for MCMC sampling.  Since it is a reasonably complex model, we expect the MAP estimation to take some time. So let's save it after we've found it. Note that we define a function for finding the MAP below, assuming it will receive a namespace with some variables in it. Then we attach that function to the PMF class, where it will have such a namespace after initialization. The PMF class is defined in pieces this way so I can say a few things between each piece to make it clearer.  try:\n    import ujson as json\nexcept ImportError:\n    import json\n\n\n# First define functions to save our MAP estimate after it is found.\n# We adapt these from `pymc3`'s `backends` module, where the original\n# code is used to save the traces from MCMC samples.\ndef save_np_vars(vars, savedir):\n     Save a dictionary of numpy variables to `savedir`. We assume\n    the directory does not exist; an OSError will be raised if it does.\n     \n    logging.info('writing numpy vars to directory: %s' % savedir)\n    os.mkdir(savedir)\n    shapes = {}\n    for varname in vars:\n        data = vars[varname]\n        var_file = os.path.join(savedir, varname + '.txt')\n        np.savetxt(var_file, data.reshape(-1, data.size))\n        shapes[varname] = data.shape\n\n        ## Store shape information for reloading.\n        shape_file = os.path.join(savedir, 'shapes.json')\n        with open(shape_file, 'w') as sfh:\n            json.dump(shapes, sfh)\n\n\ndef load_np_vars(savedir):\n     Load numpy variables saved with `save_np_vars`. \n    shape_file = os.path.join(savedir, 'shapes.json')\n    with open(shape_file, 'r') as sfh:\n        shapes = json.load(sfh)\n\n    vars = {}\n    for varname, shape in shapes.items():\n        var_file = os.path.join(savedir, varname + '.txt')\n        vars[varname] = np.loadtxt(var_file).reshape(shape)\n\n    return vars\n\n\n# Now define the MAP estimation infrastructure.\ndef _map_dir(self):\n    basename = 'pmf-map-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _find_map(self):\n     Find mode of posterior using Powell optimization. \n    tstart = time.time()\n    with self.model:\n        logging.info('finding PMF MAP using Powell optimization...')\n        self._map = pm.find_MAP(fmin=sp.optimize.fmin_powell, disp=True)\n\n    elapsed = int(time.time() - tstart)\n    logging.info('found PMF MAP in %d seconds' % elapsed)\n\n    # This is going to take a good deal of time to find, so let's save it.\n    save_np_vars(self._map, self.map_dir)\n\ndef _load_map(self):\n    self._map = load_np_vars(self.map_dir)\n\ndef _map(self):\n    try:\n        return self._map\n    except:\n        if os.path.isdir(self.map_dir):\n            self.load_map()\n        else:\n            self.find_map()\n        return self._map\n\n\n# Update our class with the new MAP infrastructure.\nPMF.find_map = _find_map\nPMF.load_map = _load_map\nPMF.map_dir = property(_map_dir)\nPMF.map = property(_map)  So now our PMF class has a  map   property  which will either be found using Powell optimization or loaded from a previous optimization. Once we have the MAP, we can use it as a starting point for our MCMC sampler. We'll need a sampling function in order to draw MCMC samples to approximate the posterior distribution of the PMF model.  # Draw MCMC samples.\ndef _trace_dir(self):\n    basename = 'pmf-mcmc-d%d' % self.dim\n    return os.path.join('data', basename)\n\ndef _draw_samples(self, nsamples=1000, njobs=2):\n    # First make sure the trace_dir does not already exist.\n    if os.path.isdir(self.trace_dir):\n        raise OSError(\n            'trace directory %s already exists. Please move or delete.' % self.trace_dir)\n    start = self.map  # use our MAP as the starting point\n    with self.model:\n        logging.info('drawing %d samples using %d jobs' % (nsamples, njobs))\n        step = pm.NUTS(scaling=start)\n        backend = pm.backends.Text(self.trace_dir)\n        logging.info('backing up trace to directory: %s' % self.trace_dir)\n        self.trace = pm.sample(nsamples, step, start=start, njobs=njobs, trace=backend)\n\ndef _load_trace(self):\n    with self.model:\n        self.trace = pm.backends.text.load(self.trace_dir)\n\n\n# Update our class with the sampling infrastructure.\nPMF.trace_dir = property(_trace_dir)\nPMF.draw_samples = _draw_samples\nPMF.load_trace = _load_trace  We could define some kind of default trace property like we did for the MAP, but that would mean using possibly nonsensical values for  nsamples  and  njobs . Better to leave it as a non-optional call to  draw_samples . Finally, we'll need a function to make predictions using our inferred values for  $U$  and  $V$ . For user  $i$  and joke  $j$ , a prediction is generated by drawing from  $\\mathcal{N}(U_i V_j^T, \\alpha)$ . To generate predictions from the sampler, we generate an  $R$  matrix for each  $U$  and  $V$  sampled, then we combine these by averaging over the  $K$  samples.  \\begin{equation}\nP(R_{ij}^* \\given R, \\alpha, \\alpha_U, \\alpha_V) \\approx\n    \\frac{1}{K} \\sum_{k=1}^K \\mathcal{N}(U_i V_j^T, \\alpha)\n\\end{equation}  We'll want to inspect the individual  $R$  matrices before averaging them for diagnostic purposes. So we'll write code for the averaging piece during evaluation. The function below simply draws an  $R$  matrix given a  $U$  and  $V$  and the fixed  $\\alpha$  stored in the PMF object.  def _predict(self, U, V):\n     Estimate R from the given values of U and V. \n    R = np.dot(U, V.T)\n    n, m = R.shape\n    sample_R = np.array([\n        [np.random.normal(R[i,j], self.std) for j in xrange(m)]\n        for i in xrange(n)\n    ])\n\n    # bound ratings\n    low, high = self.bounds\n    sample_R[sample_R   low] = low\n    sample_R[sample_R   high] = high\n    return sample_R\n\n\nPMF.predict = _predict  One final thing to note: the dot products in this model are often constrained using a logistic function  $g(x) = 1/(1 + exp(-x))$ , that bounds the predictions to the range [0, 1]. To facilitate this bounding, the ratings are also mapped to the range [0, 1] using  $t(x) = (x + min) / range$ . The authors of PMF also introduced a constrained version which performs better on users with less ratings [3]. Both models are generally improvements upon the basic model presented here. However, in the interest of time and space, these will not be implemented here.",
            "title": "Probabilistic Matrix Factorization"
        },
        {
            "location": "/pmf-pymc/#evaluation",
            "text": "",
            "title": "Evaluation"
        },
        {
            "location": "/pmf-pymc/#metrics",
            "text": "In order to understand how effective our models are, we'll need to be able to evaluate them. We'll be evaluating in terms of root mean squared error (RMSE), which looks like this:  \\begin{equation}\nRMSE = \\sqrt{ \\frac{ \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} (R_{ij} - R_{ij}^*)^2 }\n                   { \\sum_{i=1}^N \\sum_{j=1}^M I_{ij} } }\n\\end{equation}  In this case, the RMSE can be thought of as the standard deviation of our predictions from the actual user preferences.  # Define our evaluation function.\ndef rmse(test_data, predicted):\n     Calculate root mean squared error.\n    Ignoring missing values in the test data.\n     \n    I = ~np.isnan(test_data)   # indicator for missing values\n    N = I.sum()                # number of non-missing values\n    sqerror = abs(test_data - predicted) ** 2  # squared error array\n    mse = sqerror[I].sum() / N                 # mean squared error\n    return np.sqrt(mse)                        # RMSE",
            "title": "Metrics"
        },
        {
            "location": "/pmf-pymc/#training-data-vs-test-data",
            "text": "The next thing we need to do is split our data into a training set and a test set. Matrix factorization techniques use  transductive learning  rather than inductive learning. So we produce a test set by taking a random sample of the cells in the full  $N \\times M$  data matrix. The values selected as test samples are replaced with  nan  values in a copy of the original data matrix to produce the training set. Since we'll be producing random splits, let's also write out the train/test sets generated. This will allow us to replicate our results. We'd like to be able to idenfity which split is which, so we'll take a hash of the indices selected for testing and use that to save the data.  import hashlib\n\n\n# Define a function for splitting train/test data.\ndef split_train_test(data, percent_test=10):\n     Split the data into train/test sets.\n    :param int percent_test: Percentage of data to use for testing. Default 10.\n     \n    n, m = data.shape             # # users, # jokes\n    N = n * m                     # # cells in matrix\n    test_size = N / percent_test  # use 10% of data as test set\n    train_size = N - test_size    # and remainder for training\n\n    # Prepare train/test ndarrays.\n    train = data.copy().values\n    test = np.ones(data.shape) * np.nan\n\n    # Draw random sample of training data to use for testing.\n    tosample = np.where(~np.isnan(train))       # ignore nan values in data\n    idx_pairs = zip(tosample[0], tosample[1])   # tuples of row/col index pairs\n    indices = np.arange(len(idx_pairs))         # indices of index pairs\n    sample = np.random.choice(indices, replace=False, size=test_size)\n\n    # Transfer random sample from train set to test set.\n    for idx in sample:\n        idx_pair = idx_pairs[idx]\n        test[idx_pair] = train[idx_pair]  # transfer to test set\n        train[idx_pair] = np.nan          # remove from train set\n\n    # Verify everything worked properly\n    assert(np.isnan(train).sum() == test_size)\n    assert(np.isnan(test).sum() == train_size)\n\n    # Finally, hash the indices and save the train/test sets.\n    index_string = ''.join(map(str, np.sort(sample)))\n    name = hashlib.sha1(index_string).hexdigest()\n    savedir = os.path.join('data', name)\n    save_np_vars({'train': train, 'test': test}, savedir)\n\n    # Return train set, test set, and unique hash of indices.\n    return train, test, name\n\n\ndef load_train_test(name):\n     Load the train/test sets. \n    savedir = os.path.join('data', name)\n    vars = load_np_vars(savedir)\n    return vars['train'], vars['test']\n\n# train, test, name = split_train_test(data)  In order to facilitate reproducibility, I've produced a train/test split using the code above which we'll now use for all the evaluations below.  train, test = load_train_test('6bb8d06c69c0666e6da14c094d4320d115f1ffc8')",
            "title": "Training Data vs. Test Data"
        },
        {
            "location": "/pmf-pymc/#results",
            "text": "# Let's see the results:\nbaselines = {}\nfor name in baseline_methods:\n    Method = baseline_methods[name]\n    method = Method(train)\n    baselines[name] = method.rmse(test)\n    print '%s RMSE:\\t%.5f' % (method, baselines[name])  Uniform Random Baseline RMSE:   7.77062\nGlobal Mean Baseline RMSE:  5.25004\nMean Of Means Baseline RMSE:    4.79832  As expected: the uniform random baseline is the worst by far, the global mean baseline is next best, and the mean of means method is our best baseline. Now let's see how PMF stacks up.  # We use a fixed precision for the likelihood.\n# This reflects uncertainty in the dot product.\n# We choose 2 in the footsteps Salakhutdinov\n# Mnihof.\nALPHA = 2\n\n# The dimensionality D; the number of latent factors.\n# We can adjust this higher to try to capture more subtle\n# characteristics of each joke. However, the higher it is,\n# the more expensive our inference procedures will be.\n# Specifically, we have D(N + M) latent variables. For our\n# Jester dataset, this means we have D(1100), so for 5\n# dimensions, we are sampling 5500 latent variables.\nDIM = 5\n\n\npmf = PMF(train, DIM, ALPHA, std=0.05)  INFO:root:building the PMF model\nINFO:root:done building the PMF model",
            "title": "Results"
        },
        {
            "location": "/pmf-pymc/#predictions-using-map",
            "text": "# Find MAP for PMF.\npmf.find_map()\n# pmf.load_map()  INFO:root:finding MAP using Powell optimization...\nINFO:root:found MAP in 2575 seconds\n\n\nOptimization terminated successfully.\n         Current function value: 1553644.881552\n         Iterations: 33\n         Function evaluations: 1644948  Excellent. The first thing we want to do is make sure the MAP estimate we obtained is reasonable. We can do this by computing RMSE on the predicted ratings obtained from the MAP values of  $U$  and  $V$ . First we define a function for generating the predicted ratings  $R$  from  $U$  and  $V$ . We ensure the actual rating bounds are enforced by setting all values below -10 to -10 and all values above 10 to 10. Finally, we compute RMSE for both the training set and the test set. We expect the test RMSE to be higher. The difference between the two gives some idea of how much we have overfit. Some difference is always expected, but a very low RMSE on the training set with a high RMSE on the test set is a definite sign of overfitting.  def eval_map(pmf_model, train, test):\n    U = pmf_model.map['U']\n    V = pmf_model.map['V']\n\n    # Make predictions and calculate RMSE on train   test sets.\n    predictions = pmf_model.predict(U, V)\n    train_rmse = rmse(train, predictions)\n    test_rmse = rmse(test, predictions)\n    overfit = test_rmse - train_rmse\n\n    # Print report.\n    print 'PMF MAP training RMSE: %.5f' % train_rmse\n    print 'PMF MAP testing RMSE:  %.5f' % test_rmse\n    print 'Train/test difference: %.5f' % overfit\n\n    return test_rmse\n\n\n# Add eval function to PMF class.\nPMF.eval_map = eval_map  # Evaluate PMF MAP estimates.\npmf_map_rmse = pmf.eval_map(train, test)\npmf_improvement = baselines['mom'] - pmf_map_rmse\nprint 'PMF MAP Improvement:   %.5f' % pmf_improvement  PMF MAP training RMSE: 4.00824\nPMF MAP testing RMSE:  4.02974\nTrain/test difference: 0.02150\nPMF MAP Improvement:   0.76858  So we see a pretty nice improvement here when compared to our best baseline, which was the mean of means method. We also have a fairly small difference in the RMSE values between the train and the test sets. This indicates that the point estimates for  $\\alpha_U$  and  $\\alpha_V$  that we calculated from our data are doing a good job of controlling model complexity. Now let's see if we can improve our estimates by approximating our posterior distribution with MCMC sampling. We'll draw 1000 samples and back them up using the  pymc3.backend.Text  backend.",
            "title": "Predictions Using MAP"
        },
        {
            "location": "/pmf-pymc/#predictions-using-mcmc",
            "text": "# Draw MCMC samples.\npmf.draw_samples(5000, njobs=3)\n\n# uncomment to load previous trace rather than drawing new samples.\n# pmf.load_trace()  INFO:root:drawing 5000 samples using 3 jobs\n/home/mack/anaconda/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:133: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *\nINFO:root:backing up trace to directory: data/pmf-mcmc-d5\n\n\n [-----------------100%-----------------] 5001 of 5000 complete in 7506.2 sec  Diagnostics and Posterior Predictive Check  The next step is to check how many samples we should discard as burn-in. Normally, we'd do this using a traceplot to get some idea of where the sampled variables start to converge. In this case, we have high-dimensional samples, so we need to find a way to approximate them. One way was proposed by  Salakhutdinov and Mnih, p.886 . We can calculate the Frobenius norms of  $U$  and  $V$  at each step and monitor those for convergence. This essentially gives us some idea when the average magnitude of the latent variables is stabilizing. The equations for the Frobenius norms of  $U$  and  $V$  are shown below. We will use  numpy 's  linalg  package to calculate these.  $$\n\\|U\\|_{Fro}^2 = \\sqrt{\\sum_{i=1}^N \\sum_{d=1}^D |U_{id}|^2}, \\hspace{40pt}\n\\|V\\|_{Fro}^2 = \\sqrt{\\sum_{j=1}^M \\sum_{d=1}^D |V_{jd}|^2}\n$$  def _norms(pmf_model, monitor=('U', 'V'), ord='fro'):\n     Return norms of latent variables at each step in the\n    sample trace. These can be used to monitor convergence\n    of the sampler.\n     \n    monitor = ('U', 'V')\n    norms = {var: [] for var in monitor}\n    for sample in pmf_model.trace:\n        for var in monitor:\n            norms[var].append(np.linalg.norm(sample[var], ord))\n    return norms\n\n\ndef _traceplot(pmf_model):\n     Plot Frobenius norms of U and V as a function of sample #. \n    trace_norms = pmf_model.norms()\n    u_series = pd.Series(trace_norms['U'])\n    v_series = pd.Series(trace_norms['V'])\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 7))\n    u_series.plot(kind='line', ax=ax1, grid=False,\n                  title= $\\|U\\|_{Fro}^2$ at Each Sample )\n    v_series.plot(kind='line', ax=ax2, grid=False,\n                  title= $\\|V\\|_{Fro}^2$ at Each Sample )\n    ax1.set_xlabel( Sample Number )\n    ax2.set_xlabel( Sample Number )\n\n\nPMF.norms = _norms\nPMF.traceplot = _traceplot  pmf.traceplot()   It appears we get convergence of  $U$  and  $V$  after about 200 samples. When testing for convergence, we also want to see convergence of the particular statistics we are looking for, since different characteristics of the posterior may converge at different rates. Let's also do a traceplot of the RSME. We'll compute RMSE for both the train and the test set, even though the convergence is indicated by RMSE on the training set alone. In addition, let's compute a running RMSE on the train/test sets to see how aggregate performance improves or decreases as we continue to sample.  def _running_rmse(pmf_model, test_data, train_data, burn_in=0, plot=True):\n     Calculate RMSE for each step of the trace to monitor convergence.\n     \n    burn_in = burn_in if len(pmf_model.trace)  = burn_in else 0\n    results = {'per-step-train': [], 'running-train': [],\n               'per-step-test': [], 'running-test': []}\n    R = np.zeros(test_data.shape)\n    for cnt, sample in enumerate(pmf_model.trace[burn_in:]):\n        sample_R = pmf_model.predict(sample['U'], sample['V'])\n        R += sample_R\n        running_R = R / (cnt + 1)\n        results['per-step-train'].append(rmse(train_data, sample_R))\n        results['running-train'].append(rmse(train_data, running_R))\n        results['per-step-test'].append(rmse(test_data, sample_R))\n        results['running-test'].append(rmse(test_data, running_R))\n\n    results = pd.DataFrame(results)\n\n    if plot:\n        results.plot(\n            kind='line', grid=False, figsize=(15, 7),\n            title='Per-step and Running RMSE From Posterior Predictive')\n\n    # Return the final predictions, and the RMSE calculations\n    return running_R, results\n\n\nPMF.running_rmse = _running_rmse  predicted, results = pmf.running_rmse(test, train, burn_in=200)   # And our final RMSE?\nfinal_test_rmse = results['running-test'].values[-1]\nfinal_train_rmse = results['running-train'].values[-1]\nprint 'Posterior predictive train RMSE: %.5f' % final_train_rmse\nprint 'Posterior predictive test RMSE:  %.5f' % final_test_rmse\nprint 'Train/test difference:           %.5f' % (final_test_rmse - final_train_rmse)\nprint 'Improvement from MAP:            %.5f' % (pmf_map_rmse - final_test_rmse)\nprint 'Improvement from Mean of Means:  %.5f' % (baselines['mom'] - final_test_rmse)  Posterior predictive train RMSE: 3.92230\nPosterior predictive test RMSE:  4.18027\nTrain/test difference:           0.25797\nImprovement from MAP:            -0.15052\nImprovement from Mean of Means:  0.61806  We have some interesting results here. As expected, our MCMC sampler provides lower error on the training set. However, it seems it does so at the cost of overfitting the data. This results in a decrease in test RMSE as compared to the MAP, even though it is still much better than our best baseline. So why might this be the case? Recall that we used point estimates for our precision paremeters  $\\alpha_U$  and  $\\alpha_V$  and we chose a fixed precision  $\\alpha$ . It is quite likely that by doing this, we constrained our posterior in a way that biased it towards the training data. In reality, the variance in the user ratings and the joke ratings is unlikely to be equal to the means of sample variances we used. Also, the most reasonable observation precision  $\\alpha$  is likely different as well.",
            "title": "Predictions using MCMC"
        },
        {
            "location": "/pmf-pymc/#summary-of-results",
            "text": "Let's summarize our results.  size = 100  # RMSE doesn't really change after 100th sample anyway.\nall_results = pd.DataFrame({\n    'uniform random': np.repeat(baselines['ur'], size),\n    'global means': np.repeat(baselines['gm'], size),\n    'mean of means': np.repeat(baselines['mom'], size),\n    'PMF MAP': np.repeat(pmf_map_rmse, size),\n    'PMF MCMC': results['running-test'][:size],\n})\nfig, ax = plt.subplots(figsize=(10, 5))\nall_results.plot(kind='line', grid=False, ax=ax,\n                 title='RMSE for all methods')\nax.set_xlabel( Number of Samples )\nax.set_ylabel( RMSE )  matplotlib.text.Text at 0x7f5327451a10",
            "title": "Summary of Results"
        },
        {
            "location": "/pmf-pymc/#summary",
            "text": "We set out to predict user preferences for unseen jokes. First we discussed the intuitive notion behind the user-user and item-item neighborhood approaches to collaborative filtering. Then we formalized our intuitions. With a firm understanding of our problem context, we moved on to exploring our subset of the Jester data. After discovering some general patterns, we defined three baseline methods: uniform random, global mean, and mean of means. With the goal of besting our baseline methods, we implemented the basic version of Probabilistic Matrix Factorization (PMF) using  pymc3 .  Our results demonstrate that the mean of means method is our best baseline on our prediction task. As expected, we are able to obtain a significant decrease in RMSE using the PMF MAP estimate obtained via Powell optimization. We illustrated one way to monitor convergence of an MCMC sampler with a high-dimensionality sampling space using the Frobenius norms of the sampled variables. The traceplots using this method seem to indicate that our sampler converged to the posterior. Results using this posterior showed that attempting to improve the MAP estimation using MCMC sampling actually overfit the training data and increased test RMSE. This was likely caused by the constraining of the posterior via fixed precision parameters  $\\alpha$ ,  $\\alpha_U$ , and  $\\alpha_V$ .  As a followup to this analysis, it would be interesting to also implement the logistic and constrained versions of PMF. We expect both models to outperform the basic PMF model. We could also implement the  fully Bayesian version of PMF  (BPMF), which places hyperpriors on the model parameters to automatically learn ideal mean and precision parameters for  $U$  and  $V$ . This would likely resolve the issue we faced in this analysis. We would expect BPMF to improve upon the MAP estimation produced here by learning more suitable hyperparameters and parameters. For a basic (but working!) implementation of BPMF in  pymc3 , see  this gist .  If you made it this far, then congratulations! You now have some idea of how to build a basic recommender system. These same ideas and methods can be used on many different recommendation tasks. Items can be movies, products, advertisements, courses, or even other people. Any time you can build yourself a user-item matrix with user preferences in the cells, you can use these types of collaborative filtering algorithms to predict the missing values. If you want to learn more about recommender systems, the first reference is a good place to start.",
            "title": "Summary"
        },
        {
            "location": "/pmf-pymc/#references",
            "text": "Y. Koren, R. Bell, and C. Volinsky, \u201cMatrix Factorization Techniques for Recommender Systems,\u201d Computer, vol. 42, no. 8, pp. 30\u201337, Aug. 2009.  K. Goldberg, T. Roeder, D. Gupta, and C. Perkins, \u201cEigentaste: A constant time collaborative filtering algorithm,\u201d Information Retrieval, vol. 4, no. 2, pp. 133\u2013151, 2001.  A. Mnih and R. Salakhutdinov, \u201cProbabilistic matrix factorization,\u201d in Advances in neural information processing systems, 2007, pp. 1257\u20131264.  S. J. Nowlan and G. E. Hinton, \u201cSimplifying Neural Networks by Soft Weight-sharing,\u201d Neural Comput., vol. 4, no. 4, pp. 473\u2013493, Jul. 1992.  R. Salakhutdinov and A. Mnih, \u201cBayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo,\u201d in Proceedings of the 25th International Conference on Machine Learning, New York, NY, USA, 2008, pp. 880\u2013887.",
            "title": "References"
        },
        {
            "location": "/posterior_predictive/",
            "text": "Posterior Predictive Checks in PyMC3\n\n\nPPCs are a great way to validate a model. The idea is to generate data sets from the model using parameter settings from draws from the posterior.\n\n\nPyMC3\n has random number support thanks to \nMark Wibrow\n as implemented in \nPR784\n.\n\n\nHere we will implement a general routine to draw samples from the observed nodes of a model.\n\n\n%load_ext autoreload\n%autoreload 2\n\n\n\n\n%matplotlib inline\nimport numpy as np\nimport pymc3 as pm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\n\n\n\n\nLets generate a very simple model:\n\n\ndata = np.random.randn(100)\n\nwith pm.Model() as model: \n    mu = pm.Normal('mu', mu=0, sd=1, testval=0)\n    sd = pm.HalfNormal('sd', sd=1)\n    n = pm.Normal('n', mu=mu, sd=sd, observed=data)\n\n    step = pm.NUTS()\n    trace = pm.sample(5000, step)\n\n\n\n\n [-----------------100%-----------------] 5000 of 5000 complete in 2.5 sec\n\n/Users/santon/anaconda/envs/pymc3/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:135: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *\n\n\n\npm.traceplot(trace);\n\n\n\n\n\n\nThis function will randomly draw 50 samples of parameters from the trace. Then, for each sample, it will draw 100 random numbers from a normal distribution specified by the values of \nmu\n and \nstd\n in that sample.\n\n\nppc = pm.sample_ppc(trace, samples=500, model=model, size=100)\n\n\n\n\nNow, \nppc\n contains 500 generated data sets (containing 100 samples each), each using a different parameter setting from the posterior:\n\n\nnp.asarray(ppc['n']).shape\n\n\n\n\n(500, 100)\n\n\n\nOne common way to visualize is to look if the model can reproduce the patterns observed in the real data. For example, how close are the inferred means to the actual sample mean:\n\n\nax = plt.subplot()\nsns.distplot([n.mean() for n in ppc['n']], kde=False, ax=ax)\nax.axvline(data.mean())\nax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');\n\n\n\n\n\n\nPrediction\n\n\nThe same pattern can be used for prediction. Here we're building a logistic regression model. Note that since we're dealing the full posterior, we're also getting uncertainty in our predictions for free.\n\n\n# Use a theano shared variable to be able to exchange the data the model runs on\nfrom theano import shared\n\n\n\n\ndef invlogit(x):\n    return np.exp(x) / (1 + np.exp(x))\n\nn = 4000\nn_oos = 50\ncoeff = 1.\n\npredictors = np.random.normal(size=n)\n# Turn predictor into a shared var so that we can change it later\npredictors_shared = shared(predictors)\n\noutcomes = np.random.binomial(1, invlogit(coeff * predictors))\n\n\n\n\noutcomes\n\n\n\n\narray([1, 1, 0, ..., 0, 1, 1])\n\n\n\npredictors_oos = np.random.normal(size=50)\noutcomes_oos = np.random.binomial(1, invlogit(coeff * predictors_oos))\n\n\n\n\ndef tinvlogit(x):\n    import theano.tensor as t\n    return t.exp(x) / (1 + t.exp(x))\n\nwith pm.Model() as model:\n    coeff = pm.Normal('coeff', mu=0, sd=1)\n    p = tinvlogit(coeff * predictors_shared)\n\n    o = pm.Bernoulli('o', p, observed=outcomes)\n\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(500, step)\n\n\n\n\n [-----------------100%-----------------] 500 of 500 complete in 2.9 sec\n\n\n\n# Changing values here will also change values in the model\npredictors_shared.set_value(predictors_oos)\n\n\n\n\n# Simply running PPC will use the updated values and do prediction\nppc = pm.sample_ppc(trace, model=model, samples=500)\n\n\n\n\nMean predicted values plus error bars to give sense of uncertainty in prediction\n\n\nplt.errorbar(x=predictors_oos, y=np.asarray(ppc['o']).mean(axis=0), yerr=np.asarray(ppc['o']).std(axis=0), linestyle='', marker='o')\nplt.plot(predictors_oos, outcomes_oos, 'o')\nplt.ylim(-.05, 1.05)\nplt.xlabel('predictor')\nplt.ylabel('outcome')\n\n\n\n\nmatplotlib.text.Text at 0x129c81890",
            "title": "Posterior Predictive checks and prediction"
        },
        {
            "location": "/posterior_predictive/#posterior-predictive-checks-in-pymc3",
            "text": "PPCs are a great way to validate a model. The idea is to generate data sets from the model using parameter settings from draws from the posterior.  PyMC3  has random number support thanks to  Mark Wibrow  as implemented in  PR784 .  Here we will implement a general routine to draw samples from the observed nodes of a model.  %load_ext autoreload\n%autoreload 2  %matplotlib inline\nimport numpy as np\nimport pymc3 as pm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict  Lets generate a very simple model:  data = np.random.randn(100)\n\nwith pm.Model() as model: \n    mu = pm.Normal('mu', mu=0, sd=1, testval=0)\n    sd = pm.HalfNormal('sd', sd=1)\n    n = pm.Normal('n', mu=mu, sd=sd, observed=data)\n\n    step = pm.NUTS()\n    trace = pm.sample(5000, step)   [-----------------100%-----------------] 5000 of 5000 complete in 2.5 sec\n\n/Users/santon/anaconda/envs/pymc3/lib/python2.7/site-packages/theano/scan_module/scan_perform_ext.py:135: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility\n  from scan_perform.scan_perform import *  pm.traceplot(trace);   This function will randomly draw 50 samples of parameters from the trace. Then, for each sample, it will draw 100 random numbers from a normal distribution specified by the values of  mu  and  std  in that sample.  ppc = pm.sample_ppc(trace, samples=500, model=model, size=100)  Now,  ppc  contains 500 generated data sets (containing 100 samples each), each using a different parameter setting from the posterior:  np.asarray(ppc['n']).shape  (500, 100)  One common way to visualize is to look if the model can reproduce the patterns observed in the real data. For example, how close are the inferred means to the actual sample mean:  ax = plt.subplot()\nsns.distplot([n.mean() for n in ppc['n']], kde=False, ax=ax)\nax.axvline(data.mean())\nax.set(title='Posterior predictive of the mean', xlabel='mean(x)', ylabel='Frequency');",
            "title": "Posterior Predictive Checks in PyMC3"
        },
        {
            "location": "/posterior_predictive/#prediction",
            "text": "The same pattern can be used for prediction. Here we're building a logistic regression model. Note that since we're dealing the full posterior, we're also getting uncertainty in our predictions for free.  # Use a theano shared variable to be able to exchange the data the model runs on\nfrom theano import shared  def invlogit(x):\n    return np.exp(x) / (1 + np.exp(x))\n\nn = 4000\nn_oos = 50\ncoeff = 1.\n\npredictors = np.random.normal(size=n)\n# Turn predictor into a shared var so that we can change it later\npredictors_shared = shared(predictors)\n\noutcomes = np.random.binomial(1, invlogit(coeff * predictors))  outcomes  array([1, 1, 0, ..., 0, 1, 1])  predictors_oos = np.random.normal(size=50)\noutcomes_oos = np.random.binomial(1, invlogit(coeff * predictors_oos))  def tinvlogit(x):\n    import theano.tensor as t\n    return t.exp(x) / (1 + t.exp(x))\n\nwith pm.Model() as model:\n    coeff = pm.Normal('coeff', mu=0, sd=1)\n    p = tinvlogit(coeff * predictors_shared)\n\n    o = pm.Bernoulli('o', p, observed=outcomes)\n\n    start = pm.find_MAP()\n    step = pm.NUTS(scaling=start)\n    trace = pm.sample(500, step)   [-----------------100%-----------------] 500 of 500 complete in 2.9 sec  # Changing values here will also change values in the model\npredictors_shared.set_value(predictors_oos)  # Simply running PPC will use the updated values and do prediction\nppc = pm.sample_ppc(trace, model=model, samples=500)",
            "title": "Prediction"
        },
        {
            "location": "/posterior_predictive/#mean-predicted-values-plus-error-bars-to-give-sense-of-uncertainty-in-prediction",
            "text": "plt.errorbar(x=predictors_oos, y=np.asarray(ppc['o']).mean(axis=0), yerr=np.asarray(ppc['o']).std(axis=0), linestyle='', marker='o')\nplt.plot(predictors_oos, outcomes_oos, 'o')\nplt.ylim(-.05, 1.05)\nplt.xlabel('predictor')\nplt.ylabel('outcome')  matplotlib.text.Text at 0x129c81890",
            "title": "Mean predicted values plus error bars to give sense of uncertainty in prediction"
        },
        {
            "location": "/survival_analysis/",
            "text": "Bayesian Survival Analysis\n\n\nAuthor: Austin Rochford\n\n\nSurvival analysis\n studies the distribution of the time to an event.  Its applications span many fields across medicine, biology, engineering, and social science.  This tutorial shows how to fit and analyze a Bayesian survival model in Python using \npymc3\n.\n\n\nWe illustrate these concepts by analyzing a \nmastectomy data set\n from \nR\n's \nHSAUR\n package.\n\n\n%matplotlib inline\n\n\n\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pymc3 as pm\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\nimport seaborn as sns\nfrom statsmodels import datasets\nfrom theano import tensor as T\n\n\n\n\nCouldn't import dot_parser, loading of dot files will not be possible.\n\n\n\nFortunately, \nstatsmodels.datasets\n makes it quite easy to load a number of data sets from \nR\n.\n\n\ndf = datasets.get_rdataset('mastectomy', 'HSAUR', cache=True).data\ndf.event = df.event.astype(np.int64)\ndf.metastized = (df.metastized == 'yes').astype(np.int64)\nn_patients = df.shape[0]\npatients = np.arange(n_patients)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n  \n\n    \n\n      \n\n      \ntime\n\n      \nevent\n\n      \nmetastized\n\n    \n\n  \n\n  \n\n    \n\n      \n0\n\n      \n23\n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n1\n\n      \n47\n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n2\n\n      \n69\n\n      \n1\n\n      \n0\n\n    \n\n    \n\n      \n3\n\n      \n70\n\n      \n0\n\n      \n0\n\n    \n\n    \n\n      \n4\n\n      \n100\n\n      \n0\n\n      \n0\n\n    \n\n  \n\n\n\n\n\n\n\nn_patients\n\n\n\n\n44\n\n\n\nEach row represents observations from a woman diagnosed with breast cancer that underwent a mastectomy.  The column \ntime\n represents the time (in months) post-surgery that the woman was observed.  The column \nevent\n indicates whether or not the woman died during the observation period.  The column \nmetastized\n represents whether the cancer had \nmetastized\n prior to surgery.\n\n\nThis tutorial analyzes the relationship between survival time post-mastectomy and whether or not the cancer had metastized.\n\n\nA crash course in survival analysis\n\n\nFirst we introduce a (very little) bit of theory.  If the random variable \n$T$\n is the time to the event we are studying, survival analysis is primarily concerned with the survival function\n\n\n$$S(t) = P(T \n t) = 1 - F(t),$$\n\n\nwhere \n$F$\n is the \nCDF\n of \n$T$\n.  It is mathematically convenient to express the survival function in terms of the \nhazard rate\n, \n$\\lambda(t)$\n.  The hazard rate is the instantaneous probability that the event occurs at time \n$t$\n given that it has not yet occured.  That is,\n\n\n$$\\begin{align*}\n\\lambda(t)\n    \n = \\lim_{\\Delta t \\to 0} \\frac{P(t \n T \n t + \\Delta t\\ |\\ T \n t)}{\\Delta t} \\\\\n    \n = \\lim_{\\Delta t \\to 0} \\frac{P(t \n T \n t + \\Delta t)}{\\Delta t \\cdot P(T \n t)} \\\\\n    \n = \\frac{1}{S(t)} \\cdot \\lim_{\\Delta t \\to 0} \\frac{S(t + \\Delta t) - S(t)}{\\Delta t}\n      = -\\frac{S'(t)}{S(t)}.\n\\end{align*}$$\n\n\nSolving this differential equation for the survival function shows that\n\n\n$$S(t) = \\exp\\left(-\\int_0^s \\lambda(s)\\ ds\\right).$$\n\n\nThis representation of the survival function shows that the cumulative hazard function\n\n\n$$\\Lambda(t) = \\int_0^t \\lambda(s)\\ ds$$\n\n\nis an important quantity in survival analysis, since we may consicesly write \n$S(t) = \\exp(-\\Lambda(t)).$\n\n\nAn important, but subtle, point in survival analysis is \ncensoring\n.  Even though the quantity we are interested in estimating is the time between surgery and death, we do not observe the death of every subject.  At the point in time that we perform our analysis, some of our subjects will thankfully still be alive. In the case of our mastectomy study, \ndf.event\n is one if the subject's death was observed (the observation is not censored) and is zero if the death was not observed (the observation is censored).\n\n\ndf.event.mean()\n\n\n\n\n0.59090909090909094\n\n\n\nJust over 40% of our observations are censored.  We visualize the observed durations and indicate which observations are censored below.\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nblue, _, red = sns.color_palette()[:3]\n\nax.hlines(patients[df.event.values == 0], 0, df[df.event.values == 0].time,\n          color=blue, label='Censored');\n\nax.hlines(patients[df.event.values == 1], 0, df[df.event.values == 1].time,\n          color=red, label='Uncensored');\n\nax.scatter(df[df.metastized.values == 1].time, patients[df.metastized.values == 1],\n           color='k', zorder=10, label='Metastized');\n\nax.set_xlim(left=0);\nax.set_xlabel('Months since mastectomy');\n\nax.set_ylim(-0.25, n_patients + 0.25);\n\nax.legend(loc='center right');\n\n\n\n\n\n\nWhen an observation is censored (\ndf.event\n is zero), \ndf.time\n is not the subject's survival time.  All we can conclude from such a censored obsevation is that the subject's true survival time exceeds \ndf.time\n.\n\n\nThis is enough basic surival analysis theory for the purposes of this tutorial; for a more extensive introduction, consult Aalen et al.^[Aalen, Odd, Ornulf Borgan, and Hakon Gjessing. Survival and event history analysis: a process point of view. Springer Science \n Business Media, 2008.]\n\n\nBayesian proportional hazards model\n\n\nThe two most basic estimators in survial analysis are the \nKaplan-Meier estimator\n of the survival function and the \nNelson-Aalen estimator\n of the cumulative hazard function.  However, since we want to understand the impact of metastization on survival time, a risk regression model is more appropriate.  Perhaps the most commonly used risk regression model is \nCox's proportional hazards model\n.  In this model, if we have covariates \n$\\mathbf{x}$\n and regression coefficients \n$\\beta$\n, the hazard rate is modeled as\n\n\n$$\\lambda(t) = \\lambda_0(t) \\exp(\\mathbf{x} \\beta).$$\n\n\nHere \n$\\lambda_0(t)$\n is the baseline hazard, which is independent of the covariates \n$\\mathbf{x}$\n.  In this example, the covariates are the one-dimensonal vector \ndf.metastized\n.\n\n\nUnlike in many regression situations, \n$\\mathbf{x}$\n should not include a constant term corresponding to an intercept.  If \n$\\mathbf{x}$\n includes a constant term corresponding to an intercept, the model becomes \nunidentifiable\n.  To illustrate this unidentifiability, suppose that\n\n\n$$\\lambda(t) = \\lambda_0(t) \\exp(\\beta_0 + \\mathbf{x} \\beta) = \\lambda_0(t) \\exp(\\beta_0) \\exp(\\mathbf{x} \\beta).$$\n\n\nIf \n$\\tilde{\\beta}_0 = \\beta_0 + \\delta$\n and \n$\\tilde{\\lambda}_0(t) = \\lambda_0(t) \\exp(-\\delta)$\n, then \n$\\lambda(t) = \\tilde{\\lambda}_0(t) \\exp(\\tilde{\\beta}_0 + \\mathbf{x} \\beta)$\n as well, making the model with \n$\\beta_0$\n unidentifiable.\n\n\nIn order to perform Bayesian inference with the Cox model, we must specify priors on \n$\\beta$\n and \n$\\lambda_0(t)$\n.  We place a normal prior on \n$\\beta$\n, \n$\\beta \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2),$\n where \n$\\mu_{\\beta} \\sim N(0, 10^2)$\n and \n$\\sigma_{\\beta} \\sim U(0, 10)$\n.\n\n\nA suitable prior on \n$\\lambda_0(t)$\n is less obvious.  We choose a semiparametric prior, where \n$\\lambda_0(t)$\n is a piecewise constant function.  This prior requires us to partition the time range in question into intervals with endpoints \n$0 \\leq s_1 \n s_2 \n \\cdots \n s_N$\n.  With this partition, \n$\\lambda_0 (t) = \\lambda_j$\n if \n$s_j \\leq t \n s_{j + 1}$\n.  With \n$\\lambda_0(t)$\n constrained to have this form, all we need to do is choose priors for the \n$N - 1$\n values \n$\\lambda_j$\n.  We use independent vague priors \n$\\lambda_j \\sim \\operatorname{Gamma}(10^{-2}, 10^{-2}).$\n  For our mastectomy example, we make each interval three months long.\n\n\ninterval_length = 3\ninterval_bounds = np.arange(0, df.time.max() + interval_length + 1, interval_length)\nn_intervals = interval_bounds.size - 1\nintervals = np.arange(n_intervals)\n\n\n\n\nWe see how deaths and censored observations are distributed in these intervals.\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.hist(df[df.event == 1].time.values, bins=interval_bounds,\n        color=red, alpha=0.5, lw=0,\n        label='Uncensored');\nax.hist(df[df.event == 0].time.values, bins=interval_bounds,\n        color=blue, alpha=0.5, lw=0,\n        label='Censored');\n\nax.set_xlim(0, interval_bounds[-1]);\nax.set_xlabel('Months since mastectomy');\n\nax.set_yticks([0, 1, 2, 3]);\nax.set_ylabel('Number of observations');\n\nax.legend();\n\n\n\n\n\n\nWith the prior distributions on \n$\\beta$\n and \n$\\lambda_0(t)$\n chosen, we now show how the model may be fit using MCMC simulation with \npymc3\n.  The key observation is that the piecewise-constant proportional hazard model is \nclosely related\n to a Poisson regression model.   (The models are not identical, but their likelihoods differ by a factor that depends only on the observed data and not the parameters \n$\\beta$\n and \n$\\lambda_j$\n.  For details, see Germ\u00e1n Rodr\u00edguez's WWS 509 \ncourse notes\n.)\n\n\nWe define indicator variables based on whether or the \n$i$\n-th suject died in the \n$j$\n-th interval,\n\n\n$$d_{i, j} = \\begin{cases}\n    1 \n \\textrm{if subject } i \\textrm{ died in interval } j \\\\\n    0 \n \\textrm{otherwise}\n\\end{cases}.$$\n\n\nlast_period = np.floor((df.time - 0.01) / interval_length)\n\ndeath = np.zeros((n_patients, n_intervals))\ndeath[patients, last_period] = df.event\n\n\n\n\nWe also define \n$t_{i, j}$\n to be the amount of time the \n$i$\n-th subject was at risk in the \n$j$\n-th interval.\n\n\nexposure = np.greater_equal.outer(df.time, interval_bounds[:-1]) * interval_length\nexposure[patients, last_period] = df.time - interval_bounds[last_period]\n\n\n\n\nFinally, denote the risk incurred by the \n$i$\n-th subject in the \n$j$\n-th interval as \n$\\lambda_{i, j} = \\lambda_j \\exp(\\mathbf{x}_i \\beta)$\n.\n\n\nWe may approximate \n$d_{i, j}$\n with a Possion random variable with mean \n$t_{i, j}\\ \\lambda_{i, j}$\n.  This approximation leads to the following \npymc3\n model.\n\n\nSEED = 5078864 # from random.org\n\n\n\n\nwith pm.Model() as model:\n    lambda0 = pm.Gamma('lambda0', 0.01, 0.01, shape=n_intervals)\n\n    sigma = pm.Uniform('sigma', 0., 10.)\n    tau = pm.Deterministic('tau', sigma**-2)\n    mu_beta = pm.Normal('mu_beta', 0., 10**-2)\n    beta = pm.Normal('beta', mu_beta, tau)\n\n    lambda_ = pm.Deterministic('lambda_', T.outer(T.exp(beta * df.metastized), lambda0))\n    mu = pm.Deterministic('mu', exposure * lambda_)\n\n    obs = pm.Poisson('obs', mu, observed=death)\n\n\n\n\nWe now sample from the model.\n\n\nn_samples = 40000\nburn = 20000\nthin = 20\n\n\n\n\nwith model:\n    step = pm.Metropolis()\n    trace_ = pm.sample(n_samples, step, random_seed=SEED)\n\n\n\n\n [-----------------100%-----------------] 40000 of 40000 complete in 44.2 sec\n\n\n\ntrace = trace_[burn::thin]\n\n\n\n\nWe see that the hazard rate for subjects whose cancer has metastized is about one and a half times the rate of those whose cancer has not metastized.\n\n\nnp.exp(trace['beta'].mean())\n\n\n\n\n1.645592148084472\n\n\n\npm.traceplot(trace, vars=['beta']);\n\n\n\n\n\n\npm.autocorrplot(trace, varnames=['beta']);\n\n\n\n\n\n\nWe now examine the effect of metastization on both the cumulative hazard and on the survival function.\n\n\nbase_hazard = trace['lambda0']\nmet_hazard = trace['lambda0'] * np.exp(np.atleast_2d(trace['beta']).T)\n\n\n\n\ndef cum_hazard(hazard):\n    return (interval_length * hazard).cumsum(axis=-1)\n\ndef survival(hazard):\n    return np.exp(-cum_hazard(hazard))\n\n\n\n\ndef plot_with_hpd(x, hazard, f, ax, color=None, label=None, alpha=0.05):\n    mean = f(hazard.mean(axis=0))\n\n    percentiles = 100 * np.array([alpha / 2., 1. - alpha / 2.])\n    hpd = np.percentile(f(hazard), percentiles, axis=0)\n\n    ax.fill_between(x, hpd[0], hpd[1], color=color, alpha=0.25)\n    ax.step(x, mean, color=color, label=label);\n\n\n\n\nfig, (hazard_ax, surv_ax) = plt.subplots(ncols=2, sharex=True, sharey=False, figsize=(16, 6))\n\nplot_with_hpd(interval_bounds[:-1], base_hazard, cum_hazard,\n              hazard_ax, color=blue, label='Had not metastized')\nplot_with_hpd(interval_bounds[:-1], met_hazard, cum_hazard,\n              hazard_ax, color=red, label='Metastized')\n\nhazard_ax.set_xlim(0, df.time.max());\nhazard_ax.set_xlabel('Months since mastectomy');\n\nhazard_ax.set_ylabel(r'Cumulative hazard $\\Lambda(t)$');\n\nhazard_ax.legend(loc=2);\n\nplot_with_hpd(interval_bounds[:-1], base_hazard, survival,\n              surv_ax, color=blue)\nplot_with_hpd(interval_bounds[:-1], met_hazard, survival,\n              surv_ax, color=red)\n\nsurv_ax.set_xlim(0, df.time.max());\nsurv_ax.set_xlabel('Months since mastectomy');\n\nsurv_ax.set_ylabel('Survival function $S(t)$');\n\nfig.suptitle('Bayesian survival model');\n\n\n\n\n\n\nWe see that the cumulative hazard for metastized subjects increases more rapidly initially (through about seventy months), after which it increases roughly in parallel with the baseline cumulative hazard.\n\n\nThese plots also show the pointwise 95% high posterior density interval for each function.  One of the distinct advantages of the Bayesian model fit with \npymc3\n is the inherent quantification of uncertainty in our estimates.\n\n\nTime varying effects\n\n\nAnother of the advantages of the model we have built is its flexibility.  From the plots above, we may reasonable believe that the additional hazard due to metastization varies over time; it seems plausible that cancer that has metastized increases the hazard rate immediately after the mastectomy, but that the risk due to metastization decreases over time.  We can accomodate this mechanism in our model by allowing the regression coefficients to vary over time.  In the time-varying coefficent model, if \n$s_j \\leq t \n s_{j + 1}$\n, we let \n$\\lambda(t) = \\lambda_j \\exp(\\mathbf{x} \\beta_j).$\n  The sequence of regression coefficients \n$\\beta_1, \\beta_2, \\ldots, \\beta_{N - 1}$\n form a normal random walk with \n$\\beta_1 \\sim N(0, 1)$\n, \n$\\beta_j\\ |\\ \\beta_{j - 1} \\sim N(\\beta_{j - 1}, 1)$\n.\n\n\nWe implement this model in \npymc3\n as follows.\n\n\nwith pm.Model() as time_varying_model:\n    lambda0 = pm.Gamma('lambda0', 0.01, 0.01, shape=n_intervals)\n\n    beta = GaussianRandomWalk('beta', tau=1., shape=n_intervals)\n\n    lambda_ = pm.Deterministic('h', lambda0 * T.exp(T.outer(T.constant(df.metastized), beta)))\n    mu = pm.Deterministic('mu', exposure * lambda_)\n\n    obs = pm.Poisson('obs', mu, observed=death)\n\n\n\n\nWe proceed to sample from this model.\n\n\nwith time_varying_model:\n    step = pm.Metropolis()\n    time_varying_trace_ = pm.sample(n_samples, step, random_seed=SEED)\n\n\n\n\n [-----------------100%-----------------] 40000 of 40000 complete in 49.3 sec\n\n\n\ntime_varying_trace = time_varying_trace_[burn::thin]\n\n\n\n\nWe see from the plot of \n$\\beta_j$\n over time below that initially \n$\\beta_j \n 0$\n, indicating an elevated hazard rate due to metastization, but that this risk declines as \n$\\beta_j \n 0$\n eventually.\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nbeta_hpd = np.percentile(time_varying_trace['beta'], [2.5, 97.5], axis=0)\nbeta_low = beta_hpd[0]\nbeta_high = beta_hpd[1]\nax.fill_between(interval_bounds[:-1], beta_low, beta_high,\n                color=blue, alpha=0.25);\nbeta_hat = time_varying_trace['beta'].mean(axis=0)\nax.step(interval_bounds[:-1], beta_hat, color=blue);\nax.scatter(interval_bounds[last_period[(df.event.values == 1) \n (df.metastized == 1)]],\n           beta_hat[last_period[(df.event.values == 1) \n (df.metastized == 1)]],\n           c=red, zorder=10, label='Died, cancer metastized');\nax.scatter(interval_bounds[last_period[(df.event.values == 0) \n (df.metastized == 1)]],\n           beta_hat[last_period[(df.event.values == 0) \n (df.metastized == 1)]],\n           c=blue, zorder=10, label='Censored, cancer metastized');\n\nax.set_xlim(0, df.time.max());\nax.set_xlabel('Months since mastectomy');\n\nax.set_ylabel(r'$\\beta_j$');\n\nax.legend();\n\n\n\n\n\n\nThe coefficients \n$\\beta_j$\n begin declining rapidly around one hundred months post-mastectomy, which seems reasonable, given that only three of twelve subjects whose cancer had metastized lived past this point died during the study.\n\n\nThe change in our estimate of the cumulative hazard and survival functions due to time-varying effects is also quite apparent in the following plots.\n\n\ntv_base_hazard = time_varying_trace['lambda0']\ntv_met_hazard = time_varying_trace['lambda0'] * np.exp(np.atleast_2d(time_varying_trace['beta']))\n\n\n\n\nfig, ax = plt.subplots(figsize=(8, 6))\n\nax.step(interval_bounds[:-1], cum_hazard(base_hazard.mean(axis=0)),\n        color=blue, label='Had not metastized');\nax.step(interval_bounds[:-1], cum_hazard(met_hazard.mean(axis=0)),\n        color=red, label='Metastized');\n\nax.step(interval_bounds[:-1], cum_hazard(tv_base_hazard.mean(axis=0)),\n        color=blue, linestyle='--', label='Had not metastized (time varying effect)');\nax.step(interval_bounds[:-1], cum_hazard(tv_met_hazard.mean(axis=0)),\n        color=red, linestyle='--', label='Metastized (time varying effect)');\n\nax.set_xlim(0, df.time.max() - 4);\nax.set_xlabel('Months since mastectomy');\n\nax.set_ylim(0, 2);\nax.set_ylabel(r'Cumulative hazard $\\Lambda(t)$');\n\nax.legend(loc=2);\n\n\n\n\n\n\nfig, (hazard_ax, surv_ax) = plt.subplots(ncols=2, sharex=True, sharey=False, figsize=(16, 6))\n\nplot_with_hpd(interval_bounds[:-1], tv_base_hazard, cum_hazard,\n              hazard_ax, color=blue, label='Had not metastized')\nplot_with_hpd(interval_bounds[:-1], tv_met_hazard, cum_hazard,\n              hazard_ax, color=red, label='Metastized')\n\nhazard_ax.set_xlim(0, df.time.max());\nhazard_ax.set_xlabel('Months since mastectomy');\n\nhazard_ax.set_ylim(0, 2);\nhazard_ax.set_ylabel(r'Cumulative hazard $\\Lambda(t)$');\n\nhazard_ax.legend(loc=2);\n\nplot_with_hpd(interval_bounds[:-1], tv_base_hazard, survival,\n              surv_ax, color=blue)\nplot_with_hpd(interval_bounds[:-1], tv_met_hazard, survival,\n              surv_ax, color=red)\n\nsurv_ax.set_xlim(0, df.time.max());\nsurv_ax.set_xlabel('Months since mastectomy');\n\nsurv_ax.set_ylabel('Survival function $S(t)$');\n\nfig.suptitle('Bayesian survival model with time varying effects');\n\n\n\n\n\n\nWe have really only scratched the surface of both survival analysis and the Bayesian approach to survival analysis.  More information on Bayesian survival analysis is available in Ibrahim et al.^[Ibrahim, Joseph G., Ming\u2010Hui Chen, and Debajyoti Sinha. Bayesian survival analysis. John Wiley \n Sons, Ltd, 2005.]  (For example, we may want to account for individual frailty in either or original or time-varying models.)\n\n\nThis tutorial is available as an \nIPython\n notebook \nhere\n.  It is adapted from a blog post that first appeared \nhere\n.",
            "title": "Survival Analysis"
        },
        {
            "location": "/survival_analysis/#bayesian-survival-analysis",
            "text": "Author: Austin Rochford  Survival analysis  studies the distribution of the time to an event.  Its applications span many fields across medicine, biology, engineering, and social science.  This tutorial shows how to fit and analyze a Bayesian survival model in Python using  pymc3 .  We illustrate these concepts by analyzing a  mastectomy data set  from  R 's  HSAUR  package.  %matplotlib inline  from matplotlib import pyplot as plt\nimport numpy as np\nimport pymc3 as pm\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\nimport seaborn as sns\nfrom statsmodels import datasets\nfrom theano import tensor as T  Couldn't import dot_parser, loading of dot files will not be possible.  Fortunately,  statsmodels.datasets  makes it quite easy to load a number of data sets from  R .  df = datasets.get_rdataset('mastectomy', 'HSAUR', cache=True).data\ndf.event = df.event.astype(np.int64)\ndf.metastized = (df.metastized == 'yes').astype(np.int64)\nn_patients = df.shape[0]\npatients = np.arange(n_patients)  df.head()   \n   \n     \n       \n       time \n       event \n       metastized \n     \n   \n   \n     \n       0 \n       23 \n       1 \n       0 \n     \n     \n       1 \n       47 \n       1 \n       0 \n     \n     \n       2 \n       69 \n       1 \n       0 \n     \n     \n       3 \n       70 \n       0 \n       0 \n     \n     \n       4 \n       100 \n       0 \n       0 \n     \n      n_patients  44  Each row represents observations from a woman diagnosed with breast cancer that underwent a mastectomy.  The column  time  represents the time (in months) post-surgery that the woman was observed.  The column  event  indicates whether or not the woman died during the observation period.  The column  metastized  represents whether the cancer had  metastized  prior to surgery.  This tutorial analyzes the relationship between survival time post-mastectomy and whether or not the cancer had metastized.  A crash course in survival analysis  First we introduce a (very little) bit of theory.  If the random variable  $T$  is the time to the event we are studying, survival analysis is primarily concerned with the survival function  $$S(t) = P(T   t) = 1 - F(t),$$  where  $F$  is the  CDF  of  $T$ .  It is mathematically convenient to express the survival function in terms of the  hazard rate ,  $\\lambda(t)$ .  The hazard rate is the instantaneous probability that the event occurs at time  $t$  given that it has not yet occured.  That is,  $$\\begin{align*}\n\\lambda(t)\n      = \\lim_{\\Delta t \\to 0} \\frac{P(t   T   t + \\Delta t\\ |\\ T   t)}{\\Delta t} \\\\\n      = \\lim_{\\Delta t \\to 0} \\frac{P(t   T   t + \\Delta t)}{\\Delta t \\cdot P(T   t)} \\\\\n      = \\frac{1}{S(t)} \\cdot \\lim_{\\Delta t \\to 0} \\frac{S(t + \\Delta t) - S(t)}{\\Delta t}\n      = -\\frac{S'(t)}{S(t)}.\n\\end{align*}$$  Solving this differential equation for the survival function shows that  $$S(t) = \\exp\\left(-\\int_0^s \\lambda(s)\\ ds\\right).$$  This representation of the survival function shows that the cumulative hazard function  $$\\Lambda(t) = \\int_0^t \\lambda(s)\\ ds$$  is an important quantity in survival analysis, since we may consicesly write  $S(t) = \\exp(-\\Lambda(t)).$  An important, but subtle, point in survival analysis is  censoring .  Even though the quantity we are interested in estimating is the time between surgery and death, we do not observe the death of every subject.  At the point in time that we perform our analysis, some of our subjects will thankfully still be alive. In the case of our mastectomy study,  df.event  is one if the subject's death was observed (the observation is not censored) and is zero if the death was not observed (the observation is censored).  df.event.mean()  0.59090909090909094  Just over 40% of our observations are censored.  We visualize the observed durations and indicate which observations are censored below.  fig, ax = plt.subplots(figsize=(8, 6))\n\nblue, _, red = sns.color_palette()[:3]\n\nax.hlines(patients[df.event.values == 0], 0, df[df.event.values == 0].time,\n          color=blue, label='Censored');\n\nax.hlines(patients[df.event.values == 1], 0, df[df.event.values == 1].time,\n          color=red, label='Uncensored');\n\nax.scatter(df[df.metastized.values == 1].time, patients[df.metastized.values == 1],\n           color='k', zorder=10, label='Metastized');\n\nax.set_xlim(left=0);\nax.set_xlabel('Months since mastectomy');\n\nax.set_ylim(-0.25, n_patients + 0.25);\n\nax.legend(loc='center right');   When an observation is censored ( df.event  is zero),  df.time  is not the subject's survival time.  All we can conclude from such a censored obsevation is that the subject's true survival time exceeds  df.time .  This is enough basic surival analysis theory for the purposes of this tutorial; for a more extensive introduction, consult Aalen et al.^[Aalen, Odd, Ornulf Borgan, and Hakon Gjessing. Survival and event history analysis: a process point of view. Springer Science   Business Media, 2008.]  Bayesian proportional hazards model  The two most basic estimators in survial analysis are the  Kaplan-Meier estimator  of the survival function and the  Nelson-Aalen estimator  of the cumulative hazard function.  However, since we want to understand the impact of metastization on survival time, a risk regression model is more appropriate.  Perhaps the most commonly used risk regression model is  Cox's proportional hazards model .  In this model, if we have covariates  $\\mathbf{x}$  and regression coefficients  $\\beta$ , the hazard rate is modeled as  $$\\lambda(t) = \\lambda_0(t) \\exp(\\mathbf{x} \\beta).$$  Here  $\\lambda_0(t)$  is the baseline hazard, which is independent of the covariates  $\\mathbf{x}$ .  In this example, the covariates are the one-dimensonal vector  df.metastized .  Unlike in many regression situations,  $\\mathbf{x}$  should not include a constant term corresponding to an intercept.  If  $\\mathbf{x}$  includes a constant term corresponding to an intercept, the model becomes  unidentifiable .  To illustrate this unidentifiability, suppose that  $$\\lambda(t) = \\lambda_0(t) \\exp(\\beta_0 + \\mathbf{x} \\beta) = \\lambda_0(t) \\exp(\\beta_0) \\exp(\\mathbf{x} \\beta).$$  If  $\\tilde{\\beta}_0 = \\beta_0 + \\delta$  and  $\\tilde{\\lambda}_0(t) = \\lambda_0(t) \\exp(-\\delta)$ , then  $\\lambda(t) = \\tilde{\\lambda}_0(t) \\exp(\\tilde{\\beta}_0 + \\mathbf{x} \\beta)$  as well, making the model with  $\\beta_0$  unidentifiable.  In order to perform Bayesian inference with the Cox model, we must specify priors on  $\\beta$  and  $\\lambda_0(t)$ .  We place a normal prior on  $\\beta$ ,  $\\beta \\sim N(\\mu_{\\beta}, \\sigma_{\\beta}^2),$  where  $\\mu_{\\beta} \\sim N(0, 10^2)$  and  $\\sigma_{\\beta} \\sim U(0, 10)$ .  A suitable prior on  $\\lambda_0(t)$  is less obvious.  We choose a semiparametric prior, where  $\\lambda_0(t)$  is a piecewise constant function.  This prior requires us to partition the time range in question into intervals with endpoints  $0 \\leq s_1   s_2   \\cdots   s_N$ .  With this partition,  $\\lambda_0 (t) = \\lambda_j$  if  $s_j \\leq t   s_{j + 1}$ .  With  $\\lambda_0(t)$  constrained to have this form, all we need to do is choose priors for the  $N - 1$  values  $\\lambda_j$ .  We use independent vague priors  $\\lambda_j \\sim \\operatorname{Gamma}(10^{-2}, 10^{-2}).$   For our mastectomy example, we make each interval three months long.  interval_length = 3\ninterval_bounds = np.arange(0, df.time.max() + interval_length + 1, interval_length)\nn_intervals = interval_bounds.size - 1\nintervals = np.arange(n_intervals)  We see how deaths and censored observations are distributed in these intervals.  fig, ax = plt.subplots(figsize=(8, 6))\n\nax.hist(df[df.event == 1].time.values, bins=interval_bounds,\n        color=red, alpha=0.5, lw=0,\n        label='Uncensored');\nax.hist(df[df.event == 0].time.values, bins=interval_bounds,\n        color=blue, alpha=0.5, lw=0,\n        label='Censored');\n\nax.set_xlim(0, interval_bounds[-1]);\nax.set_xlabel('Months since mastectomy');\n\nax.set_yticks([0, 1, 2, 3]);\nax.set_ylabel('Number of observations');\n\nax.legend();   With the prior distributions on  $\\beta$  and  $\\lambda_0(t)$  chosen, we now show how the model may be fit using MCMC simulation with  pymc3 .  The key observation is that the piecewise-constant proportional hazard model is  closely related  to a Poisson regression model.   (The models are not identical, but their likelihoods differ by a factor that depends only on the observed data and not the parameters  $\\beta$  and  $\\lambda_j$ .  For details, see Germ\u00e1n Rodr\u00edguez's WWS 509  course notes .)  We define indicator variables based on whether or the  $i$ -th suject died in the  $j$ -th interval,  $$d_{i, j} = \\begin{cases}\n    1   \\textrm{if subject } i \\textrm{ died in interval } j \\\\\n    0   \\textrm{otherwise}\n\\end{cases}.$$  last_period = np.floor((df.time - 0.01) / interval_length)\n\ndeath = np.zeros((n_patients, n_intervals))\ndeath[patients, last_period] = df.event  We also define  $t_{i, j}$  to be the amount of time the  $i$ -th subject was at risk in the  $j$ -th interval.  exposure = np.greater_equal.outer(df.time, interval_bounds[:-1]) * interval_length\nexposure[patients, last_period] = df.time - interval_bounds[last_period]  Finally, denote the risk incurred by the  $i$ -th subject in the  $j$ -th interval as  $\\lambda_{i, j} = \\lambda_j \\exp(\\mathbf{x}_i \\beta)$ .  We may approximate  $d_{i, j}$  with a Possion random variable with mean  $t_{i, j}\\ \\lambda_{i, j}$ .  This approximation leads to the following  pymc3  model.  SEED = 5078864 # from random.org  with pm.Model() as model:\n    lambda0 = pm.Gamma('lambda0', 0.01, 0.01, shape=n_intervals)\n\n    sigma = pm.Uniform('sigma', 0., 10.)\n    tau = pm.Deterministic('tau', sigma**-2)\n    mu_beta = pm.Normal('mu_beta', 0., 10**-2)\n    beta = pm.Normal('beta', mu_beta, tau)\n\n    lambda_ = pm.Deterministic('lambda_', T.outer(T.exp(beta * df.metastized), lambda0))\n    mu = pm.Deterministic('mu', exposure * lambda_)\n\n    obs = pm.Poisson('obs', mu, observed=death)  We now sample from the model.  n_samples = 40000\nburn = 20000\nthin = 20  with model:\n    step = pm.Metropolis()\n    trace_ = pm.sample(n_samples, step, random_seed=SEED)   [-----------------100%-----------------] 40000 of 40000 complete in 44.2 sec  trace = trace_[burn::thin]  We see that the hazard rate for subjects whose cancer has metastized is about one and a half times the rate of those whose cancer has not metastized.  np.exp(trace['beta'].mean())  1.645592148084472  pm.traceplot(trace, vars=['beta']);   pm.autocorrplot(trace, varnames=['beta']);   We now examine the effect of metastization on both the cumulative hazard and on the survival function.  base_hazard = trace['lambda0']\nmet_hazard = trace['lambda0'] * np.exp(np.atleast_2d(trace['beta']).T)  def cum_hazard(hazard):\n    return (interval_length * hazard).cumsum(axis=-1)\n\ndef survival(hazard):\n    return np.exp(-cum_hazard(hazard))  def plot_with_hpd(x, hazard, f, ax, color=None, label=None, alpha=0.05):\n    mean = f(hazard.mean(axis=0))\n\n    percentiles = 100 * np.array([alpha / 2., 1. - alpha / 2.])\n    hpd = np.percentile(f(hazard), percentiles, axis=0)\n\n    ax.fill_between(x, hpd[0], hpd[1], color=color, alpha=0.25)\n    ax.step(x, mean, color=color, label=label);  fig, (hazard_ax, surv_ax) = plt.subplots(ncols=2, sharex=True, sharey=False, figsize=(16, 6))\n\nplot_with_hpd(interval_bounds[:-1], base_hazard, cum_hazard,\n              hazard_ax, color=blue, label='Had not metastized')\nplot_with_hpd(interval_bounds[:-1], met_hazard, cum_hazard,\n              hazard_ax, color=red, label='Metastized')\n\nhazard_ax.set_xlim(0, df.time.max());\nhazard_ax.set_xlabel('Months since mastectomy');\n\nhazard_ax.set_ylabel(r'Cumulative hazard $\\Lambda(t)$');\n\nhazard_ax.legend(loc=2);\n\nplot_with_hpd(interval_bounds[:-1], base_hazard, survival,\n              surv_ax, color=blue)\nplot_with_hpd(interval_bounds[:-1], met_hazard, survival,\n              surv_ax, color=red)\n\nsurv_ax.set_xlim(0, df.time.max());\nsurv_ax.set_xlabel('Months since mastectomy');\n\nsurv_ax.set_ylabel('Survival function $S(t)$');\n\nfig.suptitle('Bayesian survival model');   We see that the cumulative hazard for metastized subjects increases more rapidly initially (through about seventy months), after which it increases roughly in parallel with the baseline cumulative hazard.  These plots also show the pointwise 95% high posterior density interval for each function.  One of the distinct advantages of the Bayesian model fit with  pymc3  is the inherent quantification of uncertainty in our estimates.  Time varying effects  Another of the advantages of the model we have built is its flexibility.  From the plots above, we may reasonable believe that the additional hazard due to metastization varies over time; it seems plausible that cancer that has metastized increases the hazard rate immediately after the mastectomy, but that the risk due to metastization decreases over time.  We can accomodate this mechanism in our model by allowing the regression coefficients to vary over time.  In the time-varying coefficent model, if  $s_j \\leq t   s_{j + 1}$ , we let  $\\lambda(t) = \\lambda_j \\exp(\\mathbf{x} \\beta_j).$   The sequence of regression coefficients  $\\beta_1, \\beta_2, \\ldots, \\beta_{N - 1}$  form a normal random walk with  $\\beta_1 \\sim N(0, 1)$ ,  $\\beta_j\\ |\\ \\beta_{j - 1} \\sim N(\\beta_{j - 1}, 1)$ .  We implement this model in  pymc3  as follows.  with pm.Model() as time_varying_model:\n    lambda0 = pm.Gamma('lambda0', 0.01, 0.01, shape=n_intervals)\n\n    beta = GaussianRandomWalk('beta', tau=1., shape=n_intervals)\n\n    lambda_ = pm.Deterministic('h', lambda0 * T.exp(T.outer(T.constant(df.metastized), beta)))\n    mu = pm.Deterministic('mu', exposure * lambda_)\n\n    obs = pm.Poisson('obs', mu, observed=death)  We proceed to sample from this model.  with time_varying_model:\n    step = pm.Metropolis()\n    time_varying_trace_ = pm.sample(n_samples, step, random_seed=SEED)   [-----------------100%-----------------] 40000 of 40000 complete in 49.3 sec  time_varying_trace = time_varying_trace_[burn::thin]  We see from the plot of  $\\beta_j$  over time below that initially  $\\beta_j   0$ , indicating an elevated hazard rate due to metastization, but that this risk declines as  $\\beta_j   0$  eventually.  fig, ax = plt.subplots(figsize=(8, 6))\n\nbeta_hpd = np.percentile(time_varying_trace['beta'], [2.5, 97.5], axis=0)\nbeta_low = beta_hpd[0]\nbeta_high = beta_hpd[1]\nax.fill_between(interval_bounds[:-1], beta_low, beta_high,\n                color=blue, alpha=0.25);\nbeta_hat = time_varying_trace['beta'].mean(axis=0)\nax.step(interval_bounds[:-1], beta_hat, color=blue);\nax.scatter(interval_bounds[last_period[(df.event.values == 1)   (df.metastized == 1)]],\n           beta_hat[last_period[(df.event.values == 1)   (df.metastized == 1)]],\n           c=red, zorder=10, label='Died, cancer metastized');\nax.scatter(interval_bounds[last_period[(df.event.values == 0)   (df.metastized == 1)]],\n           beta_hat[last_period[(df.event.values == 0)   (df.metastized == 1)]],\n           c=blue, zorder=10, label='Censored, cancer metastized');\n\nax.set_xlim(0, df.time.max());\nax.set_xlabel('Months since mastectomy');\n\nax.set_ylabel(r'$\\beta_j$');\n\nax.legend();   The coefficients  $\\beta_j$  begin declining rapidly around one hundred months post-mastectomy, which seems reasonable, given that only three of twelve subjects whose cancer had metastized lived past this point died during the study.  The change in our estimate of the cumulative hazard and survival functions due to time-varying effects is also quite apparent in the following plots.  tv_base_hazard = time_varying_trace['lambda0']\ntv_met_hazard = time_varying_trace['lambda0'] * np.exp(np.atleast_2d(time_varying_trace['beta']))  fig, ax = plt.subplots(figsize=(8, 6))\n\nax.step(interval_bounds[:-1], cum_hazard(base_hazard.mean(axis=0)),\n        color=blue, label='Had not metastized');\nax.step(interval_bounds[:-1], cum_hazard(met_hazard.mean(axis=0)),\n        color=red, label='Metastized');\n\nax.step(interval_bounds[:-1], cum_hazard(tv_base_hazard.mean(axis=0)),\n        color=blue, linestyle='--', label='Had not metastized (time varying effect)');\nax.step(interval_bounds[:-1], cum_hazard(tv_met_hazard.mean(axis=0)),\n        color=red, linestyle='--', label='Metastized (time varying effect)');\n\nax.set_xlim(0, df.time.max() - 4);\nax.set_xlabel('Months since mastectomy');\n\nax.set_ylim(0, 2);\nax.set_ylabel(r'Cumulative hazard $\\Lambda(t)$');\n\nax.legend(loc=2);   fig, (hazard_ax, surv_ax) = plt.subplots(ncols=2, sharex=True, sharey=False, figsize=(16, 6))\n\nplot_with_hpd(interval_bounds[:-1], tv_base_hazard, cum_hazard,\n              hazard_ax, color=blue, label='Had not metastized')\nplot_with_hpd(interval_bounds[:-1], tv_met_hazard, cum_hazard,\n              hazard_ax, color=red, label='Metastized')\n\nhazard_ax.set_xlim(0, df.time.max());\nhazard_ax.set_xlabel('Months since mastectomy');\n\nhazard_ax.set_ylim(0, 2);\nhazard_ax.set_ylabel(r'Cumulative hazard $\\Lambda(t)$');\n\nhazard_ax.legend(loc=2);\n\nplot_with_hpd(interval_bounds[:-1], tv_base_hazard, survival,\n              surv_ax, color=blue)\nplot_with_hpd(interval_bounds[:-1], tv_met_hazard, survival,\n              surv_ax, color=red)\n\nsurv_ax.set_xlim(0, df.time.max());\nsurv_ax.set_xlabel('Months since mastectomy');\n\nsurv_ax.set_ylabel('Survival function $S(t)$');\n\nfig.suptitle('Bayesian survival model with time varying effects');   We have really only scratched the surface of both survival analysis and the Bayesian approach to survival analysis.  More information on Bayesian survival analysis is available in Ibrahim et al.^[Ibrahim, Joseph G., Ming\u2010Hui Chen, and Debajyoti Sinha. Bayesian survival analysis. John Wiley   Sons, Ltd, 2005.]  (For example, we may want to account for individual frailty in either or original or time-varying models.)  This tutorial is available as an  IPython  notebook  here .  It is adapted from a blog post that first appeared  here .",
            "title": "Bayesian Survival Analysis"
        },
        {
            "location": "/GP-smoothing/",
            "text": "Gaussian Process (GP) smoothing\n\n\nThis example deals with the case when we want to \nsmooth\n the observed data points \n$(x_i, y_i)$\n of some 1-dimensional function \n$y=f(x)$\n, by finding the new values \n$(x_i, y'_i)$\n such that the new data is more \"smooth\" (see more on the definition of smoothness through allocation of variance in the model description below) when moving along the \n$x$\n axis. \n\n\nIt is important to note that we are \nnot\n dealing with the problem of interpolating the function \n$y=f(x)$\n at the unknown values of \n$x$\n. Such problem would be called \"regression\" not \"smoothing\", and will be considered in other examples.\n\n\nIf we assume the functional dependency between \n$x$\n and \n$y$\n is \nlinear\n then, by making the independence and normality assumptions about the noise, we can infer a straight line that approximates the dependency between the variables, i.e. perform a linear regression. We can also fit more complex functional dependencies (like quadratic, cubic, etc), if we know the functional form of the dependency in advance.\n\n\nHowever, the \nfunctional form\n of \n$y=f(x)$\n is \nnot always known in advance\n, and it might be hard to choose which one to fit, given the data. For example, you wouldn't necessarily know which function to use, given the following observed data. Assume you haven't seen the formula that generated it:\n\n\n%pylab inline\nfigsize(12, 6);\n\n\n\n\nPopulating the interactive namespace from numpy and matplotlib\n\n\n\nimport numpy as np\nimport scipy.stats as stats\n\nx = np.linspace(0, 50, 100)\ny = (np.exp(1.0 + np.power(x, 0.5) - np.exp(x/15.0)) + \n     np.random.normal(scale=1.0, size=x.shape))\n\nplot(x, y);\nxlabel(\nx\n);\nylabel(\ny\n);\ntitle(\nObserved Data\n);\n\n\n\n\n\n\nLet's try a linear regression first\n\n\nAs humans, we see that there is a non-linear dependency with some noise, and we would like to capture that dependency. If we perform a linear regression, we see that the \"smoothed\" data is less than satisfactory:\n\n\nplot(x, y);\nxlabel(\nx\n);\nylabel(\ny\n);\n\nlin = stats.linregress(x, y)\nplot(x, lin.intercept + lin.slope * x);\ntitle(\nLinear Smoothing\n);\n\n\n\n\n\n\nLinear regression model recap\n\n\nThe linear regression assumes there is a linear dependency between the input \n$x$\n and output \n$y$\n, sprinkled with some noise around it so that for each observed data point we have:\n\n\n$$ y_i = a + b\\, x_i + \\epsilon_i $$\n\n\nwhere the observation errors at each data point satisfy:\n\n\n$$ \\epsilon_i \\sim N(0, \\sigma^2) $$\n\n\nwith the same \n$\\sigma$\n, and the errors are independent:\n\n\n$$ cov(\\epsilon_i, \\epsilon_j) = 0 \\: \\text{ for } i \\neq j $$\n\n\nThe parameters of this model are \n$a$\n, \n$b$\n, and \n$\\sigma$\n. It turns out that, under these assumptions, the maximum likelihood estimates of \n$a$\n and \n$b$\n don't depend on \n$\\sigma$\n. Then \n$\\sigma$\n can be estimated separately, after finding the most likely values for \n$a$\n and \n$b$\n.\n\n\nGaussian Process smoothing model\n\n\nThis model allows departure from the linear dependency by assuming that the dependency between \n$x$\n and \n$y$\n is a Brownian motion over the domain of \n$x$\n. This doesn't go as far as assuming a particular functional dependency between the variables. Instead, by \ncontrolling the standard deviation of the unobserved Brownian motion\n we can achieve different levels of smoothness of the recovered functional dependency at the original data points. \n\n\nThe particular model we are going to discuss assumes that the observed data points are \nevenly spaced\n across the domain of \n$x$\n, and therefore can be indexed by \n$i=1,\\dots,N$\n without the loss of generality. The model is described as follows:\n\n\n$$ z_i \\sim N(z_{i-1} + \\mu, (1 - \\alpha)\\cdot\\sigma^2) \\: \\text{ for } i=2,\\dots,N $$\n\n\n$$ z_1 \\sim ImproperFlat(-\\infty,\\infty) $$\n\n\n$$ y_i \\sim N(z_i, \\alpha\\cdot\\sigma^2) $$\n\n\nwhere \n$z$\n is the hidden Brownian motion, \n$y$\n is the observed data, and the total variance \n$\\sigma^2$\n of each ovservation is split between the hidden Brownian motion and the noise in proportions of \n$1 - \\alpha$\n and \n$\\alpha$\n respectively, with parameter \n$0 \n \\alpha \n 1$\n specifying the degree of smoothing.\n\n\nWhen we estimate the maximum likelihood values of the hidden process \n$z_i$\n at each of the data points, \n$i=1,\\dots,N$\n, these values provide an approximation of the functional dependency \n$y=f(x)$\n as \n$\\mathrm{E}\\,[f(x_i)] = z_i$\n at the original data points \n$x_i$\n only. Therefore, again, the method is called smoothing and not regression.\n\n\nLet's describe the above GP-smoothing model in PyMC3\n\n\nimport pymc3 as pm\nfrom theano import shared\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\nfrom scipy import optimize\n\n\n\n\nLet's create a model with a shared parameter for specifying different levels of smoothing. We use very wide priors for the \"mu\" and \"tau\" parameters of the hidden Brownian motion, which you can adjust according to your application.\n\n\nLARGE_NUMBER = 1e5\n\nmodel = pm.Model()\nwith model:\n    smoothing_param = shared(0.9)\n    mu = pm.Normal(\nmu\n, sd=LARGE_NUMBER)\n    tau = pm.Exponential(\ntau\n, 1.0/LARGE_NUMBER)\n    z = GaussianRandomWalk(\nz\n,\n                           mu=mu,\n                           tau=tau / (1.0 - smoothing_param), \n                           shape=y.shape)\n    obs = pm.Normal(\nobs\n, \n                    mu=z, \n                    tau=tau / smoothing_param, \n                    observed=y)\n\n\n\n\nLet's also make a helper function for inferring the most likely values of \n$z$\n:\n\n\ndef infer_z(smoothing):\n    with model:\n        smoothing_param.set_value(smoothing)\n        res = pm.find_MAP(vars=[z], fmin=optimize.fmin_l_bfgs_b)\n        return res['z']\n\n\n\n\nPlease note that in this example, we are only looking at the MAP estimate of the unobserved variables. We are not really interested in inferring the posterior distributions. Instead, we have a control parameter \n$\\alpha$\n which lets us allocate the variance between the hidden Brownian motion and the noise. Other goals and/or different models may require sampling to obtain the posterior distributions, but for our goal a MAP estimate will suffice.\n\n\nExploring different levels of smoothing\n\n\nLet's try to allocate 50% variance to the noise, and see if the result matches our expectations.\n\n\nsmoothing = 0.5\nz_val = infer_z(smoothing)\n\nplot(x, y);\nplot(x, z_val);\ntitle(\nSmoothing={}\n.format(smoothing));\n\n\n\n\n\n\nIt appears that the variance is split evenly between the noise and the hidden process, as expected. \n\n\nLet's try gradually increasing the smoothness parameter to see if we can obtain smoother data:\n\n\nsmoothing = 0.9\nz_val = infer_z(smoothing)\n\nplot(x, y);\nplot(x, z_val);\ntitle(\nSmoothing={}\n.format(smoothing));\n\n\n\n\n\n\nSmoothing \"to the limits\"\n\n\nBy increading the smoothing parameter, we can gradually make the inferred values of the hidden Brownian motion approach the average value of the data. This is because as we increase the smoothing parameter, we allow less and less of the variance to be allocated to the Brownian motion, so eventually it aproaches the process which almost doesn't change over the domain of \n$x$\n:\n\n\nfig, axes = subplots(2, 2)\n\nfor ax, smoothing in zip(axes.ravel(), [0.95, 0.99, 0.999, 0.9999]):\n\n    z_val = infer_z(smoothing)\n\n    ax.plot(x, y)\n    ax.plot(x, z_val)\n    ax.set_title('Smoothing={:05.4f}'.format(smoothing))\n\n\n\n\n\n\nInteractive smoothing\n\n\nBelow you can interactively test different levels of smoothing. Notice, because we use a \nshared Theano variable\n to specify the smoothing above, the model doesn't need to be recompiled every time you move the slider, and so the \ninference is fast\n!\n\n\nfrom IPython.html.widgets import interact\n@interact(smoothing=[0.01,0.99])\ndef plot_smoothed(smoothing=0.9):\n    z_val = infer_z(smoothing)\n\n    plot(x, y);\n    plot(x, z_val);\n    title(\nSmoothing={}\n.format(smoothing));\n\n\n\n\n\n\nThis example originally contributed by: Andrey Kuzmenko, http://github.com/akuz",
            "title": "Gaussian Process (GP) smoothing"
        },
        {
            "location": "/GP-smoothing/#gaussian-process-gp-smoothing",
            "text": "This example deals with the case when we want to  smooth  the observed data points  $(x_i, y_i)$  of some 1-dimensional function  $y=f(x)$ , by finding the new values  $(x_i, y'_i)$  such that the new data is more \"smooth\" (see more on the definition of smoothness through allocation of variance in the model description below) when moving along the  $x$  axis.   It is important to note that we are  not  dealing with the problem of interpolating the function  $y=f(x)$  at the unknown values of  $x$ . Such problem would be called \"regression\" not \"smoothing\", and will be considered in other examples.  If we assume the functional dependency between  $x$  and  $y$  is  linear  then, by making the independence and normality assumptions about the noise, we can infer a straight line that approximates the dependency between the variables, i.e. perform a linear regression. We can also fit more complex functional dependencies (like quadratic, cubic, etc), if we know the functional form of the dependency in advance.  However, the  functional form  of  $y=f(x)$  is  not always known in advance , and it might be hard to choose which one to fit, given the data. For example, you wouldn't necessarily know which function to use, given the following observed data. Assume you haven't seen the formula that generated it:  %pylab inline\nfigsize(12, 6);  Populating the interactive namespace from numpy and matplotlib  import numpy as np\nimport scipy.stats as stats\n\nx = np.linspace(0, 50, 100)\ny = (np.exp(1.0 + np.power(x, 0.5) - np.exp(x/15.0)) + \n     np.random.normal(scale=1.0, size=x.shape))\n\nplot(x, y);\nxlabel( x );\nylabel( y );\ntitle( Observed Data );   Let's try a linear regression first  As humans, we see that there is a non-linear dependency with some noise, and we would like to capture that dependency. If we perform a linear regression, we see that the \"smoothed\" data is less than satisfactory:  plot(x, y);\nxlabel( x );\nylabel( y );\n\nlin = stats.linregress(x, y)\nplot(x, lin.intercept + lin.slope * x);\ntitle( Linear Smoothing );   Linear regression model recap  The linear regression assumes there is a linear dependency between the input  $x$  and output  $y$ , sprinkled with some noise around it so that for each observed data point we have:  $$ y_i = a + b\\, x_i + \\epsilon_i $$  where the observation errors at each data point satisfy:  $$ \\epsilon_i \\sim N(0, \\sigma^2) $$  with the same  $\\sigma$ , and the errors are independent:  $$ cov(\\epsilon_i, \\epsilon_j) = 0 \\: \\text{ for } i \\neq j $$  The parameters of this model are  $a$ ,  $b$ , and  $\\sigma$ . It turns out that, under these assumptions, the maximum likelihood estimates of  $a$  and  $b$  don't depend on  $\\sigma$ . Then  $\\sigma$  can be estimated separately, after finding the most likely values for  $a$  and  $b$ .  Gaussian Process smoothing model  This model allows departure from the linear dependency by assuming that the dependency between  $x$  and  $y$  is a Brownian motion over the domain of  $x$ . This doesn't go as far as assuming a particular functional dependency between the variables. Instead, by  controlling the standard deviation of the unobserved Brownian motion  we can achieve different levels of smoothness of the recovered functional dependency at the original data points.   The particular model we are going to discuss assumes that the observed data points are  evenly spaced  across the domain of  $x$ , and therefore can be indexed by  $i=1,\\dots,N$  without the loss of generality. The model is described as follows:  $$ z_i \\sim N(z_{i-1} + \\mu, (1 - \\alpha)\\cdot\\sigma^2) \\: \\text{ for } i=2,\\dots,N $$  $$ z_1 \\sim ImproperFlat(-\\infty,\\infty) $$  $$ y_i \\sim N(z_i, \\alpha\\cdot\\sigma^2) $$  where  $z$  is the hidden Brownian motion,  $y$  is the observed data, and the total variance  $\\sigma^2$  of each ovservation is split between the hidden Brownian motion and the noise in proportions of  $1 - \\alpha$  and  $\\alpha$  respectively, with parameter  $0   \\alpha   1$  specifying the degree of smoothing.  When we estimate the maximum likelihood values of the hidden process  $z_i$  at each of the data points,  $i=1,\\dots,N$ , these values provide an approximation of the functional dependency  $y=f(x)$  as  $\\mathrm{E}\\,[f(x_i)] = z_i$  at the original data points  $x_i$  only. Therefore, again, the method is called smoothing and not regression.  Let's describe the above GP-smoothing model in PyMC3  import pymc3 as pm\nfrom theano import shared\nfrom pymc3.distributions.timeseries import GaussianRandomWalk\nfrom scipy import optimize  Let's create a model with a shared parameter for specifying different levels of smoothing. We use very wide priors for the \"mu\" and \"tau\" parameters of the hidden Brownian motion, which you can adjust according to your application.  LARGE_NUMBER = 1e5\n\nmodel = pm.Model()\nwith model:\n    smoothing_param = shared(0.9)\n    mu = pm.Normal( mu , sd=LARGE_NUMBER)\n    tau = pm.Exponential( tau , 1.0/LARGE_NUMBER)\n    z = GaussianRandomWalk( z ,\n                           mu=mu,\n                           tau=tau / (1.0 - smoothing_param), \n                           shape=y.shape)\n    obs = pm.Normal( obs , \n                    mu=z, \n                    tau=tau / smoothing_param, \n                    observed=y)  Let's also make a helper function for inferring the most likely values of  $z$ :  def infer_z(smoothing):\n    with model:\n        smoothing_param.set_value(smoothing)\n        res = pm.find_MAP(vars=[z], fmin=optimize.fmin_l_bfgs_b)\n        return res['z']  Please note that in this example, we are only looking at the MAP estimate of the unobserved variables. We are not really interested in inferring the posterior distributions. Instead, we have a control parameter  $\\alpha$  which lets us allocate the variance between the hidden Brownian motion and the noise. Other goals and/or different models may require sampling to obtain the posterior distributions, but for our goal a MAP estimate will suffice.  Exploring different levels of smoothing  Let's try to allocate 50% variance to the noise, and see if the result matches our expectations.  smoothing = 0.5\nz_val = infer_z(smoothing)\n\nplot(x, y);\nplot(x, z_val);\ntitle( Smoothing={} .format(smoothing));   It appears that the variance is split evenly between the noise and the hidden process, as expected.   Let's try gradually increasing the smoothness parameter to see if we can obtain smoother data:  smoothing = 0.9\nz_val = infer_z(smoothing)\n\nplot(x, y);\nplot(x, z_val);\ntitle( Smoothing={} .format(smoothing));   Smoothing \"to the limits\"  By increading the smoothing parameter, we can gradually make the inferred values of the hidden Brownian motion approach the average value of the data. This is because as we increase the smoothing parameter, we allow less and less of the variance to be allocated to the Brownian motion, so eventually it aproaches the process which almost doesn't change over the domain of  $x$ :  fig, axes = subplots(2, 2)\n\nfor ax, smoothing in zip(axes.ravel(), [0.95, 0.99, 0.999, 0.9999]):\n\n    z_val = infer_z(smoothing)\n\n    ax.plot(x, y)\n    ax.plot(x, z_val)\n    ax.set_title('Smoothing={:05.4f}'.format(smoothing))   Interactive smoothing  Below you can interactively test different levels of smoothing. Notice, because we use a  shared Theano variable  to specify the smoothing above, the model doesn't need to be recompiled every time you move the slider, and so the  inference is fast !  from IPython.html.widgets import interact\n@interact(smoothing=[0.01,0.99])\ndef plot_smoothed(smoothing=0.9):\n    z_val = infer_z(smoothing)\n\n    plot(x, y);\n    plot(x, z_val);\n    title( Smoothing={} .format(smoothing));   This example originally contributed by: Andrey Kuzmenko, http://github.com/akuz",
            "title": "Gaussian Process (GP) smoothing"
        }
    ]
}